# ============================================================================
# OpenEvolve Agent Configuration for AgentKernelArena
# ============================================================================

max_iterations: 10
checkpoint_interval: 5
timeout_seconds: 600

# ============================================================================
# LLM Configuration
# ============================================================================
llm:
  # API Base URL (Optional)
  # If not provided, defaults to:
  #   1. OPENAI_API_BASE environment variable, or
  #   2. https://api.openai.com/v1 (standard OpenAI API)
  # 
  # IMPORTANT: Model name must match the API endpoint format!
  #   - For OpenAI API: use "gpt-4o-mini", "gpt-4o", etc.
  #   - For AMD Claude API: use "claude-sonnet-4", "claude-opus-4", etc.
  #   - For Groq: use "llama-3.1-70b-versatile", etc.
  #
  # Uncomment one of these configurations:
  
  # --- Standard OpenAI (default) ---
  # api_base: "https://api.openai.com/v1"
  # models:
  #   - name: "gpt-4o-mini"
  
  # --- AMD LLM API (Claude) ---
  # api_base: "https://llm-api.amd.com/claude3/deployments/claude-sonnet-4"
  # models:
  #   - name: "claude-sonnet-4"
  
  # --- Groq ---
  # api_base: "https://api.groq.com/openai/v1"
  # models:
  #   - name: "llama-3.1-70b-versatile"
  
  # --- Local vLLM ---
  # api_base: "http://localhost:8000/v1"
  # models:
  #   - name: "meta-llama/Llama-3.1-70B-Instruct"
  
  models:
    - name: "claude-sonnet-4"  # Model name must contain "claude" for AMD Claude API
      weight: 1.0
      # Per-model api_base (Optional, overrides shared api_base)
      # api_base: "https://api.openai.com/v1"
  
  temperature: 0.7

database:
  population_size: 50
  num_islands: 2
  log_prompts: false

evaluator:
  timeout: 300  # Increased to 5 minutes for Triton compilation + GPU tests
  parallel_evaluations: 1
  cascade_evaluation: false
  verbose: true  # Enable detailed stdout/stderr logging
