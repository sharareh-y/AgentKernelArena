# API and Model configuration: openai, claude, openrouter, or vllm
provider: claude

# OpenAI Configuration (GPT-5)
openai:
  api_key: ${OPENAI_API_KEY}  # or set directly
  base_url: https://api.openai.com/v1
  model: gpt-5  # Options: gpt-5
  effort: low  # For GPT-5: low, medium, high
  max_tokens: 272_000  # For GPT-5 this maps to max_output_tokens, for others it's max_tokens
  timeout: 1800

# Claude Configuration (Opus 4, Sonnet 4.5)
claude:
  api_key: ${ANTHROPIC_API_KEY}  # or set directly
  base_url: https://api.anthropic.com
  model: claude-sonnet-4-5-20250929  # Options: claude-opus-4-1-20250805, claude-sonnet-4-5-20250929
  temperature: 0.1
  max_tokens: 64000
  timeout: 1000
  thinking: enabled  # Enable thinking process output (enabled/disabled, default: disabled)
  thinking_budget: 4096  # Budget tokens for thinking when enabled (minimum 1024)

# OpenRouter Configuration (o4-mini, o4-mini-7b-preview)
openrouter:
  api_key: ${OPENROUTER_API_KEY}  # or set directly
  base_url: https://openrouter.ai
  model: qwen/qwen3-next-80b-a3b-instruct  # Options: openai/o4-mini, openai/o4-mini-7b-preview
  effort: high  # Reasoning effort: minimal, low, medium, high
  max_output_tokens: 64000  # Max output tokens for reasoning models
  temperature: 0.1  # Temperature (optional, often not used with reasoning models)
  timeout: 1800

# vLLM Local Model Configuration
vllm:
  base_url: http://0.0.0.0:8000/v1  # vLLM server endpoint
  model: /group/ossdphi_algo_scratch_02/liuji/research/hip_llm_project/LLaMA-Factory/saves/qwen3-8b/full/sft_aicuda_gpumode_5ep # Qwen/Qwen3-8B  # Model name on vLLM server
  api_key: dummy  # API key as configured in vLLM server (--api-key dummy)
  temperature: 0.1
  max_tokens: 28_000
  timeout: 300

