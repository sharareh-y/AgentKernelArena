# Copyright(C) [2026] Advanced Micro Devices, Inc. All rights reserved.
"""
Utility functions for kernelgen module.
"""

import logging
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime


# Level multipliers for scoring
LEVEL_MULTIPLIERS = {
    "level1": 1,
    "level2": 10,
    "level3": 100,
}


def setup_logging() -> Tuple[logging.Logger, Path]:
    """
    Setup logging to both console and timestamped log file.

    Returns:
        tuple: (logger, log_file_path)
    """
    # Create logs directory
    log_dir = Path(__file__).parent / "logs"
    log_dir.mkdir(exist_ok=True)

    # Create timestamped log file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"kernelgen_{timestamp}.log"

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

    logger = logging.getLogger(__name__)
    return logger, log_file


def save_results(results: Dict[str, Any], output_file: Path, logger: logging.Logger) -> None:
    """
    Save benchmark results to a JSON file.

    Args:
        results: Dictionary of benchmark results
        output_file: Path to save the results
        logger: Logger instance
    """
    # Ensure parent directory exists
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # Save results
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    logger.info(f"\nResults saved to: {output_file}")


def print_summary(results: Dict[str, Any], logger: logging.Logger) -> None:
    """
    Print a summary of benchmark results grouped by level.

    Args:
        results: Dictionary of benchmark results
        logger: Logger instance
    """
    # Separate results by level
    by_level = {"level1": [], "level2": [], "level3": []}

    for model_name, result in results.items():
        # Skip metadata keys
        if model_name.startswith('_'):
            continue

        if isinstance(result, dict):
            level = result.get("level", "unknown")
            if level in by_level:
                by_level[level].append((model_name, result))

    logger.info("\n" + "="*100)
    logger.info("BENCHMARK SUMMARY")
    logger.info("="*100)

    # Calculate and display score statistics
    score_stats = calculate_total_score(results)

    logger.info(f"\nðŸ“Š OVERALL STATISTICS:")
    logger.info(f"  Total Models: {score_stats['total_models']}")
    logger.info(f"  Successful: {score_stats['successful_models']}")
    logger.info(f"  Total Raw Score: {score_stats['total_raw_score']:,}")
    logger.info(f"  Total Weighted Score: {score_stats['total_weighted_score']:,}")

    # Print level breakdown
    logger.info(f"\nðŸ“ˆ LEVEL BREAKDOWN:")
    for level_name, stats in score_stats['level_breakdown'].items():
        if stats['count'] > 0:
            logger.info(f"\n  {level_name.upper()} (Multiplier: {LEVEL_MULTIPLIERS[level_name]}x):")
            logger.info(f"    Models: {stats['count']}")
            logger.info(f"    Successful: {stats['successful']}")
            logger.info(f"    Raw Score: {stats['raw_score']:,}")
            logger.info(f"    Weighted Score: {stats['weighted_score']:,}")
            if stats['count'] > 0:
                avg_raw = stats['raw_score'] / stats['count']
                logger.info(f"    Average Raw Score: {avg_raw:.1f}")

    # Print header for detailed results
    logger.info(f"\nðŸ“‹ DETAILED RESULTS:")
    logger.info(f"{'Model':<50} {'Level':<8} {'Score':<10} {'Weighted':<12} {'Status':<10}")
    logger.info('-'*100)

    total_weighted_score = 0
    total_models = 0

    for level in ["level1", "level2", "level3"]:
        if not by_level[level]:
            continue

        multiplier = LEVEL_MULTIPLIERS[level]
        level_total = 0

        for model_name, result in by_level[level]:
            score = result["score"]
            weighted = score * multiplier
            status = result["status"]

            # Truncate long model names
            display_name = model_name[:48] + ".." if len(model_name) > 50 else model_name

            status_symbol = "âœ“" if status == "success" else "âœ—"
            logger.info(f"{display_name:<50} {level:<8} {score:<10} {weighted:<12} {status_symbol} {status:<10}")

            level_total += weighted
            total_models += 1

        if by_level[level]:
            logger.info(f"{'-'*100}")
            logger.info(f"{'Level ' + level + ' Total:':<50} {'':<8} {'':<10} {level_total:<12}")
            total_weighted_score += level_total

    logger.info(f"\n{'='*100}")
    logger.info(f"{'GRAND TOTAL:':<50} {'':<8} {'':<10} {total_weighted_score:<12} ({total_models} models)")
    logger.info(f"{'='*100}\n")


def calculate_total_score(results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calculate total score and statistics from benchmark results.

    Scoring system:
    - Compilation success: 20 points
    - Correctness pass: 100 points
    - Performance: 100 * speedup (e.g., 2x = 200, 3x = 300)

    Level multipliers:
    - Level 1: 1x
    - Level 2: 10x
    - Level 3: 100x

    Returns:
        Dictionary with total_score, weighted_score, and breakdown by level
    """
    total_raw_score = 0
    total_weighted_score = 0
    level_stats = {
        "level1": {"count": 0, "raw_score": 0, "weighted_score": 0, "successful": 0, "compiled": 0, "correct": 0},
        "level2": {"count": 0, "raw_score": 0, "weighted_score": 0, "successful": 0, "compiled": 0, "correct": 0},
        "level3": {"count": 0, "raw_score": 0, "weighted_score": 0, "successful": 0, "compiled": 0, "correct": 0}
    }

    for model_name, result in results.items():
        # Skip metadata keys
        if model_name.startswith('_'):
            continue

        if isinstance(result, dict) and 'score' in result:
            score = result.get('score', 0)
            level = result.get('level', 'unknown')
            status = result.get('status', 'failed')

            if level in level_stats:
                multiplier = LEVEL_MULTIPLIERS.get(level, 1)
                weighted = score * multiplier

                level_stats[level]['count'] += 1
                level_stats[level]['raw_score'] += score
                level_stats[level]['weighted_score'] += weighted

                # Track different success levels
                if score >= 20:  # At least compiled
                    level_stats[level]['compiled'] += 1
                if score >= 120:  # Compiled + correct
                    level_stats[level]['correct'] += 1
                if status == 'success' and score > 120:  # Has performance gain
                    level_stats[level]['successful'] += 1

                total_raw_score += score
                total_weighted_score += weighted

    return {
        "total_raw_score": total_raw_score,
        "total_weighted_score": total_weighted_score,
        "level_breakdown": level_stats,
        "total_models": sum(stats['count'] for stats in level_stats.values()),
        "successful_models": sum(stats['successful'] for stats in level_stats.values()),
        "compiled_models": sum(stats['compiled'] for stats in level_stats.values()),
        "correct_models": sum(stats['correct'] for stats in level_stats.values())
    }


def load_existing_results(results_file: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Load existing results from a JSON file if it exists.

    Args:
        results_file: Path to the results file
        logger: Logger instance

    Returns:
        Dictionary of existing results, or empty dict if file doesn't exist
    """
    if results_file.exists():
        try:
            with open(results_file, 'r') as f:
                data = json.load(f)
                # Extract just the results, not metadata
                if 'results' in data:
                    results = data['results']
                else:
                    results = data
                logger.info(f"Loaded {len(results)} existing results from {results_file}")
                return results
        except Exception as e:
            logger.warning(f"Could not load existing results: {e}")

    return {}