compile_command:
- python -c "import ast; ast.parse(open('test_gemm_no_scf.py').read())"
correctness_command:
- pytest -vv -x --maxfail=1 test_gemm_no_scf.py -k "not test_performance and not test_save_performance_results"
performance_command:
- pytest -vv -x --maxfail=1 test_gemm_no_scf.py -k "test_performance or test_save_performance_results"
task_type: instruction2triton
task_result_template: task_result_template.yaml
prompt:
  cheatsheet: null
  instructions: "\nYou are an expert in triton programming language. You will be given\
    \ the function definition for the `matmul_no_scf_kernel` kernels. Your task is\
    \ to complete the kernel code. Only complete the kernel code in the function definition,\
    \ DONT remove any python imports or helper utils in the instruction/code provided,\
    \ DONT change/interfere with the provided function definition and parameter list.\n\
    \nThis kernel, `matmul_no_scf_kernel`,  performs single block of matrix multiplication\
    \ (C = A @ B) without Structured Control Flow (SCF)\n**Your objective is to implement\
    \ the body of both the kernels `matmul_no_scf_kernel`.**\n\nYou must ensure that:\n\
    1.  All arguments received by `matmul_no_scf_kernel` are kept intact and not modified.\n\
    2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n\
    ```\nThe full definitions for `matmul_no_scf_kernel` and relevant helper utilities\
    \ are provided in the context below. You only need to complete the code for `matmul_no_scf_kernel`\
    \ whilst keeping other things intact. DONT remove Imports and HELPER utils.\n\n\
    ######################################## Imports ########################################\
    \ \nimport itertools\nimport os\nimport re\n\nimport pytest\nimport torch\nfrom\
    \ torch.testing import assert_close\n\nimport triton\nimport triton.language as\
    \ tl\n\n######################################## Imports ########################################\
    \ \n\n\n\n\n@triton.jit\ndef matmul_no_scf_kernel(\n    a_ptr,  # tl.pointer_type(dtype)\n\
    \    b_ptr,  # tl.pointer_type(dtype)\n    c_ptr,  # tl.pointer_type(dtype)\n\
    \    M: int,\n    N: int,\n    K: int,\n    stride_am: int,\n    stride_ak: int,\n\
    \    stride_bk: int,\n    stride_bn: int,\n    stride_cm: int,\n    stride_cn:\
    \ int,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n\
    \    FLOAT16_OUTPUT: tl.constexpr,\n    USE_TMA_EPILOGUE: tl.constexpr\n):\n \
    \   \"\"\"\n    Computes a single block of matrix multiplication (C = A @ B) without\
    \ explicit\n    iteration over the K dimension (i.e., no `tl.for_loop` for K accumulation,\n\
    \    implying K must be equal to BLOCK_K). This kernel is  named \"no_scf\"\n\
    \    because performing the full matmul accumulation over K would typically\n\
    \    introduce Structured Control Flow (SCF) in the generated MLIR/LLVM IR,\n\
    \    which this kernel avoids by processing only one K-block.\n\n    It loads\
    \ one block of matrix A and one block of matrix B, performs a dot\n    product,\
    \ and stores the resulting block to matrix C. The kernel should support\n    optional\
    \ casting of the output to float16 and an optional TMA-based epilogue\n    for\
    \ storing the result.\n\n    Parameters:\n    -----------\n    a_ptr : tl.pointer_type\n\
    \        Pointer to the input matrix A.\n    b_ptr : tl.pointer_type\n       \
    \ Pointer to the input matrix B.\n    c_ptr : tl.pointer_type\n        Pointer\
    \ to the output matrix C.\n    M : int\n        Number of rows in matrix A and\
    \ C. Expected to be equal to BLOCK_M.\n    N : int\n        Number of columns\
    \ in matrix B and C. Expected to be equal to BLOCK_N.\n    K : int\n        Number\
    \ of columns in matrix A and rows in matrix B (the common dimension).\n      \
    \  Expected to be equal to BLOCK_K.\n    stride_am : int\n        Stride of matrix\
    \ A along the M dimension (row stride).\n    stride_ak : int\n        Stride of\
    \ matrix A along the K dimension (column stride).\n    stride_bk : int\n     \
    \   Stride of matrix B along the K dimension (row stride).\n    stride_bn : int\n\
    \        Stride of matrix B along the N dimension (column stride).\n    stride_cm\
    \ : int\n        Stride of matrix C along the M dimension (row stride).\n    stride_cn\
    \ : int\n        Stride of matrix C along the N dimension (column stride).\n \
    \   BLOCK_M : tl.constexpr\n        Compile-time constant defining the height\
    \ of the blocks to be processed from\n        matrices A and C.\n    BLOCK_N :\
    \ tl.constexpr\n        Compile-time constant defining the width of the blocks\
    \ to be processed from\n        matrices B and C.\n    BLOCK_K : tl.constexpr\n\
    \        Compile-time constant defining the width of the block from matrix A and\n\
    \        the height of the block from matrix B (common dimension for dot product).\n\
    \    FLOAT16_OUTPUT : tl.constexpr\n        Compile-time boolean constant. If\
    \ True, the output matrix C will be cast\n        to float16 before storing. Otherwise,\
    \ it will be stored in the compute\n        precision (e.g., float32).\n    USE_TMA_EPILOGUE\
    \ : tl.constexpr\n        Compile-time boolean constant. If True, the epilogue\
    \ (storing the result C)\n        will use Tensor Memory Access (TMA) operations\
    \ via `tl.make_block_ptr`\n        and `tl.store`. If False, it will use a more\
    \ traditional epilogue by\n        calculating destination pointers manually with\
    \ `tl.arange` and `tl.store`.\n    \"\"\"\n    # Your code here\n\n"
  source_code: null
source_file_path: []
target_kernel_functions:
- matmul_no_scf_kernel
