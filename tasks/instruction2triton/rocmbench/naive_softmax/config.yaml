compile_command:
- python -c "import ast; ast.parse(open('naive_softmax.py').read())"
correctness_command:
- pytest -vv -x --maxfail=1 naive_softmax.py -k "not test_performance and not test_save_performance_results"
performance_command:
- pytest -vv -x --maxfail=1 naive_softmax.py -k "test_performance or test_save_performance_results"
task_type: instruction2triton
task_result_template: task_result_template.yaml
prompt:
  cheatsheet: null
  instructions: "\nYou are an expert in triton programming language. You will be given\
    \ the function definition for the `softmax_kernel_naive` kernel. Your task is\
    \ to complete the kernel code. Only complete the kernel code in the function definition,\
    \ DONT remove any python imports or helper utils in the instruction/code provided,\
    \ DONT change/interfere with the provided function definition and parameter list.\n\
    \nThis kernel, `softmax_kernel_naive`,  naive softmax operation.\n\n**Your objective\
    \ is to implement the body of  the kernel `softmax_kernel_naive`.**\n\nYou must\
    \ ensure that:\n1.  All arguments received by `softmax_kernel_naive` are kept\
    \ intact and not modified.\n2. Provide your final code in ```python code block.\
    \ \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `softmax_kernel_naive`\
    \ and relevant helper utilities are provided in the context below. You only need\
    \ to complete the code for `softmax_kernel_naive` whilst keeping other things\
    \ intact. DONT remove Imports and HELPER utils.\n\n########################################\
    \ Imports ########################################\nimport pytest\nimport torch\n\
    from torch.testing import assert_close\n\nimport triton\nimport triton.language\
    \ as tl\n\ndtype_mapping = {\n    'float16': torch.float16,\n    'float32': torch.float32,\n\
    }\n######################################## Imports ########################################\n\
    \n\n @triton.jit\ndef softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols,\
    \ BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Computes the softmax function over\
    \ the last dimension of a 2D input tensor.\n\n    Each program instance is responsible\
    \ for processing a single row of the input tensor.\n\n    Parameters\n    ----------\n\
    \    in_ptr\n        Pointer to the 2D input tensor.\n    output_ptr\n       \
    \ Pointer to the 2D output tensor where the result is stored. The dimensions\n\
    \        of this tensor should match the input tensor.\n    row_stride\n     \
    \   The number of elements to skip in memory to move from the start of one\n \
    \       row to the start of the next. This is used to correctly index into the\n\
    \        input and output tensors.\n    n_cols\n        The size of the last dimension\
    \ of the tensor (i.e., the number of columns\n        in each row).\n    BLOCK_SIZE\
    \ : tl.constexpr\n        A compile-time constant that defines the size of the\
    \ data block that each\n        instance processes in a single operation. This\
    \ is used to tile the\n        computation over the columns of a row.\n    \"\"\
    \"    # Each program instance processes a single row of the input tensor.\n  \
    \  # 1. Get the row index\n    row_idx = tl.program_id(axis=0)\n\n    # 2. Compute\
    \ offsets for the current row.\n    # The naive kernel assumes that the number\
    \ of columns is a power of 2.\n    # and that `BLOCK_SIZE` is equal to `n_cols`.\n\
    \    row_start_ptr = in_ptr + row_idx * row_stride\n    col_offsets = tl.arange(0,\
    \ BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n\n    # 3. Load the\
    \ row into a 1D block.\n    # `mask` is used to handle rows where `n_cols` is\
    \ not a power of 2.\n    mask = col_offsets < n_cols\n    # load the input data;\
    \ use `other=-float('inf')` to ensure correct `max` calculation\n    row = tl.load(input_ptrs,\
    \ mask=mask, other=-float('inf'))\n\n    # 4. Compute softmax.\n    #    a. Subtract\
    \ the maximum value for numerical stability.\n    row_minus_max = row - tl.max(row,\
    \ axis=0)\n    #    b. Compute the numerator.\n    numerator = tl.exp(row_minus_max)\n\
    \    #    c. Compute the denominator.\n    denominator = tl.sum(numerator, axis=0)\n\
    \    #    d. Normalize.\n    softmax_output = numerator / denominator\n\n    #\
    \ 5. Write the result to the output tensor.\n    output_row_start_ptr = output_ptr\
    \ + row_idx * row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n\
    \    tl.store(output_ptrs, softmax_output, mask=mask)\n\n"
  source_code: null
source_file_path: []
target_kernel_functions:
- softmax_kernel_naive
