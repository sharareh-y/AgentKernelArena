compile_command:
- python -c "import ast; ast.parse(open('test_flashattention_fwd.py').read())"
correctness_command:
- pytest -vv -x --maxfail=1 test_flashattention_fwd.py -k "not test_performance and
  not test_save_performance_results"
performance_command:
- pytest -vv -x --maxfail=1 test_flashattention_fwd.py -k "test_performance or test_save_performance_results"
task_type: instruction2triton
task_result_template: task_result_template.yaml
prompt:
  cheatsheet: null
  instructions: "\nYou are an expert in triton programming language. You will be given\
    \ the function definition for the `flash_fwd_kernel` kernels. Your task is to\
    \ complete the kernel code. Only complete the kernel code in the function definition,\
    \ DONT remove any python imports or helper utils in the instruction/code provided,\
    \ DONT change/interfere with the provided function definition and parameter list.\n\
    \nThis kernel, `flash_fwd_kernel`,  performs forward pass of the FlashAttention\
    \ algorithm\n**Your objective is to implement the body of both the kernels `flash_fwd_kernel`.**\n\
    \nYou must ensure that:\n1.  All arguments received by `flash_fwd_kernel` are\
    \ kept intact and not modified.\n2. Provide you final code in ```python code block.\
    \ \nExample:\n```python\n<YOUR-CODE-HERE>\n```\nThe full definitions for `flash_fwd_kernel`\
    \ and relevant helper utilities are provided in the context below. You only need\
    \ to complete the code for `flash_fwd_kernel` whilst keeping other things intact.\
    \ DONT remove Imports and HELPER utils.\n\n########################################\
    \ Imports ######################################## \n\n# import numpy as np\n\
    import pytest\nimport torch\n\nimport triton\nimport triton.language as tl\n########################################\
    \ Imports ######################################## \n\n\n@triton.jit\ndef flash_fwd_kernel(\n\
    \    Q, K, V, sm_scale,  # Input tensors and softmax scale\n    L, M,  # Intermediate\
    \ tensors for online softmax\n    Out,  # Output tensor\n    stride_qz, stride_qh,\
    \ stride_qm, stride_qk,  # Strides for Q\n    stride_kz, stride_kh, stride_kn,\
    \ stride_kk,  # Strides for K\n    stride_vz, stride_vh, stride_vk, stride_vn,\
    \  # Strides for V\n    stride_oz, stride_oh, stride_om, stride_on,  # Strides\
    \ for Out\n    Z, H, N_CTX, D0,  # Tensor dimensions\n    BLOCK_M: tl.constexpr,\
    \ BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr  # Block sizes\n):\n    \"\
    \"\"\n    Computes the forward pass of FlashAttention.\n\n    Args:\n        Q\
    \ (Tensor): Query tensor with shape (Z, H, N_CTX, D_HEAD). This kernel expects\
    \ a pointer to the beginning of this tensor.\n        K (Tensor): Key tensor with\
    \ shape (Z, H, N_CTX, D_HEAD). This kernel expects a pointer to the beginning\
    \ of this tensor.\n        V (Tensor): Value tensor with shape (Z, H, N_CTX, D_HEAD).\
    \ This kernel expects a pointer to the beginning of this tensor.\n        sm_scale\
    \ (float): Scaling factor applied to the QK^T product before softmax. Typically\
    \ 1/sqrt(D_HEAD).\n        L (Tensor): Output tensor of shape (Z*H, N_CTX) used\
    \ to store the row-wise sum of `exp(scores - max_scores)` for the online softmax\
    \ calculation.\n                    It acts as the normalizer `l_i` in the FlashAttention\
    \ algorithm.\n        M (Tensor): Output tensor of shape (Z*H, N_CTX) used to\
    \ store the row-wise maximum of QK^T scores (`m_i` in FlashAttention) for numerically\
    \ stable online softmax.\n        Out (Tensor): Output tensor of shape (Z, H,\
    \ N_CTX, D_HEAD) where the attention output is stored.\n        stride_qz (int):\
    \ Stride for the Z (batch) dimension of the Q tensor, in terms of number of elements.\n\
    \        stride_qh (int): Stride for the H (head) dimension of the Q tensor, in\
    \ terms of number of elements.\n        stride_qm (int): Stride for the M (query\
    \ sequence length, N_CTX) dimension of the Q tensor, in terms of number of elements.\n\
    \        stride_qk (int): Stride for the K (head dimension, D_HEAD) dimension\
    \ of the Q tensor, in terms of number of elements.\n        stride_kz (int): Stride\
    \ for the Z (batch) dimension of the K tensor.\n        stride_kh (int): Stride\
    \ for the H (head) dimension of the K tensor.\n        stride_kn (int): Stride\
    \ for the N (key sequence length, N_CTX) dimension of the K tensor.\n        stride_kk\
    \ (int): Stride for the K (head dimension, D_HEAD) dimension of the K tensor.\n\
    \        stride_vz (int): Stride for the Z (batch) dimension of the V tensor.\n\
    \        stride_vh (int): Stride for the H (head) dimension of the V tensor.\n\
    \        stride_vk (int): Stride for the K (key/value sequence length, N_CTX)\
    \ dimension of the V tensor. (Note: `_vk` here refers to the sequence dim for\
    \ V).\n        stride_vn (int): Stride for the N (head dimension, D_HEAD) dimension\
    \ of the V tensor. (Note: `_vn` here refers to the head dim for V).\n        stride_oz\
    \ (int): Stride for the Z (batch) dimension of the Out tensor.\n        stride_oh\
    \ (int): Stride for the H (head) dimension of the Out tensor.\n        stride_om\
    \ (int): Stride for the M (query sequence length, N_CTX) dimension of the Out\
    \ tensor.\n        stride_on (int): Stride for the N (head dimension, D_HEAD)\
    \ dimension of the Out tensor.\n        Z (int): Batch size.\n        H (int):\
    \ Number of attention heads.\n        N_CTX (int): Sequence length (context length).\
    \ Assumed to be the same for Q, K, and V for simplicity in this kernel's structure,\
    \ particularly for causal masking and L, M storage.\n        D0 (int): This parameter\
    \ represents the sequence length dimension (N_CTX) for a single head's data matrix.\
    \ It is used in `tl.make_block_ptr` for the `shape` argument's first dimension\
    \ when viewing a head's Q, K, or V data. It should be equal to N_CTX.\n      \
    \  BLOCK_M (tl.constexpr): The size of the block along the query sequence length\
    \ dimension (M). Queries are processed in blocks of this size.\n        BLOCK_DMODEL\
    \ (tl.constexpr): The head dimension size (D_HEAD). The kernel processes the full\
    \ head dimension.\n        BLOCK_N (tl.constexpr): The size of the block along\
    \ the key/value sequence length dimension (N). Keys and values are loaded and\
    \ processed in blocks of this size.\n    \"\"\"\n    # Your code here\n\n"
  source_code: null
source_file_path: []
target_kernel_functions:
- flash_fwd_kernel
