compile_command:
- python fused_rwkv6_kernel.py
correctness_command:
- python fused_rwkv6_kernel_perf.py
performance_command:
- tb_eval -f fused_rwkv6_kernel.py -o fused_rwkv6_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `fused_recurrent_rwkv6_fwd_kernel` is designed\
    \ for efficient execution of a specific recurrent neural network operation used\
    \ in the RWKV-6 model. This kernel processes the input tensors `q`, `k`, `v`,\
    \ `w`, and `u` to perform fused operations that involve recurrent processing over\
    \ time `T`. The kernel can optionally use an initial hidden state (`USE_INITIAL_STATE`)\
    \ and store the final hidden state (`STORE_FINAL_STATE`). It supports reversing\
    \ the input sequence (`REVERSE`). The main compute loop iterates over the sequence\
    \ length `T` to perform recurrent updates by loading slices of `k`, `v`, and applying\
    \ operations with `b_w` and `b_u` to update the hidden state and compute the output\
    \ `b_o`. The function `FusedRecurrentRWKV6Function` is an autograd function that\
    \ encapsulates the forward pass using this Triton kernel, setting up grid parameters\
    \ (`BK`, `BV`, `NK`, `NV`) for parallel execution. It manages memory allocation\
    \ for outputs and optionally for final hidden states, while also saving necessary\
    \ tensors for backward pass in PyTorch's autograd. The `fused_recurrent_rwkv6`\
    \ function acts as a user-facing API to perform this computation, with the ability\
    \ to scale `q` by `scale` and return both the output tensor and optionally the\
    \ final hidden state.\n            \nThe test code is:\n\n\nimport torch\n\ndef\
    \ test_fused_recurrent_rwkv6():\n    # Define input dimensions\n    B, H, T, K,\
    \ V = 2, 3, 4, 8, 8\n\n    # Create random input tensors\n    r = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, device='cuda')\n    k = torch.randn(B, H, T, K,\
    \ dtype=torch.float32, device='cuda')\n    v = torch.randn(B, H, T, V, dtype=torch.float32,\
    \ device='cuda')\n    w = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n\
    \    u = torch.randn(H, K, dtype=torch.float32, device='cuda')\n\n    # Prepare\
    \ a dictionary to store results\n    results = {}\n\n    # Test without initial\
    \ state, without final state, forward\n    o, final_state = fused_recurrent_rwkv6(r,\
    \ k, v, w, u, scale=0.5, initial_state=None, output_final_state=False)\n    results[\"\
    test_case_1\"] = {\"output\": o.shape, \"final_state\": final_state}\n\n    #\
    \ Test with initial state, without final state, forward\n    initial_state = torch.randn(B,\
    \ H, K, V, dtype=torch.float32, device='cuda')\n    o, final_state = fused_recurrent_rwkv6(r,\
    \ k, v, w, u, scale=0.5, initial_state=initial_state, output_final_state=False)\n\
    \    results[\"test_case_2\"] = {\"output\": o.shape, \"final_state\": final_state}\n\
    \n    # Test without initial state, with final state, forward\n    o, final_state\
    \ = fused_recurrent_rwkv6(r, k, v, w, u, scale=0.5, initial_state=None, output_final_state=True)\n\
    \    results[\"test_case_3\"] = {\"output\": o.shape, \"final_state\": final_state.shape}\n\
    \n    # Test with initial state, with final state, forward\n    o, final_state\
    \ = fused_recurrent_rwkv6(r, k, v, w, u, scale=0.5, initial_state=initial_state,\
    \ output_final_state=True)\n    results[\"test_case_4\"] = {\"output\": o.shape,\
    \ \"final_state\": final_state.shape}\n\n    return results\n\nresult_gold = test_fused_recurrent_rwkv6()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fused_rwkv6_kernel.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_rwkv6_kernel
