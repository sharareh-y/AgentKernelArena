compile_command:
- python batched_vecmat_mult.py
correctness_command:
- python batched_vecmat_mult_perf.py
performance_command:
- tb_eval -f batched_vecmat_mult.py -o batched_vecmat_mult_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The code implements a batched vector-matrix multiplication using Triton.\
    \ The primary kernel function is `batched_vecmat_kernel`, which performs the core\
    \ operations. This kernel function is designed to handle inputs `A` (shape [dim_m,\
    \ dim_k]) and `B` (shape [dim_m, dim_n, dim_k]), where the goal is to compute\
    \ the vector-matrix product for each vector-matrix pair defined by corresponding\
    \ slices in A and B. The kernel operates in blocks, defined by `block_m`, `block_n`,\
    \ and `block_k`, which partition the matrices for parallel processing.\n\n   \
    \ Inside the kernel, `m_index` and `n_index` are the block indices for the current\
    \ program instance, determined by Triton's grid structure. `output_tile` computes\
    \ the output matrix indices that this block will modify. The `vecmat` variable\
    \ accumulates the results for the block. The loop iterates over `k_blocks`, partitioning\
    \ the K dimension, loading slices of `A` and `B`, broadcasting `a` to match `b`'s\
    \ shape, and computing the dot product for the block using `tl.sum`. The result\
    \ for each block is stored using `tl.store`.\n\n    The `batched_vecmat` function\
    \ initializes tensors A, B, and the output on the GPU using PyTorch. It checks\
    \ that the dimensions are divisible by the respective block sizes and computes\
    \ the grid dimensions. The Triton kernel is then launched over this grid, passing\
    \ the initialized tensors and configuration parameters, ensuring that the vector-matrix\
    \ multiplication is performed for all specified blocks in the input dimensions,\
    \ and the result is stored in `output`.\n    \nThe test code is:\n\n\n# Function\
    \ 3: Test the correctness of the Triton kernel against the reference implementation\n\
    def test_vecmat():\n    M, N, K = 128, 128, 128\n    block_m, block_n, block_k\
    \ = 16, 32, 64\n\n    results = {}\n    output = batched_vecmat(M, N, K, block_m,\
    \ block_n, block_k)\n    results['test_case_1'] = output.clone()  # Store first\
    \ result\n\n    output2 = batched_vecmat(M, N, K, block_m, block_n, block_k, num_warps=2,\
    \ num_stages=2)\n    results['test_case_2'] = output2.clone()  # Store second\
    \ result with different key\n\n    return results\n\n# Run the test\nresult_gold\
    \ = test_vecmat()\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ batched_vecmat_mult.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- batched_vecmat_mult
