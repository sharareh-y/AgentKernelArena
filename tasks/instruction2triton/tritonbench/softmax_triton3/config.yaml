compile_command:
- python softmax_triton3.py
correctness_command:
- python softmax_triton3_perf.py
performance_command:
- tb_eval -f softmax_triton3.py -o softmax_triton3_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The given Triton code defines a GPU-accelerated softmax operation\
    \ through two functions: `softmax_kernel` and `softmax`.\n            - `softmax_kernel(output_ptr,\
    \ input_ptr, row_stride, n_cols, mask_ptr, BLOCK_SIZE)`: This is a Triton JIT-compiled\
    \ function designed to calculate the softmax of rows in a 2D tensor. It first\
    \ identifies the row index using `tl.program_id(0)` and computes the starting\
    \ pointer for the row. `col_offsets` and `input_ptrs` define the block of columns\
    \ to process. The function loads the input row into SRAM, adjusts it by subtracting\
    \ the maximum value for numerical stability, and optionally adds a mask if `mask_ptr`\
    \ is not `None`. It computes the exponentials, sums them to derive the denominator,\
    \ and divides each element to produce the softmax output, which is stored back\
    \ in the output tensor.\n            - `softmax(input: torch.Tensor, mask: torch.Tensor\
    \ = None, dim=-1) -> torch.Tensor`: This is the wrapper function that sets up\
    \ the Triton kernel. It first validates the input tensor and mask dimensions and\
    \ ensures the operation is along the last dimension. The input tensor is reshaped\
    \ into 2D if necessary. Depending on the size of the input, the function selects\
    \ grid and block sizes, choosing an appropriate number of warps to optimize performance.\
    \ For large row counts, it uses lambda functions to calculate grid dimensions\
    \ dynamically. It then invokes `softmax_kernel` with the calculated parameters\
    \ and returns the resulting tensor containing softmax values.\n            \n\
    The test code is:\n\n\ndef test_softmax():\n    # Test Case 1: Small matrix without\
    \ mask\n    input_tensor_1 = torch.randn(32, 128, dtype=torch.float16, device='cuda')\n\
    \    output_tensor_1 = softmax(input_tensor_1)\n\n    # Test Case 2: Small matrix\
    \ with mask\n    input_tensor_2 = torch.randn(32, 128, dtype=torch.float16, device='cuda')\n\
    \    mask_tensor_2 = torch.randint(0, 2, (32, 128), dtype=torch.float16, device='cuda')\n\
    \    output_tensor_2 = softmax(input_tensor_2, mask=mask_tensor_2)\n\n    # Test\
    \ Case 3: Larger matrix without mask\n    input_tensor_3 = torch.randn(1024, 512,\
    \ dtype=torch.float16, device='cuda')\n    output_tensor_3 = softmax(input_tensor_3)\n\
    \n    # Test Case 4: Larger matrix with mask\n    input_tensor_4 = torch.randn(1024,\
    \ 512, dtype=torch.float16, device='cuda')\n    mask_tensor_4 = torch.randint(0,\
    \ 2, (1024, 512), dtype=torch.float16, device='cuda')\n    output_tensor_4 = softmax(input_tensor_4,\
    \ mask=mask_tensor_4)\n\n    # Test Case 5: Very large matrix without mask\n \
    \   input_tensor_5 = torch.randn(100000, 256, dtype=torch.float16, device='cuda')\n\
    \    output_tensor_5 = softmax(input_tensor_5)\n\n    # Test Case 6: Very large\
    \ matrix with mask\n    input_tensor_6 = torch.randn(100000, 256, dtype=torch.float16,\
    \ device='cuda')\n    mask_tensor_6 = torch.randint(0, 2, (100000, 256), dtype=torch.float16,\
    \ device='cuda')\n    output_tensor_6 = softmax(input_tensor_6, mask=mask_tensor_6)\n\
    \n    return {\n        \"test_case_1\": output_tensor_1,\n        \"test_case_2\"\
    : output_tensor_2,\n        \"test_case_3\": output_tensor_3,\n        \"test_case_4\"\
    : output_tensor_4,\n        \"test_case_5\": output_tensor_5,\n        \"test_case_6\"\
    : output_tensor_6\n    }\n\n# Run the test function\nresult_gold = test_softmax()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py softmax_triton3.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- softmax_triton3
