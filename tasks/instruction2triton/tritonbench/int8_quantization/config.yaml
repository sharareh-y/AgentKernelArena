compile_command:
- python int8_quantization.py
correctness_command:
- python int8_quantization_perf.py
performance_command:
- tb_eval -f int8_quantization.py -o int8_quantization_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided code includes Triton kernels designed to convert query and\
    \ key matrices into int8 format, suitable for applications requiring low precision\
    \ and optimized memory usage. The process is encapsulated in two specialized kernels,\
    \ q_kernel_per_block_int8 and k_kernel_per_block_int8, each responsible for quantizing\
    \ a block of the respective matrices.\n\n        The q_kernel_per_block_int8 function\
    \ loads a block of the query matrix, normalizes it by a scaling factor derived\
    \ from its max absolute value, quantizes to int8 while rounding, and stores both\
    \ the quantized data and the scaling factor for later reconstruction. The factor\
    \ ensures precision is maintained post quantization.\n\n        Similarly, the\
    \ k_kernel_per_block_int8 processes key matrices, performing an identical operation\
    \ tailored for the key matrix dimensions and blocks.\n\n        Both functions\
    \ are invoked by per_block_int8, which handles the necessary reshaping of input\
    \ matrices based on dimensionality. It initializes empty int8 and scaling tensors,\
    \ calculates the grid configuration for kernel execution, and launches the respective\
    \ Triton kernels with appropriate parameters.\n\n        Inputs and Outputs:\n\
    \        - q (torch.Tensor): Input query matrix.\n        - k (torch.Tensor):\
    \ Input key matrix.\n        - BLKQ, BLKK (int): Block sizes for the kernels'\
    \ operation.\n        - q_int8, k_int8 (torch.Tensor): Outputs holding the int8\
    \ representations.\n        - q_scale, k_scale (torch.Tensor): Output scaling\
    \ factors for precision recovery.\n\n        Key Parameters and Code Flow:\n \
    \       - The Triton kernels utilize program_id to distinguish thread execution\
    \ contexts, calculate offsets for memory access, and utilize block-wise processing\
    \ via arithmetic with block sizes BLKQ and BLKK.\n        - Input tensors are\
    \ reshaped for uniform handling irrespective of the original dimensions (3D or\
    \ 4D tensors).\n        - The scaling ensures that the largest absolute value\
    \ in each block maps to the range of representable int8 values, with the quantization\
    \ mimicking nearest integer rounding.\n        - Overall, the implementation serves\
    \ to efficiently convert matrices to a space-efficient int8 format while storing\
    \ necessary metadata (scales) for accurate de-quantization.\n    \nThe test code\
    \ is:\n\n\nimport torch\n\n# Test function for per_block_int8\ndef test_per_block_int8():\n\
    \    # Define the dimensions for the test\n    B = 2  # Batch size\n    L = 256\
    \  # Sequence length\n    C = 64  # Feature dimension\n\n    # Create random input\
    \ tensors for q and k\n    q = torch.randn((B, L, C), dtype=torch.float32, device='cuda')\n\
    \    k = torch.randn((B, L, C), dtype=torch.float32, device='cuda')\n\n    # Call\
    \ the per_block_int8 function\n    q_int8, q_scale, k_int8, k_scale = per_block_int8(q,\
    \ k)\n\n    # Store the results in a dictionary\n    results = {\n        \"test_case_1\"\
    : (q_int8.clone(), q_scale.clone(), k_int8.clone(), k_scale.clone()),\n    }\n\
    \n    # Additional test cases with varied input dimensions\n    B_new = 1\n  \
    \  L_new = 512\n    C_new = 128\n\n    q_new = torch.randn((B_new, L_new, C_new),\
    \ dtype=torch.float32, device='cuda')\n    k_new = torch.randn((B_new, L_new,\
    \ C_new), dtype=torch.float32, device='cuda')\n\n    q_int8_new, q_scale_new,\
    \ k_int8_new, k_scale_new = per_block_int8(q_new, k_new)\n    results[\"test_case_2\"\
    ] = (q_int8_new.clone(), q_scale_new.clone(), k_int8_new.clone(), k_scale_new.clone())\n\
    \n    return results\n\n# Run the test\nresult_gold = test_per_block_int8()\n\n\
    \nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py int8_quantization.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int8_quantization
