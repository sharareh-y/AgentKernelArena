compile_command:
- python triton_matmul.py
correctness_command:
- python triton_matmul_perf.py
performance_command:
- tb_eval -f triton_matmul.py -o triton_matmul_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided Triton kernel, `matmul_kernel`, is a specialized GPU matrix\
    \ multiplication operation. \n        It employs a blocked tiling strategy for\
    \ efficient computation of the result matrix `c` from input matrices `a` and `b`.\
    \ \n        Within this kernel, operations are parallelized across blocks defined\
    \ by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K. \n        These blocks allow\
    \ the kernel to load sub-matrices, perform computations, and manage memory more\
    \ efficiently.\n\n        The kernel begins by computing indices for thread execution,\
    \ segmenting the operation across various program IDs derived from the grid dimensions.\
    \ \n        For each thread block, it computes offsets `offs_am`, `offs_bn`, and\
    \ `offs_k` to read data from the input matrices.\n\n        In a loop iterating\
    \ over slices of the K dimension, sub-matrices are loaded using `tl.load` with\
    \ masks to handle boundary conditions. \n        These matrices are then multiplied\
    \ using `tl.dot`, accumulating results in a local accumulator. \n        Memory\
    \ access patterns are optimized using `tl.max_contiguous` and `tl.multiple_of`\
    \ to align data in cache-friendly ways.\n\n        The function finally writes\
    \ the accumulated results to the output matrix `c`, with care taken to respect\
    \ bounds and using conditional storage via `tl.store`.\n\n        The `matmul`\
    \ function wraps this kernel, preparing inputs and meta-parameters based on the\
    \ matrix data types and dimensions. \n        It enforces input compatibility,\
    \ establishes execution grid dimensions, and sets device memory for output. \n\
    \        Configuration parameters such as BLOCK_SIZE_M, num_stages, and num_warps\
    \ are determined per data type, \n        ensuring optimal kernel execution tailored\
    \ for either float16 or Triton's experimental float8 types.\n    \nThe test code\
    \ is:\n\n\nimport torch\n\n# Test for matmul\ndef test_matmul():\n    results\
    \ = {}\n    M, K, N = 256, 128, 256\n\n    # Test case 1: torch.float16\n    a\
    \ = torch.randn((M, K), dtype=torch.float16, device='cuda')\n    b = torch.randn((K,\
    \ N), dtype=torch.float16, device='cuda')\n    c = matmul(a, b)\n    results['test_case_1']\
    \ = c\n\n    return results\n\n# Run all tests\nresult_gold = test_matmul()\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py triton_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- triton_matmul
