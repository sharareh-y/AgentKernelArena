compile_command:
- python vector_addition.py
correctness_command:
- python vector_addition_perf.py
performance_command:
- tb_eval -f vector_addition.py -o vector_addition_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided code defines a Triton kernel called `add_kernel` and\
    \ a wrapper function `add` to perform element-wise addition of two CUDA tensors.\
    \ `add_kernel` is a Triton JIT-compiled function that processes vectors in blocks\
    \ of a specified size (`BLOCK_SIZE`). The kernel function takes pointers to the\
    \ input vectors (`x_ptr` and `y_ptr`), an output vector pointer (`output_ptr`),\
    \ and the number of elements (`n_elements`). It computes the sum of the elements\
    \ from the two input vectors and stores the result in the output vector. \n\n\
    \            The kernel uses `tl.program_id(axis=0)` to determine the block index\
    \ for a 1D launch grid. `block_start` is calculated using the block index and\
    \ `BLOCK_SIZE`, and `offsets` are computed for accessing elements within this\
    \ block. A mask is created to handle out-of-bounds accesses, ensuring operations\
    \ only occur on valid elements. `tl.load` and `tl.store` functions are utilized\
    \ with the mask to safely load input elements and store the computed output.\n\
    \n            The `add` function serves as a wrapper, ensuring all tensors are\
    \ on the CUDA device, calculating the total number of elements, and defining a\
    \ grid based on the number of elements and `BLOCK_SIZE`. It then launches the\
    \ kernel with the computed grid. The function returns the output tensor containing\
    \ the summed elements.\n            \nThe test code is:\n\n\ndef test_add():\n\
    \    torch.manual_seed(0)\n    size = 98432\n    x = torch.rand(size, device='cuda')\n\
    \    y = torch.rand(size, device='cuda')\n    \n    # Test case 1\n    output_triton_1\
    \ = add(x, y)\n    \n    # Test case 2\n    size_2 = 1024\n    x_2 = torch.rand(size_2,\
    \ device='cuda')\n    y_2 = torch.rand(size_2, device='cuda')\n    output_triton_2\
    \ = add(x_2, y_2)\n    \n    # Test case 3\n    size_3 = 2048\n    x_3 = torch.rand(size_3,\
    \ device='cuda')\n    y_3 = torch.rand(size_3, device='cuda')\n    output_triton_3\
    \ = add(x_3, y_3)\n    \n    # Test case 4\n    size_4 = 4096\n    x_4 = torch.rand(size_4,\
    \ device='cuda')\n    y_4 = torch.rand(size_4, device='cuda')\n    output_triton_4\
    \ = add(x_4, y_4)\n    \n    results = {\n        \"test_case_1\": output_triton_1,\n\
    \        \"test_case_2\": output_triton_2,\n        \"test_case_3\": output_triton_3,\n\
    \        \"test_case_4\": output_triton_4\n    }\n    \n    return results\n\n\
    result_gold = test_add()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ vector_addition.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- vector_addition
