compile_command:
- python layer_norm_fwd.py
correctness_command:
- python layer_norm_fwd_perf.py
performance_command:
- tb_eval -f layer_norm_fwd.py -o layer_norm_fwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The `_layer_norm_fwd_1pass_kernel` function is a Triton-based CUDA kernel\
    \ for computing layer normalization in a single pass. It is capable of handling\
    \ several advanced operations: applying dropout, adding biases, and optionally\
    \ handling residual connections or additional input tensors (`X1`, `W1`, `B1`).\
    \ The function operates over 2D tensors `X` and `Y` with dimensions MxN. The main\
    \ inputs are `X`, `Y`, `W` (weights), and `B` (biases). It supports optional components\
    \ like `RESIDUAL`, `X1`, `W1`, `B1` and involves additional computations like\
    \ scaling with `ROWSCALE`, random dropout determined by `SEEDS`, and storing results\
    \ in `DROPOUT_MASK`. The computationally intensive parts are the normalization\
    \ steps, which involve calculating the mean and variance (except in RMS norm mode),\
    \ and applying weights and biases to produce the output. This is all highly parallelized\
    \ for efficiency on GPUs. A collection of configurations and heuristics helps\
    \ optimize the kernel's performance based on input size and features used. Outputs\
    \ include normalized outputs stored in `Y` (and optionally `Y1`), computed mean\
    \ and inverse standard deviation (for non-RMS normalization), adjusted residuals\
    \ if `residual_out` is specified, as well as dropout seeds and masks.\n    \n\
    The test code is:\n\n\nimport torch\n\ndef test_layer_norm_fwd():\n    # Define\
    \ the input parameters\n    M, N = 64, 128  # Example dimensions\n    eps = 1e-5\n\
    \    dropout_p = 0.1\n\n    # Create random input tensors\n    x = torch.randn(M,\
    \ N, device='cuda', dtype=torch.float32)\n    weight = torch.randn(N, device='cuda',\
    \ dtype=torch.float32)\n    bias = torch.randn(N, device='cuda', dtype=torch.float32)\n\
    \    residual = torch.randn(M, N, device='cuda', dtype=torch.float32)\n    x1\
    \ = torch.randn(M, N, device='cuda', dtype=torch.float32)\n    weight1 = torch.randn(N,\
    \ device='cuda', dtype=torch.float32)\n    bias1 = torch.randn(N, device='cuda',\
    \ dtype=torch.float32)\n    rowscale = torch.randn(M, device='cuda', dtype=torch.float32)\n\
    \n    results = {}\n\n    # Test case 1: Basic layer norm\n    results['test_case_1']\
    \ = _layer_norm_fwd(x, weight, bias, eps)\n\n    # Test case 2: Layer norm with\
    \ residual\n    results['test_case_2'] = _layer_norm_fwd(x, weight, bias, eps,\
    \ residual=residual)\n\n    # Test case 3: Layer norm with additional input tensors\n\
    \    results['test_case_3'] = _layer_norm_fwd(x, weight, bias, eps, x1=x1, weight1=weight1,\
    \ bias1=bias1)\n\n    # Test case 4: Layer norm with dropout\n    results['test_case_4']\
    \ = _layer_norm_fwd(x, weight, bias, eps, dropout_p=dropout_p, return_dropout_mask=True)\n\
    \n    # Test case 5: Layer norm with row scaling\n    results['test_case_5'] =\
    \ _layer_norm_fwd(x, weight, bias, eps, rowscale=rowscale)\n\n    return results\n\
    \nresult_gold = test_layer_norm_fwd()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ layer_norm_fwd.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- layer_norm_fwd
