compile_command:
- python rotary_transform_ops.py
correctness_command:
- python rotary_transform_ops_perf.py
performance_command:
- tb_eval -f rotary_transform_ops.py -o rotary_transform_ops_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel 'rotary_kernel' applies rotary positional encoding\
    \ on the input tensor X using cosine (COS) and sine (SIN) tensors. The kernel\
    \ supports both variable and fixed sequence lengths, controlled by IS_VARLEN.\
    \ It processes data in blocks, with dimensions controlled by BLOCK_M and BLOCK_K,\
    \ determining the workload per thread. The kernel adjusts input and output pointers\
    \ based on batch, head, and sequence index calculations. The interleaved data\
    \ layout is supported via INTERLEAVED, affecting how input data is accessed and\
    \ processed. The optional conjugate transformation is controlled by CONJUGATE,\
    \ affecting the sine component. The 'apply_rotary' function is a high-level Python\
    \ wrapper preparing the input tensors and invoking the kernel with appropriate\
    \ grid and block configurations. It handles data type checks, ensures tensor contiguity,\
    \ and manages inplace operations. It calculates grid sizes based on sequence lengths,\
    \ and sets up strides for various tensor dimensions (batch, sequence, heads, headdim).\
    \ The function ensures the kernel operates correctly, given the input tensor shapes\
    \ and specified operation parameters.\n            \nThe test code is:\n\n\nimport\
    \ torch\n\n# Define the test function\ndef test_apply_rotary():\n    results =\
    \ {}\n\n    # Test case 1: Basic test with fixed sequence length\n    x = torch.randn(2,\
    \ 4, 3, 8, device='cuda', dtype=torch.float32)\n    cos = torch.randn(4, 4, device='cuda',\
    \ dtype=torch.float32)\n    sin = torch.randn(4, 4, device='cuda', dtype=torch.float32)\n\
    \    seqlen_offsets = 0\n    results['test_case_1'] = apply_rotary(x, cos, sin,\
    \ seqlen_offsets)\n\n    # Test case 2: Variable length sequences with cu_seqlens\n\
    \    cu_seqlens = torch.tensor([0, 2, 4], device='cuda', dtype=torch.int32)\n\
    \    max_seqlen = 4\n    x_varlen = torch.randn(4, 3, 8, device='cuda', dtype=torch.float32)\n\
    \    results['test_case_2'] = apply_rotary(x_varlen, cos, sin, cu_seqlens=cu_seqlens,\
    \ max_seqlen=max_seqlen)\n\n    # Test case 3: Interleaved and conjugate flags\n\
    \    results['test_case_3'] = apply_rotary(x, cos, sin, seqlen_offsets, interleaved=True,\
    \ conjugate=True)\n\n    # Test case 4: seqlen_offsets as a tensor\n    seqlen_offsets_tensor\
    \ = torch.tensor([0, 1], device='cuda', dtype=torch.int32)\n    results['test_case_4']\
    \ = apply_rotary(x, cos, sin, seqlen_offsets_tensor)\n\n    return results\n\n\
    result_gold = test_apply_rotary()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rotary_transform_ops.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rotary_transform_ops
