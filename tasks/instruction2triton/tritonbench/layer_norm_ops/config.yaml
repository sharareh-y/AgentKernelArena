compile_command:
- python layer_norm_ops.py
correctness_command:
- python layer_norm_ops_perf.py
performance_command:
- tb_eval -f layer_norm_ops.py -o layer_norm_ops_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The forward function `_layer_norm_fwd_1pass_kernel` in Triton performs\
    \ layer normalization on a row of a matrix with configurable settings for using\
    \ residual connections, storing residual outputs, RMS normalization, and applying\
    \ a bias. Inputs include tensors for data, weights, and optionally biases and\
    \ residuals. Each row computes the mean and variance, normalizes the data, applies\
    \ weights and biases, and stores the result. The kernel utilizes Triton's `autotune`\
    \ to optimize for various configurations by selecting the number of warps. \n\n\
    \        The backward function `_layer_norm_bwd_kernel` computes the gradients\
    \ of input data, weights, and biases, considering residuals if used. It can also\
    \ recompute the forward pass output when necessary for gradient accuracy. The\
    \ function supports different configurations, such as whether to store additional\
    \ residual gradients or use RMS normalization, and optimizes execution over multiple\
    \ streaming multiprocessors (SMs) by distributing rows evenly. The gradients are\
    \ accumulated across SMs and returned for use in parameter updates. \n\n     \
    \   Both functions rely on a BLOCK_N size determined by input dimensions and memory\
    \ constraints, ensuring the features do not exceed a set threshold. The forward\
    \ and backward functions handle edge cases like differing data types for outputs\
    \ and residuals. Additionally, the functions use autotuning to find the best execution\
    \ configuration for different input sizes and feature settings.\n    \nThe test\
    \ code is:\n\n\ndef test_layer_norm_fwd_bwd():\n    # \u8BBE\u7F6E\u6D4B\u8BD5\
    \u7684\u57FA\u672C\u53C2\u6570\n    M, N = 64, 1024  # 64x1024\u7684\u77E9\u9635\
    \n    x = torch.randn(M, N, dtype=torch.float32, device='cuda')\n    weight =\
    \ torch.randn(N, dtype=torch.float32, device='cuda')\n    bias = torch.randn(N,\
    \ dtype=torch.float32, device='cuda')\n    eps = 1e-6\n\n    results = {}\n\n\
    \    # \u6D4B\u8BD5\u4E0D\u4F7F\u7528 RMS norm\uFF0C\u4E14\u6CA1\u6709\u6B8B\u5DEE\
    \uFF0C\u4E14\u4E0D\u8BA1\u7B97\u8F93\u51FA\n    y, mean, rstd, residual_out =\
    \ _layer_norm_fwd(x, weight, bias, eps, residual=None, is_rms_norm=False)\n  \
    \  results['test_case_1'] = (y, mean, rstd, residual_out)\n\n    dy = torch.randn_like(y)\n\
    \    dx, dw, db, dresidual_in = _layer_norm_bwd(dy, x, weight, bias, eps, mean,\
    \ rstd)\n    results['test_case_2'] = (dx, dw, db, dresidual_in)\n\n    # \u6D4B\
    \u8BD5\u4F7F\u7528 RMS norm\uFF0C\u4E14\u6CA1\u6709\u6B8B\u5DEE\uFF0C\u4E14\u4E0D\
    \u8BA1\u7B97\u8F93\u51FA\n    y, mean, rstd, residual_out = _layer_norm_fwd(x,\
    \ weight, bias, eps, residual=None, is_rms_norm=True)\n    results['test_case_3']\
    \ = (y, mean, rstd, residual_out)\n\n    dy = torch.randn_like(y)\n    dx, dw,\
    \ db, dresidual_in = _layer_norm_bwd(dy, x, weight, bias, eps, mean, rstd, is_rms_norm=True)\n\
    \    results['test_case_4'] = (dx, dw, db, dresidual_in)\n\n    # \u6D4B\u8BD5\
    \u5E26\u6709\u6B8B\u5DEE\u7684\u60C5\u51B5\uFF0C\u4E14\u4E0D\u8BA1\u7B97\u8F93\
    \u51FA\n    residual = torch.randn_like(x)\n    y, mean, rstd, residual_out =\
    \ _layer_norm_fwd(x, weight, bias, eps, residual=residual, is_rms_norm=False)\n\
    \    results['test_case_5'] = (y, mean, rstd, residual_out)\n\n    dy = torch.randn_like(y)\n\
    \    dx, dw, db, dresidual_in = _layer_norm_bwd(dy, x, weight, bias, eps, mean,\
    \ rstd, dresidual=residual, is_rms_norm=False)\n    results['test_case_6'] = (dx,\
    \ dw, db, dresidual_in)\n\n    # \u6D4B\u8BD5\u8BA1\u7B97\u8F93\u51FA\uFF08recompute_output=True\uFF09\
    \n    y, mean, rstd, residual_out = _layer_norm_fwd(x, weight, bias, eps, residual=None,\
    \ is_rms_norm=False)\n    dy = torch.randn_like(y)\n    dx, dw, db, dresidual_in,\
    \ recomputed_y = _layer_norm_bwd(dy, x, weight, bias, eps, mean, rstd, recompute_output=True)\n\
    \    results['test_case_7'] = (dx, dw, db, dresidual_in, recomputed_y)\n\n   \
    \ # \u6D4B\u8BD5\u5E26\u6709\u6B8B\u5DEE\u7684\u60C5\u51B5\uFF0C\u8BA1\u7B97\u8F93\
    \u51FA\n    residual = torch.randn_like(x)\n    y, mean, rstd, residual_out =\
    \ _layer_norm_fwd(x, weight, bias, eps, residual=residual, is_rms_norm=False)\n\
    \    dy = torch.randn_like(y)\n    dx, dw, db, dresidual_in, recomputed_y = _layer_norm_bwd(dy,\
    \ x, weight, bias, eps, mean, rstd, dresidual=residual, recompute_output=True)\n\
    \    results['test_case_8'] = (dx, dw, db, dresidual_in, recomputed_y)\n\n   \
    \ return results\n\nresult_gold = test_layer_norm_fwd_bwd()\n\n# print(result_gold)\n\
    \nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py layer_norm_ops.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- layer_norm_ops
