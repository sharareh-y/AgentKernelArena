compile_command:
- python mean_reduction.py
correctness_command:
- python mean_reduction_perf.py
performance_command:
- tb_eval -f mean_reduction.py -o mean_reduction_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This Triton kernel implementation calculates the mean of a tensor\
    \ across specified dimensions. The kernel, `mean_dim_kernel`, receives tensor\
    \ `X`, a tensor `Mean` to store the output, the dimensions `M` and `N` indicating\
    \ the size, and `BLOCK_M` and `BLOCK_N` defining the block sizes for processing.\
    \ The kernel uses a program ID `pid` to map each computation block to the correct\
    \ row in `X`. It calculates a mask `row_mask` to ensure operations only occur\
    \ within valid tensor bounds.\n\n            The kernel computes the mean across\
    \ a specified dimension by iteratively loading elements from `X` in blocks and\
    \ accumulating their sum in `_mean`. After summing, it divides by `N` to find\
    \ the mean, storing the result in `Mean`.\n\n            The function `dim_compress`\
    \ is used to rearrange the input tensor dimensions for efficient memory access\
    \ patterns. This function takes a tensor `inp` and a list of dimensions `dims`\
    \ to reduce, returning a permuted tensor with these dimensions at the end.\n\n\
    \            The `mean_dim` function is a wrapper that prepares the inputs for\
    \ the kernel. It converts the input tensor `x` to a format compatible with the\
    \ kernel by reducing dimensions specified in `dim`. It calculates `M` as the product\
    \ of remaining dimensions after reduction and `N` as the product of reduction\
    \ dimensions. It creates an empty output tensor `out` with the required shape.\
    \ The kernel is then launched using a grid configuration determined by the `cdiv`\
    \ function, dividing `M` by `BLOCK_M`. After execution, if `keepdim` is `False`,\
    \ the output tensor's reduced dimensions are squeezed out. The function returns\
    \ the resulting tensor.\n            \nThe test code is:\n\n\nimport torch\n\n\
    def test_mean_dim():\n    results = {}\n\n    # Test case 1: Single reduction\
    \ dimension\n    b1 = torch.randn(2, 3, 4, 5, device=\"cuda\")\n    triton_result1\
    \ = mean_dim(b1, 1)\n    results['test_case_1'] = triton_result1\n\n    # Test\
    \ case 2: Multiple reduction dimensions\n    b2 = torch.randn(2, 3, 4, 5, device=\"\
    cuda\")\n    triton_result2 = mean_dim(b2, [1, 2])\n    results['test_case_2']\
    \ = triton_result2\n\n    # Test case 3: Keep dimensions\n    b3 = torch.randn(2,\
    \ 3, 4, 5, device=\"cuda\")\n    triton_result3 = mean_dim(b3, [1, 2], keepdim=True)\n\
    \    results['test_case_3'] = triton_result3\n\n    # Test case 4: Different data\
    \ type\n    b4 = torch.randn(2, 3, 4, 5, device=\"cuda\", dtype=torch.float64)\n\
    \    triton_result4 = mean_dim(b4, [1, 2], dtype=torch.float32)\n    results['test_case_4']\
    \ = triton_result4\n\n    return results\n\nresult_gold = test_mean_dim()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py mean_reduction.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- mean_reduction
