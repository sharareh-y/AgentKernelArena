compile_command:
- python matrix_vector_multip.py
correctness_command:
- python matrix_vector_multip_perf.py
performance_command:
- tb_eval -f matrix_vector_multip.py -o matrix_vector_multip_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton operator code implements a matrix-vector multiplication\
    \ kernel, `mv_kernel`, designed for efficient execution on NVIDIA GPUs. It leverages\
    \ Triton's Just-In-Time (JIT) compilation and auto-tuning features. The kernel\
    \ function multiplies a matrix `A` of size N x M with a vector `B` of size M,\
    \ resulting in an output vector `C` of size N. The kernel is parameterized by\
    \ constants `BLOCK_N` and `BLOCK_M` which determine the tile sizes for the computation,\
    \ allowing for optimal parallel processing. Within `mv_kernel`, a loop iterates\
    \ over the matrix A in blocks of `BLOCK_M` to load sub-matrices and the corresponding\
    \ block from vector B, performing element-wise multiplication and accumulation\
    \ in shared memory. The partial results are then reduced along the rows and stored\
    \ in the output vector `C`. The `mv` function wraps this kernel execution, asserting\
    \ dimension compatibility and managing CUDA resources for launching the kernel\
    \ with calculated grid dimensions.\n            \nThe test code is:\n\n\ndef test_mv():\n\
    \    # \u6D4B\u8BD5\u7528\u4F8B 2: 4x3 \u77E9\u9635\u4E0E 3x1 \u5411\u91CF\u76F8\
    \u4E58\n    A = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0],\
    \ [10.0, 11.0, 12.0]], device='cuda')\n    B = torch.tensor([1.0, 2.0, 3.0], device='cuda')\n\
    \    triton_result_2 = mv(A, B)\n\n    # \u6D4B\u8BD5\u7528\u4F8B 3: 32x16 \u77E9\
    \u9635\u4E0E 16x1 \u5411\u91CF\u76F8\u4E58\n    A = torch.randn(32, 16, device='cuda')\n\
    \    B = torch.randn(16, device='cuda')\n    triton_result_3 = mv(A, B)\n\n  \
    \  return {\n        \"test_case_2\": triton_result_2,\n        \"test_case_3\"\
    : triton_result_3,\n    }\n\nresult_gold = test_mv()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ matrix_vector_multip.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matrix_vector_multip
