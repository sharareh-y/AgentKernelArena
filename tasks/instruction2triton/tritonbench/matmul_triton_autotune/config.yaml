compile_command:
- python matmul_triton_autotune.py
correctness_command:
- python matmul_triton_autotune_perf.py
performance_command:
- tb_eval -f matmul_triton_autotune.py -o matmul_triton_autotune_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton code outlines an implementation for matrix multiplication\
    \ on GPU, enhanced by an autotuning mechanism which optimizes the kernel for various\
    \ GPU configurations. The key computational routine is encapsulated in `matmul_kernel`,\
    \ which multiplies matrices A (MxK) and B (KxN) to produce C (MxN). The kernel\
    \ is decorated with `@triton.autotune` to select optimal parameters dynamically\
    \ based on input dimensions (M, N, K).\n\n            The `matmul` function acts\
    \ as a convenient interface, validating input tensor compatibility, ensuring contiguity,\
    \ and calling the kernel with the right execution grid computed using the `grid`\
    \ lambda function. The result is stored in a new tensor `c` of shape (M, N).\n\
    \n            Core logic inside `matmul_kernel`:\n            - Computes grid\
    \ and block indices to iterate over matrix chunks, using block size constants\
    \ like `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`.\n            - Pointers\
    \ (`a_ptrs`, `b_ptrs`) are computed for accessing matrix blocks, leveraging input\
    \ strides.\n            - An accumulator is initialized to zero and filled via\
    \ dot-product operations across the K dimension.\n            - If specified,\
    \ a leaky ReLU activation is applied, modifying the result where needed.\n   \
    \         - The result matrix C is stored conditionally to handle edge cases on\
    \ matrix boundaries.\n\n            The `leaky_relu` function is conditionally\
    \ applied, performing element-wise operations to modify negative values in the\
    \ accumulator. Triton's use of `tl.where` facilitates this transformation efficiently.\n\
    \n            Overall, this implementation exemplifies efficient, configurable\
    \ GPU-based matrix multiplication suitable for diverse GPU architectures.\n  \
    \          \nThe test code is:\n\n\nimport torch\n\n# Test case 1: Basic matrix\
    \ multiplication without activation\ndef test_matmul():\n    results = {}\n  \
    \  \n    # Test case 1: Basic matrix multiplication without activation\n    M,\
    \ K, N = 128, 64, 256\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    c = matmul(a,\
    \ b)\n    results['test_case_1'] = c\n\n    # Test case 2: Matrix multiplication\
    \ with Leaky ReLU activation\n    M, K, N = 128, 64, 256\n    a = torch.randn((M,\
    \ K), device='cuda', dtype=torch.float16)\n    b = torch.randn((K, N), device='cuda',\
    \ dtype=torch.float16)\n    c = matmul(a, b, activation=\"leaky_relu\")\n    results['test_case_2']\
    \ = c\n\n    # Test case 3: Different matrix sizes\n    M, K, N = 256, 128, 512\n\
    \    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    b = torch.randn((K,\
    \ N), device='cuda', dtype=torch.float16)\n    c = matmul(a, b)\n    results['test_case_3']\
    \ = c\n\n    return results\n\n# Run tests\nresult_gold = test_matmul()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matmul_triton_autotune.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_triton_autotune
