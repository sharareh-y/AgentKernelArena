compile_command:
- python chunk_gate_recurrence.py
correctness_command:
- python chunk_gate_recurrence_perf.py
performance_command:
- tb_eval -f chunk_gate_recurrence.py -o chunk_gate_recurrence_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This Triton kernel module is focused on enabling efficient recurrent\
    \ computation with chunked gate processing. The module consists of two main kernels,\
    \ `_fwd_recurrence` for forward propagation and `_bwd_recurrence` for backward\
    \ propagation, and a high-level interface class, `ChunkGateRecurrent`.\n\n   \
    \     `_fwd_recurrence`:\n        - Purpose: Executes forward pass with recurrent\
    \ updates across multiple blocks of input data.\n        - Parameters:\n     \
    \       - `S` and `d`: Input data and decay factors for each block.\n        \
    \    - `O`: Output tensor for storing results.\n            - `NUM_HEAD`, `NUM_BLOCK`:\
    \ Define the number of heads and blocks.\n            - `D_MODEL_K`, `D_MODEL_V`:\
    \ Overall dimensions of key and value models.\n            - `BLOCK_MODEL_K`,\
    \ `BLOCK_MODEL_V`: Dimensions of each block.\n            - `last_kv`: Tensor\
    \ storing the previous iteration's key/value state, if applicable.\n        -\
    \ Execution: Initializes accumulators, optionally using `last_kv`, then iterates\
    \ over blocks to apply recurrent transformation and store results in `O`.\n\n\
    \        `_bwd_recurrence`:\n        - Purpose: Computes gradients by reversing\
    \ the operations performed in the forward pass.\n        - Parameters:\n     \
    \       - `S`, `d`: Similar roles as in the forward pass.\n            - `DI`,\
    \ `DG`, `DL`, `DS`: Tensors for storing computed gradients.\n            - `NUM_HEAD`,\
    \ `NUM_BLOCK`, `D_MODEL_K`, `D_MODEL_V`, `BLOCK_MODEL_K`, `BLOCK_MODEL_V`: Dimensional\
    \ parameters matching those in the forward pass.\n        - Execution: Iteratively\
    \ computes gradients by reversing block operations, accumulating results into\
    \ gradient tensors `DI`, `DG`, and `DL`.\n\n        `ChunkGateRecurrent`:\n  \
    \      - A custom autograd function that manages data layout and execution of\
    \ forward and backward kernels on GPUs.\n        - `forward(ctx, kv, cross_decay,\
    \ last_kv=None)`: Prepares and launches the forward kernel with given inputs.\
    \ Saves output for backward pass.\n        - `backward(ctx, DO)`: Prepares and\
    \ launches the backward kernel, calculates gradients using previously saved outputs.\n\
    \        - Uses blocks of size `BLOCK_MODEL_K` and `BLOCK_MODEL_V` to divide work\
    \ efficiently across the GPU, ensuring proper tiling and parallelism.\n    \n\
    The test code is:\n\n\nimport torch\n\ndef test_chunk_gate_recurrent():\n    #\
    \ \u5B9A\u4E49\u6D4B\u8BD5\u53C2\u6570\n    B = 2        # Batch size\n    H =\
    \ 4        # Number of heads\n    N = 64       # Number of blocks (sequence length)\n\
    \    D_k = 64     # Key dimension\n    D_v = 64     # Value dimension\n\n    #\
    \ \u521B\u5EFA\u6D4B\u8BD5\u8F93\u5165\u5F20\u91CF\n    kv = torch.randn(B, H,\
    \ N, D_k, D_v, device='cuda', dtype=torch.float32, requires_grad=True)\n    cross_decay\
    \ = torch.randn(B, H, N, device='cuda', dtype=torch.float32, requires_grad=True)\n\
    \n    # \u53EF\u9009\u7684 last_kv\n    last_kv = torch.randn(B, H, D_k, D_v,\
    \ device='cuda', dtype=torch.float32, requires_grad=True)\n\n    # \u524D\u5411\
    \u4F20\u64AD\n    output1 = chunk_gate_recurrent(kv, cross_decay, last_kv)\n \
    \   output2 = chunk_gate_recurrent(kv, cross_decay, None)\n\n    # \u6D4B\u8BD5\
    \u53CD\u5411\u4F20\u64AD\n    # \u5BF9\u8F93\u51FA\u6C42\u548C\uFF0C\u4FDD\u8BC1\
    \u6240\u6709\u5143\u7D20\u90FD\u5BF9\u68AF\u5EA6\u6709\u8D21\u732E\n    loss1\
    \ = output1.sum()\n    loss1.backward()\n\n    # \u68C0\u67E5\u68AF\u5EA6\u662F\
    \u5426\u8BA1\u7B97\u6210\u529F\n    result = {\n        \"test_case_1\": (kv.grad\
    \ is not None, cross_decay.grad is not None, last_kv.grad is not None),\n    \
    \    \"test_case_2\": (kv.grad is not None, cross_decay.grad is not None)\n  \
    \  }\n\n    return result\n\nresult_gold = test_chunk_gate_recurrent()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py chunk_gate_recurrence.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_gate_recurrence
