compile_command:
- python decay_cumsum.py
correctness_command:
- python decay_cumsum_perf.py
performance_command:
- tb_eval -f decay_cumsum.py -o decay_cumsum_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The code comprises three main Triton kernels for specific tensor operations,\
    \ often utilized in transformer-like architectures:\n\n        1. **fwd_decay_cumsum**:\n\
    \           - **Function**: This kernel performs a cumulative sum with decay for\
    \ each element in a tensor `g`, writing the results to `g_o`. It iterates over\
    \ a defined number of block rows (`BT`).\n           - **Inputs**: Includes the\
    \ input tensor `g`, output tensor `g_o`, and a decay constant, with parameters\
    \ defining block and grid sizes.\n           - **Logic**: It initializes a zero\
    \ vector `cum_decay`, iteratively loads segments from `g`, scales them by `inv_ln2`,\
    \ accumulates into `cum_decay`, and stores results in `g_o`. The kernel uses a\
    \ mask to handle boundary conditions where the iteration range exceeds tensor\
    \ dimensions.\n\n        2. **prepare_qg_kg**:\n           - **Function**: Prepares\
    \ new tensors `qg` and `kg` from input tensors `q`, `k`, and `g`. The preparation\
    \ involves applying exponential decay and scaling transformations.\n         \
    \  - **Inputs**: Tensors `q`, `k`, and `g` with outputs `qg`, `kg`, and block/grid\
    \ size parameters.\n           - **Logic**: For each block row (`BT`), it retrieves\
    \ values from `q`, `k`, and `g`, computes exponential transformations based on\
    \ cumulative decay from `g`, and applies a scaling factor. Transformed values\
    \ are stored into `qg` and `kg`, leveraging mask operations to handle dimension\
    \ bounds.\n\n        3. **bwd_decay_global_cumsum**:\n           - **Function**:\
    \ Implements the backward operation to compute gradients for the cumulative decay\
    \ operation, updating `dg` based on provided input gradients.\n           - **Inputs**:\
    \ Includes `dq_inner`, `dq_inter`, `dk_inner`, `dk_inter`, `q`, `k`, `g`, and\
    \ the gradient tensor `dg`, along with configuration parameters.\n           -\
    \ **Logic**: Backtracks through each block row (`BT`), loads input gradients and\
    \ tensors, computes the gradient of the decay operation by differentiating through\
    \ the sum and product operations of `q` and `k`, and accumulates results into\
    \ `dg`. Employs exponential decay based on the gradient load position and handles\
    \ edge cases with a mask.\n\n        **Launch Functions**: For each kernel, a\
    \ corresponding launch function computes the necessary strides based on tensor\
    \ dimensions, prepares grid dimensions (`DK // BK`, `T // BT`, `B * H`), and invokes\
    \ the respective kernel with computed parameters.\n    \nThe test code is:\n\n\
    \nimport torch\n\n# Test the kernels\ndef test_kernels():\n    # Define parameters\n\
    \    B, H, T, DK = 2, 2, 4, 8\n    scale = 1.0\n    BT, BK = 2, 4\n\n    # Create\
    \ input tensors\n    g = torch.randn((B, H, T, DK), dtype=torch.float32, device='cuda')\n\
    \    g_o = torch.zeros_like(g)\n    q = torch.randn((B, H, T, DK), dtype=torch.float32,\
    \ device='cuda')\n    k = torch.randn((B, H, T, DK), dtype=torch.float32, device='cuda')\n\
    \    qg = torch.zeros_like(q)\n    kg = torch.zeros_like(k)\n    dq_inner = torch.randn_like(q)\n\
    \    dq_inter = torch.randn_like(q)\n    dk_inner = torch.randn_like(k)\n    dk_inter\
    \ = torch.randn_like(k)\n    dg = torch.zeros_like(g)\n\n    # Launch kernels\n\
    \    launch_fwd_decay_cumsum(g, g_o, B, H, T, scale, BT, BK, DK)\n    launch_prepare_qg_kg(q,\
    \ k, g, qg, kg, B, H, T, scale, BT, BK, DK)\n    launch_bwd_decay_global_cumsum(dq_inner,\
    \ dq_inter, dk_inner, dk_inter, q, k, g, dg, B, H, T, scale, BT, BK, DK)\n\n \
    \   # Store outputs for verification\n    results = {\n        \"test_case_1\"\
    : {\n            \"g_o\": g_o.clone(),\n            \"qg\": qg.clone(),\n    \
    \        \"kg\": kg.clone(),\n            \"dg\": dg.clone()\n        }\n    }\n\
    \    return results\n\n# Run the test\nresult_gold = test_kernels()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py decay_cumsum.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- decay_cumsum
