compile_command:
- python multinomial_sampling.py
correctness_command:
- python multinomial_sampling_perf.py
performance_command:
- tb_eval -f multinomial_sampling.py -o multinomial_sampling_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The code defines a Triton-based multinomial sampling procedure for sampling indices\
    \ from a probability distribution described by the `scores` tensor. This distribution\
    \ is organized as a 2D tensor with dimensions [batch_size, num_tokens], where\
    \ each row represents a batch and each column represents a token's score. The\
    \ `multinomial_sampling` function prepares for kernel execution by setting block\
    \ sizes (BLOCK = 8, BLOCK_N = 128), creating a grid for parallel execution, and\
    \ computing necessary strides for accessing tensor elements.\n\nThe Triton kernel\
    \ `_multinomial_sampling_kernel` executes the core logic: for each batch, it initializes\
    \ sampling seeds and offsets, uses them to generate random samples, and iteratively\
    \ computes cumulative scores for the token scores block-wise. The kernel then\
    \ determines the token indices where each random sample falls within the cumulative\
    \ probability range and stores the result. This approach ensures efficient parallel\
    \ computation and supports large batch sizes with many tokens.\n\nThe test code\
    \ is:\n\n\nimport torch\n\ndef test_multinomial_sampling():\n    result_dict =\
    \ {}\n\n    # Test case 1: Basic functionality with default indices\n    scores\
    \ = torch.tensor([[0.1, 0.2, 0.7], [0.3, 0.4, 0.3]], dtype=torch.float32).cuda()\n\
    \    seeds = torch.tensor([123, 456], dtype=torch.int64).cuda()\n    offsets =\
    \ torch.tensor([0, 0], dtype=torch.int64).cuda()\n\n    outputs = multinomial_sampling(scores,\
    \ seeds, offsets)\n    result_dict['test_case_1'] = outputs\n\n    # Test case\
    \ 2: Providing custom indices\n    indices = torch.tensor([[0, 1, 2], [2, 1, 0]],\
    \ dtype=torch.int64).cuda()\n    outputs = multinomial_sampling(scores, seeds,\
    \ offsets, indices)\n    result_dict['test_case_2'] = outputs\n\n    # Test case\
    \ 3: Single token case\n    scores_single_token = torch.tensor([[1.0], [1.0]],\
    \ dtype=torch.float32).cuda()\n    outputs = multinomial_sampling(scores_single_token,\
    \ seeds, offsets)\n    result_dict['test_case_3'] = outputs\n\n    return result_dict\n\
    \nresult_gold = test_multinomial_sampling()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ multinomial_sampling.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- multinomial_sampling
