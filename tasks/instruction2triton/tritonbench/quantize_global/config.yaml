compile_command:
- python quantize_global.py
correctness_command:
- python quantize_global_perf.py
performance_command:
- tb_eval -f quantize_global.py -o quantize_global_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The Triton kernel `_quantize_global` performs global quantization on a tensor\
    \ `x` by transforming its floating-point elements into an 8-bit integer representation.\
    \ The kernel operates over CUDA using Triton's JIT compilation, and is equipped\
    \ with autotuning to optimize the `BLOCK_SIZE` for different workloads. It takes\
    \ `x_ptr`, a pointer to the input tensor `x`, `absmax_inv_ptr`, a pointer to the\
    \ precomputed inverse of the maximum absolute value of `x`, and `output_ptr`,\
    \ a pointer where the quantized results are stored. The `n_elements` parameter\
    \ specifies the total number of elements to be processed. The kernel computes\
    \ `pid` using the Triton `program_id` to identify the current program instance\
    \ and calculates `block_start` and `offsets` to determine the specific segment\
    \ of data it processes. It uses `tl.load` to read `BLOCK_SIZE` contiguous elements\
    \ from `x`, applies the quantization operation using `tl.extra.cuda.libdevice.llrint`,\
    \ which rounds the scaled values to the nearest integer, and writes the results\
    \ into the output buffer. The surrounding Python function `quantize_global` first\
    \ calculates `absmax` as the maximum absolute value of `x`, derives its reciprocal\
    \ as `absmax_inv`, initializes an output tensor, and then dispatches the Triton\
    \ kernel over a grid, with the grid size dynamically determined based on the input\
    \ size and `BLOCK_SIZE`. After execution, it returns the quantized tensor and\
    \ the original maximum absolute value, which might be used for dequantization.\n\
    \nThe test code is:\n\n\nimport torch\n\n# Test for quantize_global\ndef test_quantize_global():\n\
    \    results = {}\n    \n    # Test case 1\n    x1 = torch.randn(2048, device='cuda',\
    \ dtype=torch.float32)\n    output1, absmax1 = quantize_global(x1)\n    results['test_case_1']\
    \ = (output1, absmax1)\n    \n    # Test case 2\n    x2 = torch.randn(1024, device='cuda',\
    \ dtype=torch.float32)\n    output2, absmax2 = quantize_global(x2)\n    results['test_case_2']\
    \ = (output2, absmax2)\n    \n    # Test case 3\n    x3 = torch.randn(3072, device='cuda',\
    \ dtype=torch.float32)\n    output3, absmax3 = quantize_global(x3)\n    results['test_case_3']\
    \ = (output3, absmax3)\n    \n    # Test case 4\n    x4 = torch.randn(4096, device='cuda',\
    \ dtype=torch.float32)\n    output4, absmax4 = quantize_global(x4)\n    results['test_case_4']\
    \ = (output4, absmax4)\n    \n    return results\n\n# Run the tests\nresult_gold\
    \ = test_quantize_global()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ quantize_global.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- quantize_global
