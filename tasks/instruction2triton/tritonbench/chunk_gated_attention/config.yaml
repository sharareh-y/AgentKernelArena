compile_command:
- python chunk_gated_attention.py
correctness_command:
- python chunk_gated_attention_perf.py
performance_command:
- tb_eval -f chunk_gated_attention.py -o chunk_gated_attention_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel code defines two main functions: `chunk_gated_abc_fwd_kernel_cum`\
    \ and `chunk_gated_abc_fwd_kernel_h`, \n    both leveraging parallel computing\
    \ capabilities to perform operations on tensor data.\n\n    `chunk_gated_abc_fwd_kernel_cum`\
    \ takes an input tensor `s` and computes a cumulative result stored in `o`. \n\
    \    It is parameterized by tensor strides and block sizes such as `T` (total\
    \ number of rows), `S` (total number of columns), \n    `BT` (block row size),\
    \ and `BS` (block column size). The kernel constructs a mask `m_s` to apply cumulative\
    \ operations \n    only on relevant data points, ensuring efficient computation\
    \ through block pointer manipulation and boundary checking.\n\n    `chunk_gated_abc_fwd_kernel_h`\
    \ is tailored for applying a gated cumulative sum on input tensors: `k` (keys),\
    \ \n    `v` (values), and `g` (gating factors). It considers initial and final\
    \ state tensors `h0` and `ht` if needed. \n    The kernel processes data in chunks\
    \ defined by `BT` (block size for T), `BK` (block size for K), and `BV` (block\
    \ size for V), \n    updating the output tensor `h` via iterative transformations\
    \ of `b_h` based on `b_k` and `b_v`, influenced by gating variables `b_g`.\n\n\
    \    The `fwd_pre` function serves as a preparatory step, setting up the execution\
    \ grid for `chunk_gated_abc_fwd_kernel_cum`. \n    It reshapes and processes the\
    \ input tensor `g`, effectively mimicking a cumulative sum operation.\n\n    `fwd_inner`\
    \ configures and invokes `chunk_gated_abc_fwd_kernel_h`. It defines the computational\
    \ grid and parameters \n    for efficient execution, with capabilities for adjusting\
    \ based on warp sizes and stages. This function realizes \n    a gated accumulation\
    \ logic often used in attention mechanisms, adapting with or without state continuity.\n\
    \    \nThe test code is:\n\n\ndef test_fwd_pre_inner():\n    # Define the input\
    \ parameters\n    B, H, T, S, K, V = 2, 4, 128, 64, 32, 32  # Batch size, heads,\
    \ sequence length, etc.\n    BT, BK, BV = 32, 16, 16  # Block sizes\n    g = torch.randn(B,\
    \ H, T, S, dtype=torch.float16, device='cuda')\n    q = torch.randn(B, H, T, V,\
    \ dtype=torch.float16, device='cuda')\n    k = torch.randn(B, H, K, T, dtype=torch.float16,\
    \ device='cuda')\n    v = torch.randn(B, H, T, V, dtype=torch.float16, device='cuda')\n\
    \    h0 = torch.randn(B, H, K, V, dtype=torch.float16, device='cuda')\n    ht\
    \ = torch.empty_like(h0)\n\n    # Test the fwd_pre function\n    g_cum = fwd_pre(g,\
    \ B, H, T, S, BT)\n\n    # Test the fwd_inner function with different branches\n\
    \    results = {}\n    # Case 1: Without initial and final state, gatek=False\n\
    \    results['test_case_1'] = fwd_inner(q, k, v, g_cum, B, H, T, K, V, BT, BK,\
    \ BV, gatek=False)\n\n    # Case 2: With initial state, without final state, gatek=True\n\
    \    results['test_case_2'] = fwd_inner(q, k, v, g_cum, B, H, T, K, V, BT, BK,\
    \ BV, gatek=True, h0=h0)\n\n    # Case 3: With initial and final state, gatek=False\n\
    \    results['test_case_3'] = fwd_inner(q, k, v, g_cum, B, H, T, K, V, BT, BK,\
    \ BV, gatek=False, h0=h0, ht=ht)\n\n    # Case 4: Without initial state, with\
    \ final state, gatek=True\n    results['test_case_4'] = fwd_inner(q, k, v, g_cum,\
    \ B, H, T, K, V, BT, BK, BV, gatek=True, ht=ht)\n\n    return results\n\nresult_gold\
    \ = test_fwd_pre_inner()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ chunk_gated_attention.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_gated_attention
