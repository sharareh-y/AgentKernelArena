compile_command:
- python rms_matmul_rbe.py
correctness_command:
- python rms_matmul_rbe_perf.py
performance_command:
- tb_eval -f rms_matmul_rbe.py -o rms_matmul_rbe_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    This implementation consists of Triton kernels designed to perform matrix multiplication\
    \ enhanced by RMS normalization and optional rotary embeddings, serving particularly\
    \ well in attention-based neural network layers. \n\n1. `rms_matmul_rbe` Kernel:\n\
    \   - Function: Computes the expression c = (rms(x) * rms_w) @ w with optional\
    \ rotary embeddings applied in the epilogue.\n   - Inputs: \n     - x_ptr, w_ptr,\
    \ rms_w_ptr, out_ptr: Pointers to the input matrix, weight matrix, rms of the\
    \ weight matrix, and output matrix, respectively.\n     - M, N, K: Dimensions\
    \ of the matrices involved.\n     - Strides: Strides for batch, M, N, K dimensions\
    \ for each matrix.\n     - start_token_position, USE_FP8, RBE_EPILOGUE, THETA,\
    \ EPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K: Constants and configuration flags\
    \ for numerical precision, RMS normalization, and rotary embedding application.\n\
    \   - Logic: The kernel uses block-level computations to load, normalize using\
    \ RMS, and multiply matrices. The rotary embeddings are optionally applied to\
    \ output.\n\n2. `rms_matmul_rbe_qkv` Kernel:\n   - Function: Acts as a wrapper\
    \ to perform matrix multiplications separately for Q, K, and V matrices. It calls\
    \ `rms_matmul_rbe` thrice, once for each type of matrix.\n   - Inputs: Similar\
    \ to `rms_matmul_rbe` but for Q, K, V matrices along with their respective weight\
    \ matrices and strides.\n\n3. `rms_matmul_rbe_qkv_wrapper` Function:\n   - Purpose:\
    \ This is a high-level PyTorch interface for initializing the Triton kernel launch.\
    \ It sets up the input/output matrices, strides, and prepares the grid configuration\
    \ for the kernel execution.\n   - Logic: It ensures type and shape checks for\
    \ input matrices, manages data type transformations for kernel compatibility,\
    \ and reshapes the output matrices (q, k, v) to expected dimensions for further\
    \ processing in neural network models.\n\nThe kernels leverage Triton's ability\
    \ to run optimized parallel computations on GPUs, allowing efficient implementation\
    \ of operations common in Transformer architectures. The implementation checks\
    \ data types and dimensions to support both FP16 and FP8, offering performance\
    \ tuning via block size and grid configuration.\n\nThe test code is:\n\n\n# Test\
    \ for rms_matmul_rbe_qkv_wrapper\ndef test_rms_matmul_rbe_qkv():\n    results\
    \ = {}\n    \n    # Test case 1\n    batch, seq_len, heads, dim = [1, 16, 32,\
    \ 128]\n    embeddings_load = torch.randn([batch, seq_len, heads * dim], dtype=torch.float16,\
    \ device=\"cuda\")\n    rms_weights = torch.randn([heads * dim], dtype=torch.float16,\
    \ device=\"cuda\") * 0.2\n    q_weights_load = torch.randn([heads * dim, heads\
    \ * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    k = torch.empty((embeddings_load.shape[0],\
    \ embeddings_load.shape[1], q_weights_load.shape[-1]),\n                    dtype=q_weights_load.dtype,\
    \ device=q_weights_load.device)\n    v = torch.empty_like(k)\n    q, k, v = rms_matmul_rbe_qkv_wrapper(x=embeddings_load,\
    \ start_pos=0,\n                                         q_weight=q_weights_load,\
    \ k_weight=q_weights_load,\n                                         v_weight=q_weights_load,\
    \ rms_w=rms_weights,\n                                         k=k, v=v,\n   \
    \                                      n_heads=32,\n                         \
    \                head_dim=128)\n    results['test_case_1'] = (q.shape, k.shape,\
    \ v.shape)\n    \n    # Test case 2: Different dimensions\n    batch, seq_len,\
    \ heads, dim = [2, 32, 16, 64]\n    embeddings_load = torch.randn([batch, seq_len,\
    \ heads * dim], dtype=torch.float16, device=\"cuda\")\n    rms_weights = torch.randn([heads\
    \ * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    q_weights_load = torch.randn([heads\
    \ * dim, heads * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    k = torch.empty((embeddings_load.shape[0],\
    \ embeddings_load.shape[1], q_weights_load.shape[-1]),\n                    dtype=q_weights_load.dtype,\
    \ device=q_weights_load.device)\n    v = torch.empty_like(k)\n    q, k, v = rms_matmul_rbe_qkv_wrapper(x=embeddings_load,\
    \ start_pos=0,\n                                         q_weight=q_weights_load,\
    \ k_weight=q_weights_load,\n                                         v_weight=q_weights_load,\
    \ rms_w=rms_weights,\n                                         k=k, v=v,\n   \
    \                                      n_heads=16,\n                         \
    \                head_dim=64)\n    results['test_case_2'] = (q.shape, k.shape,\
    \ v.shape)\n    \n    # Test case 3: Different start position\n    batch, seq_len,\
    \ heads, dim = [1, 16, 32, 128]\n    embeddings_load = torch.randn([batch, seq_len,\
    \ heads * dim], dtype=torch.float16, device=\"cuda\")\n    rms_weights = torch.randn([heads\
    \ * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    q_weights_load = torch.randn([heads\
    \ * dim, heads * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    k = torch.empty((embeddings_load.shape[0],\
    \ embeddings_load.shape[1], q_weights_load.shape[-1]),\n                    dtype=q_weights_load.dtype,\
    \ device=q_weights_load.device)\n    v = torch.empty_like(k)\n    q, k, v = rms_matmul_rbe_qkv_wrapper(x=embeddings_load,\
    \ start_pos=5,\n                                         q_weight=q_weights_load,\
    \ k_weight=q_weights_load,\n                                         v_weight=q_weights_load,\
    \ rms_w=rms_weights,\n                                         k=k, v=v,\n   \
    \                                      n_heads=32,\n                         \
    \                head_dim=128)\n    results['test_case_3'] = (q.shape, k.shape,\
    \ v.shape)\n    \n    # Test case 4: Different weights\n    batch, seq_len, heads,\
    \ dim = [1, 16, 32, 128]\n    embeddings_load = torch.randn([batch, seq_len, heads\
    \ * dim], dtype=torch.float16, device=\"cuda\")\n    rms_weights = torch.randn([heads\
    \ * dim], dtype=torch.float16, device=\"cuda\") * 0.2\n    q_weights_load = torch.randn([heads\
    \ * dim, heads * dim], dtype=torch.float16, device=\"cuda\") * 0.1\n    k = torch.empty((embeddings_load.shape[0],\
    \ embeddings_load.shape[1], q_weights_load.shape[-1]),\n                    dtype=q_weights_load.dtype,\
    \ device=q_weights_load.device)\n    v = torch.empty_like(k)\n    q, k, v = rms_matmul_rbe_qkv_wrapper(x=embeddings_load,\
    \ start_pos=0,\n                                         q_weight=q_weights_load,\
    \ k_weight=q_weights_load,\n                                         v_weight=q_weights_load,\
    \ rms_w=rms_weights,\n                                         k=k, v=v,\n   \
    \                                      n_heads=32,\n                         \
    \                head_dim=128)\n    results['test_case_4'] = (q.shape, k.shape,\
    \ v.shape)\n    \n    return results\n\nresult_gold = test_rms_matmul_rbe_qkv()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py rms_matmul_rbe.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rms_matmul_rbe
