compile_command:
- python mul_exponent_compensator.py
correctness_command:
- python mul_exponent_compensator_perf.py
performance_command:
- tb_eval -f mul_exponent_compensator.py -o mul_exponent_compensator_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The code defines a Triton kernel `mul_kernel` that multiplies each\
    \ element of a given source tensor `src` with a constant value referred to as\
    \ the exponent compensator and stores the result in the destination tensor `dst`.\
    \ The kernel is designed to work on a portion of the tensor at a time, where the\
    \ portion size is controlled by a parameter `BLOCK_SIZE`, which is a compile-time\
    \ constant. The constant exponent compensator is defined as `2.0 ** (127 - 15)`\
    \ to adjust for exponent bias in floating-point representation. The kernel computes\
    \ indices for accessing elements in the source tensor using `tl.program_id(0)`\
    \ to identify the current block and `tl.arange(0, BLOCK_SIZE)` for offsets within\
    \ the block. Using these indices, it loads the corresponding elements from `src`,\
    \ multiplies them by the compensator, and stores the results in `dst`.\n\n   \
    \         The wrapper function `launch_mul_kernel` is provided to set up and invoke\
    \ this Triton kernel. It takes a PyTorch tensor `src` and an optional block size\
    \ `BLOCK_SIZE` as arguments. It creates an empty destination tensor `dst` on the\
    \ CUDA device with the same shape as `src`. It then launches the kernel with the\
    \ appropriate grid size `(src.shape[0] // BLOCK_SIZE,)`, passing `src`, `dst`,\
    \ and `BLOCK_SIZE` as parameters. Finally, the function returns the filled destination\
    \ tensor `dst` containing the results of the multiplication operation.\n     \
    \       \nThe test code is:\n\n\ndef test_mul():\n    src = torch.tensor([8323072],\
    \ dtype=torch.int32, device='cuda').view(torch.float32)\n    \n    test_cases\
    \ = {}\n    \n    # Test case 1\n    dst_triton_1 = launch_mul_kernel(src, BLOCK_SIZE=1)\n\
    \    test_cases['test_case_1'] = dst_triton_1\n\n    # Test case 2\n    dst_triton_2\
    \ = launch_mul_kernel(src, BLOCK_SIZE=2)\n    test_cases['test_case_2'] = dst_triton_2\n\
    \    \n    # Test case 3\n    dst_triton_3 = launch_mul_kernel(src, BLOCK_SIZE=4)\n\
    \    test_cases['test_case_3'] = dst_triton_3\n    \n    # Test case 4\n    dst_triton_4\
    \ = launch_mul_kernel(src, BLOCK_SIZE=8)\n    test_cases['test_case_4'] = dst_triton_4\n\
    \n    return test_cases\n\nresult_gold = test_mul()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ mul_exponent_compensator.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- mul_exponent_compensator
