compile_command:
- python fused_recurrent_delta.py
correctness_command:
- python fused_recurrent_delta_perf.py
performance_command:
- tb_eval -f fused_recurrent_delta.py -o fused_recurrent_delta_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This Triton-based operation encapsulates fused recurrent computation\
    \ which is split into two main kernels: 'fused_recurrent_fwd_kernel' and 'fused_recurrent_bwd_kernel'.\
    \ The forward kernel is designed to perform element-wise multiplication and accumulation\
    \ operations to process sequences of data in a parallelized manner using Triton's\
    \ primitives. Key inputs are tensors q, k, and v, which stand for query, key,\
    \ and value respectively, with additional parameters such as beta for scaling\
    \ adjustments and initial_state for setting the initial hidden state. The operation\
    \ also scales input queries using a specified or default scale factor. The output\
    \ can optionally include the final state of the recurrent computation. The backward\
    \ kernel computes gradients for each of the input tensors, leveraging Triton's\
    \ efficient parallel computing capabilities to perform operations such as tensor\
    \ load, store, and arithmetic computations in reverse order to update weights\
    \ during backpropagation. The FusedRecurrentFunction class integrates these kernels\
    \ into PyTorch's autograd mechanism, enabling them to be used as custom backward\
    \ functions, with the 'fused_recurrent_delta_rule' acting as the interface for\
    \ user-friendly application. Key parameters include B (batch size), H (number\
    \ of heads), T (sequence length), K and V (dimensions of the key and value tensors).\
    \ BK and BV are block sizes for partitioning the computation, and grid specifies\
    \ the execution configuration for Triton. The operations also account for different\
    \ cases such as whether initial states are used or if beta has head-wise scaling,\
    \ adapting the kernel execution path accordingly.\n            \nThe test code\
    \ is:\n\n\nimport torch\n\ndef test_fused_recurrent_delta_rule_with_backward():\n\
    \    # \u5B9A\u4E49\u5C3A\u5BF8\n    B, H, T, K, V = 2, 4, 8, 16, 32\n\n    #\
    \ \u786E\u4FDD\u8F93\u5165\u5F20\u91CF\u4E3A\u53F6\u5B50\u5F20\u91CF\uFF0C\u4E14\
    \ requires_grad=True\n    q = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda',\
    \ requires_grad=True)\n    k = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda',\
    \ requires_grad=True)\n    v = torch.randn(B, H, T, V, dtype=torch.float32, device='cuda',\
    \ requires_grad=True)\n    beta_headwise = torch.randn(B, H, T, V, dtype=torch.float32,\
    \ device='cuda', requires_grad=True)\n    beta_non_headwise = torch.randn(B, H,\
    \ T, dtype=torch.float32, device='cuda', requires_grad=True)\n    initial_state\
    \ = torch.randn(B, H, K, V, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \n    # Test 1: Headwise beta, with initial_state and final_state\n    o, final_state\
    \ = fused_recurrent_delta_rule(q, k, v, beta=beta_headwise, scale=0.1, initial_state=initial_state,\
    \ output_final_state=True)\n\n    loss = o.sum() + final_state.sum()\n    loss.backward()\n\
    \n    result_1 = {\n        \"grad_q\": q.grad.norm().item(),\n        \"grad_k\"\
    : k.grad.norm().item(),\n        \"grad_v\": v.grad.norm().item(),\n        \"\
    grad_beta_headwise\": beta_headwise.grad.norm().item(),\n        \"grad_initial_state\"\
    : initial_state.grad.norm().item()\n    }\n\n    q.grad.zero_()\n    k.grad.zero_()\n\
    \    v.grad.zero_()\n    beta_headwise.grad.zero_()\n    initial_state.grad.zero_()\n\
    \n    # Test 2: Non-headwise beta, with initial_state and final_state\n    o,\
    \ final_state = fused_recurrent_delta_rule(q, k, v, beta=beta_non_headwise, scale=0.1,\
    \ initial_state=initial_state, output_final_state=True)\n\n    loss = o.sum()\
    \ + final_state.sum()\n    loss.backward()\n\n    result_2 = {\n        \"grad_q\"\
    : q.grad.norm().item(),\n        \"grad_k\": k.grad.norm().item(),\n        \"\
    grad_v\": v.grad.norm().item(),\n        \"grad_beta_non_headwise\": beta_non_headwise.grad.norm().item(),\n\
    \        \"grad_initial_state\": initial_state.grad.norm().item()\n    }\n\n \
    \   q.grad.zero_()\n    k.grad.zero_()\n    v.grad.zero_()\n    beta_non_headwise.grad.zero_()\n\
    \    initial_state.grad.zero_()\n\n    # Test 3: No initial state, with final\
    \ state\n    o, final_state = fused_recurrent_delta_rule(q, k, v, beta=beta_headwise,\
    \ scale=0.1, initial_state=None, output_final_state=True)\n\n    loss = o.sum()\
    \ + final_state.sum()\n    loss.backward()\n\n    result_3 = {\n        \"grad_q\"\
    : q.grad.norm().item(),\n        \"grad_k\": k.grad.norm().item(),\n        \"\
    grad_v\": v.grad.norm().item(),\n        \"grad_beta_headwise\": beta_headwise.grad.norm().item()\n\
    \    }\n\n    q.grad.zero_()\n    k.grad.zero_()\n    v.grad.zero_()\n    beta_headwise.grad.zero_()\n\
    \n    # Test 4: With initial state, no final state output\n    o, _ = fused_recurrent_delta_rule(q,\
    \ k, v, beta=beta_headwise, scale=0.1, initial_state=initial_state, output_final_state=False)\n\
    \n    loss = o.sum()\n    loss.backward()\n\n    result_4 = {\n        \"grad_q\"\
    : q.grad.norm().item(),\n        \"grad_k\": k.grad.norm().item(),\n        \"\
    grad_v\": v.grad.norm().item(),\n        \"grad_beta_headwise\": beta_headwise.grad.norm().item(),\n\
    \        \"grad_initial_state\": initial_state.grad.norm().item()\n    }\n\n \
    \   return {\n        \"test_case_1\": result_1,\n        \"test_case_2\": result_2,\n\
    \        \"test_case_3\": result_3,\n        \"test_case_4\": result_4\n    }\n\
    \nresult_gold = test_fused_recurrent_delta_rule_with_backward()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py fused_recurrent_delta.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_recurrent_delta
