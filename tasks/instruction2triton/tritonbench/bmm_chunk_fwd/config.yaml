compile_command:
- python bmm_chunk_fwd.py
correctness_command:
- python bmm_chunk_fwd_perf.py
performance_command:
- tb_eval -f bmm_chunk_fwd.py -o bmm_chunk_fwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `_bmm_chunk_fwd_kernel` is responsible for computing\
    \ a batched matrix multiplication (BMM) with support for chunking, causal masking,\
    \ and sequence indexing. The function takes pointers to input tensors `a` and\
    \ `b`, an output tensor `out`, and several parameters dictating the shapes, strides,\
    \ and other characteristics needed for efficient computation on GPUs.\n\n    \
    \        Key parameters include:\n            - `BLOCK_SIZE_M`, `BLOCK_SIZE_N`,\
    \ `BLOCK_SIZE_K`: constants defining the size of each processing block for the\
    \ M, N, and K dimensions.\n            - `IS_CAUSAL`: a compile-time constant\
    \ indicating whether causal masking is applied.\n            - `HAS_SEQ_IDX`:\
    \ a compile-time constant indicating whether sequence indexing is applied.\n\n\
    \            The kernel uses program IDs (`pid`) to determine the specific data\
    \ block each thread processes, defined by combinations of batch, chunk, group,\
    \ and head indices. The kernel then computes the dot product of corresponding\
    \ sub-matrices from `a` and `b`, accumulating the result in `acc`, a local accumulation\
    \ tensor.\n\n            Once the accumulation is complete, if `HAS_SEQ_IDX` is\
    \ enabled, the sequence indices are loaded and used to zero out contributions\
    \ from mismatched indices. The resulting matrix product is stored back into the\
    \ global memory.\n\n            The `_bmm_chunk_fwd` function is a Python wrapper\
    \ that sets up kernel arguments, determines execution grid size, and launches\
    \ the kernel. It handles cases where input tensors are non-contiguous, ensures\
    \ output tensor allocation based on the computed dimensions, and adjusts for optional\
    \ parameters like sequence indexing and causality. This function is designed to\
    \ facilitate BMM operations in various configurations, making it versatile for\
    \ different input tensor shapes and application needs.\n            \nThe test\
    \ code is:\n\n\nimport torch\n\n# Test for _bmm_chunk_fwd\ndef test_bmm_chunk_fwd():\n\
    \    results = {}\n    \n    # Test case 1: Without groups, no seq_idx, not causal\n\
    \    a = torch.randn(2, 128, 64, device='cuda', dtype=torch.float16)\n    b =\
    \ torch.randn(2, 128, 64, device='cuda', dtype=torch.float16)\n    chunk_size\
    \ = 32\n    out = _bmm_chunk_fwd(a, b, chunk_size)\n    results['test_case_1']\
    \ = out.shape\n\n    # Test case 2: With groups, with seq_idx, causal\n    a =\
    \ torch.randn(2, 128, 4, 64, device='cuda', dtype=torch.float16)\n    b = torch.randn(2,\
    \ 128, 4, 64, device='cuda', dtype=torch.float16)\n    seq_idx = torch.arange(128,\
    \ device='cuda').repeat(2, 1)\n    out = _bmm_chunk_fwd(a, b, chunk_size, seq_idx=seq_idx,\
    \ causal=True)\n    results['test_case_2'] = out.shape\n\n    # Test case 3: Without\
    \ groups, with seq_idx, not causal\n    a = torch.randn(2, 128, 64, device='cuda',\
    \ dtype=torch.float16)\n    b = torch.randn(2, 128, 64, device='cuda', dtype=torch.float16)\n\
    \    seq_idx = torch.arange(128, device='cuda').repeat(2, 1)\n    out = _bmm_chunk_fwd(a,\
    \ b, chunk_size, seq_idx=seq_idx, causal=False)\n    results['test_case_3'] =\
    \ out.shape\n\n    # Test case 4: With groups, no seq_idx, not causal\n    a =\
    \ torch.randn(2, 128, 4, 64, device='cuda', dtype=torch.float16)\n    b = torch.randn(2,\
    \ 128, 4, 64, device='cuda', dtype=torch.float16)\n    out = _bmm_chunk_fwd(a,\
    \ b, chunk_size, causal=False)\n    results['test_case_4'] = out.shape\n\n   \
    \ return results\n\n# Run tests\nresult_gold = test_bmm_chunk_fwd()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py bmm_chunk_fwd.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- bmm_chunk_fwd
