compile_command:
- python chunk_cumsum_kernel.py
correctness_command:
- python chunk_cumsum_kernel_perf.py
performance_command:
- tb_eval -f chunk_cumsum_kernel.py -o chunk_cumsum_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel `chunk_global_cumsum_scalar_kernel` is designed to perform\
    \ a cumulative sum operation on a 3D tensor, where the dimensions are Batch (B),\
    \ Head (H), and Time (T). This kernel processes the tensor in chunks defined by\
    \ block size `BT`, and each block's result is stored in the output tensor `o`.\
    \ The kernel uses a loop to iterate over these chunks, loading a block of data\
    \ using `tl.make_block_ptr` and `tl.load`, performing cumulative sum with `tl.cumsum`,\
    \ and updating a running sum with `tl.sum` to carry over between chunks. `chunk_global_cumsum_scalar`\
    \ acts as the kernel's wrapper function, preparing and launching it with the appropriate\
    \ grid configuration based on the input tensor's dimensions. It accepts a PyTorch\
    \ tensor `s` and an optional `dtype`, defaulting to the dtype of `s`. It initializes\
    \ an empty output tensor `z` with the same shape and dtype as `s` to hold the\
    \ computation results. The grid configuration is determined by the batch size\
    \ and head count, ensuring each program instance processes one sequence.\n   \
    \ \nThe test code is:\n\n\nimport torch\n\n# Test for chunk_global_cumsum_scalar\n\
    def test_chunk_global_cumsum_scalar():\n    B, H, T = 2, 3, 4  # Example dimensions\n\
    \    results = {}\n    \n    # Test case 1\n    s1 = torch.rand((B, H, T), dtype=torch.float32).cuda()\n\
    \    result1 = chunk_global_cumsum_scalar(s1)\n    results['test_case_1'] = result1\n\
    \    \n    # Test case 2\n    s2 = torch.rand((B, H, T), dtype=torch.float32).cuda()\n\
    \    result2 = chunk_global_cumsum_scalar(s2)\n    results['test_case_2'] = result2\n\
    \    \n    # Test case 3\n    s3 = torch.rand((B, H, T), dtype=torch.float32).cuda()\n\
    \    result3 = chunk_global_cumsum_scalar(s3)\n    results['test_case_3'] = result3\n\
    \    \n    # Test case 4\n    s4 = torch.rand((B, H, T), dtype=torch.float32).cuda()\n\
    \    result4 = chunk_global_cumsum_scalar(s4)\n    results['test_case_4'] = result4\n\
    \    \n    return results\n\n# Run all tests\nresult_gold = test_chunk_global_cumsum_scalar()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py chunk_cumsum_kernel.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_cumsum_kernel
