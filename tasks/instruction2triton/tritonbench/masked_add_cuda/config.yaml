compile_command:
- python masked_add_cuda.py
correctness_command:
- python masked_add_cuda_perf.py
performance_command:
- tb_eval -f masked_add_cuda.py -o masked_add_cuda_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This code implements a Triton kernel called `masked_add_kernel` and\
    \ a function `masked_add` to invoke the kernel. The kernel operates on CUDA tensors\
    \ `grad_ptr`, `p_ptr`, and `p_mask_ptr` over blocks of size `BLOCK_SIZE`. It calculates\
    \ indices to work on using `tl.program_id` and checks bounds using a `mask`. It\
    \ then loads elements from `p_mask_ptr` and converts them to boolean masks using\
    \ `tl.int1`. The kernel performs an element-wise addition on `grad_ptr` with `p_ptr`\
    \ multiplied by `alpha`, masked by `p_mask`. The result is stored back in `grad_ptr`.\
    \ The wrapper function `masked_add` asserts that all tensors are on CUDA, with\
    \ the same layout and strides. It determines the grid size based on `n_elements`\
    \ and launches the kernel with the specified block size, passing tensor data and\
    \ parameters for computation.\n            \nThe test code is:\n\n\nimport torch\n\
    \n# \u6D4B\u8BD5\u4EE3\u7801\ndef test_masked_add():\n    # \u8BBE\u7F6E\u968F\
    \u673A\u79CD\u5B50\u4EE5\u4FDD\u8BC1\u7ED3\u679C\u53EF\u590D\u73B0\n    torch.manual_seed(0)\n\
    \    n = 10000  # \u9009\u62E9\u8F83\u5927\u7684\u5F20\u91CF\u5927\u5C0F\n\n \
    \   # \u751F\u6210\u968F\u673A\u5F20\u91CF\n    grad = torch.randn(n, device='cuda')\n\
    \    p_data = torch.randn(n, device='cuda')\n    p_mask = torch.randint(0, 2,\
    \ (n,), device='cuda')  # \u751F\u62100\u62161\u7684\u63A9\u7801\n\n    # Triton\u7248\
    \u672C\n    results = {}\n    \n    # Test case 1\n    grad_triton = grad.clone()\n\
    \    masked_add(grad_triton, p_data, p_mask, alpha=0.5)\n    results['test_case_1']\
    \ = grad_triton.clone()\n\n    # Test case 2: alpha = 0\n    grad_triton = grad.clone()\n\
    \    masked_add(grad_triton, p_data, p_mask, alpha=0)\n    results['test_case_2']\
    \ = grad_triton.clone()\n\n    # Test case 3: all mask values are 0\n    p_mask_zero\
    \ = torch.zeros(n, device='cuda', dtype=torch.int32)\n    grad_triton = grad.clone()\n\
    \    masked_add(grad_triton, p_data, p_mask_zero, alpha=0.5)\n    results['test_case_3']\
    \ = grad_triton.clone()\n\n    # Test case 4: all mask values are 1\n    p_mask_one\
    \ = torch.ones(n, device='cuda', dtype=torch.int32)\n    grad_triton = grad.clone()\n\
    \    masked_add(grad_triton, p_data, p_mask_one, alpha=0.5)\n    results['test_case_4']\
    \ = grad_triton.clone()\n\n    return results\n\n# \u8FD0\u884C\u6D4B\u8BD5\n\
    result_gold = test_masked_add()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ masked_add_cuda.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- masked_add_cuda
