compile_command:
- python layer_norm_triton.py
correctness_command:
- python layer_norm_triton_perf.py
performance_command:
- tb_eval -f layer_norm_triton.py -o layer_norm_triton_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton-based implementation provides a high-performance LayerNorm\
    \ operation optimized for execution on GPUs. It involves three specialized kernels:\
    \ \n            `_layer_norm_fwd_fused` performs the forward pass by normalizing\
    \ input tensor `X` over its last dimension using batch statistics (mean and variance).\
    \ It applies learned scale (`W`) and shift (`B`) parameters, storing the normalized\
    \ result in output `Y`. The kernel uses block-wise reduction for efficient computation,\
    \ storing per-row mean and inverse standard deviation (`Rstd`), which are later\
    \ used for the backward pass.\n            `_layer_norm_bwd_dx_fused` computes\
    \ the gradient of inputs (`DX`) using the output gradients (`DY`). It adjusts\
    \ for learned parameters (`W`) and uses saved batch statistics (`Mean`, `Rstd`).\
    \ The kernel handles partial reduction of gradients for weights (`DW`) and biases\
    \ (`DB`) across a workgroup, ensuring synchronization via locks during accumulation.\n\
    \            `_layer_norm_bwd_dwdb` finalizes the backward pass by aggregating\
    \ partial gradients for weights and biases collected across workgroups into final\
    \ gradients (`FINAL_DW` and `FINAL_DB`). This kernel completes the distributed\
    \ reduction using efficient memory operations.\n            The `LayerNorm` class\
    \ leverages these kernels, defining a custom PyTorch function. During the forward\
    \ pass, it reshapes inputs and prepares necessary buffers, executing `_layer_norm_fwd_fused`.\
    \ In the backward pass, `_layer_norm_bwd_dx_fused` and `_layer_norm_bwd_dwdb`\
    \ are called sequentially, computing the necessary gradients with respect to inputs,\
    \ weights, and biases. Parameters like `BLOCK_SIZE`, `GROUP_SIZE_M`, and kernel\
    \ launch configurations (number of warps) are dynamically determined based on\
    \ input dimensions to optimize performance.\n            \nThe test code is:\n\
    \n\nimport torch\n\ndef test_layer_norm_with_backward():\n    # Define the input\
    \ parameters\n    batch_size = 32\n    feature_dim = 512\n    eps = 1e-5\n\n \
    \   # Create random input data\n    x = torch.randn((batch_size, feature_dim),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n    weight = torch.ones((feature_dim,),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n    bias = torch.zeros((feature_dim,),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n\n    # Call the layer\
    \ normalization function\n    y = layer_norm(x, (feature_dim,), weight, bias,\
    \ eps)\n\n    # Define a simple loss function (sum of all elements)\n    loss\
    \ = y.sum()\n\n    # Perform backward pass\n    loss.backward()\n\n    # Verify\
    \ the gradients are non-zero\n    assert x.grad is not None, \"Gradient for input\
    \ x is None\"\n    assert weight.grad is not None, \"Gradient for weight is None\"\
    \n    assert bias.grad is not None, \"Gradient for bias is None\"\n\n    # Verify\
    \ the shapes of gradients\n    assert x.grad.shape == x.shape, f\"Gradient shape\
    \ for x is incorrect: {x.grad.shape}\"\n    assert weight.grad.shape == weight.shape,\
    \ f\"Gradient shape for weight is incorrect: {weight.grad.shape}\"\n    assert\
    \ bias.grad.shape == bias.shape, f\"Gradient shape for bias is incorrect: {bias.grad.shape}\"\
    \n\n    # Store results in a dictionary\n    results = {\n        \"test_case_1\"\
    : {\n            \"output_shape\": y.shape,\n            \"loss\": loss.item(),\n\
    \            \"x_grad_norm\": x.grad.norm().item(),\n            \"weight_grad_norm\"\
    : weight.grad.norm().item(),\n            \"bias_grad_norm\": bias.grad.norm().item()\n\
    \        }\n    }\n\n    return results\n\n# Run the test\nresult_gold = test_layer_norm_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py layer_norm_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- layer_norm_triton
