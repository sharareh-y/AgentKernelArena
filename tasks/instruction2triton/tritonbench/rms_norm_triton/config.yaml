compile_command:
- python rms_norm_triton.py
correctness_command:
- python rms_norm_triton_perf.py
performance_command:
- tb_eval -f rms_norm_triton.py -o rms_norm_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The code defines a custom operation for RMS normalization using the\
    \ Triton library, which is optimized for GPU execution. The kernel `rms_norm_kernel`\
    \ is responsible for computing the RMS normalized version of input `X` with weights\
    \ `W`, storing the results in `Y`. It operates on a per-row basis, controlled\
    \ by the program ID `pid`, which assigns each row of `X` to a separate block.\
    \ The mask ensures computations respect the dimensions of `X` even if it's smaller\
    \ than the block size. Variance is computed as `var = tl.sum(x * x, axis=0) /\
    \ N`, where `x` is the loaded data of the current row. `rrms` is calculated as\
    \ the reciprocal of the root of `var` plus a small `eps` to ensure numerical stability.\
    \ The computation `y = (x * rrms).to(Y.dtype.element_ty) * w` applies normalization\
    \ and scales by weights. The result is stored in `Y`. The `RmsNorm` class encapsulates\
    \ this in a PyTorch `Function`, providing a `forward` method which allocates outputs\
    \ and calls the kernel, passing dimensions and strides. The method `rms_norm`\
    \ wraps this `Function` for user-friendly access, taking the input tensor `x`,\
    \ its normalized shape, the weight tensor, and an optional `eps` parameter, and\
    \ returns the RMS normalized tensor.\n            \nThe test code is:\n\n\ndef\
    \ test_rms_norm():\n    # Define input parameters\n    batch_size = 32\n    feature_size\
    \ = 128\n    eps = 1e-5\n\n    # Create random input data and weights\n    x =\
    \ torch.randn(batch_size, feature_size, device='cuda', dtype=torch.float32)\n\
    \    weight = torch.randn(feature_size, device='cuda', dtype=torch.float32)\n\n\
    \    # Triton implementation\n    output = rms_norm(x, (feature_size,), weight,\
    \ eps)\n\n    # Additional test cases to cover all branches\n    test_case_1 =\
    \ rms_norm(x, (feature_size,), weight, eps)\n    test_case_2 = rms_norm(x, (feature_size,),\
    \ weight, eps=1e-6)\n    test_case_3 = rms_norm(x, (feature_size,), weight, eps=1e-7)\n\
    \    test_case_4 = rms_norm(x, (feature_size,), weight, eps=1e-8)\n\n    return\
    \ {\n        \"test_case_1\": test_case_1,\n        \"test_case_2\": test_case_2,\n\
    \        \"test_case_3\": test_case_3,\n        \"test_case_4\": test_case_4\n\
    \    }\n\nresult_gold = test_rms_norm()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rms_norm_triton.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rms_norm_triton
