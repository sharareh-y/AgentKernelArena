compile_command:
- python chunk_cumsum_vector.py
correctness_command:
- python chunk_cumsum_vector_perf.py
performance_command:
- tb_eval -f chunk_cumsum_vector.py -o chunk_cumsum_vector_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The code defines a Triton-based GPU kernel called `chunk_global_cumsum_vector_kernel`\
    \ and a corresponding Python function `chunk_global_cumsum_vector` for applying\
    \ this kernel. The kernel computes a global cumulative sum over blocks in a 4D\
    \ input tensor `s`. The tensor dimensions are interpreted as [Batch, Head, Time,\
    \ Size]. The kernel uses block processing, defined by the `BT` (block time) and\
    \ `BS` (block size) parameters. The kernel performs the following steps:\n\n \
    \   1. It determines the program's position using `tl.program_id`, which assigns\
    \ work to the kernel across blocks of the tensor.\n    2. A lower triangular mask\
    \ `m_s` is created to ensure each element's sum includes all previous elements\
    \ in the current block row.\n    3. For each block in the time dimension (`T`),\
    \ a pointer to the relevant data slice in `s` is created with `tl.make_block_ptr`.\n\
    \    4. Data from `s` is loaded into `b_s`, processed as `float32` for precision.\n\
    \    5. Matrix multiplication (`tl.dot`) with the mask `m_s` is used to compute\
    \ the block-level cumulative sum, stored in `b_c`.\n    6. The result is stored\
    \ back to the output tensor `z`.\n    7. The running sum `b_z` is updated after\
    \ processing each block column.\n\n    The `chunk_global_cumsum_vector` function\
    \ prepares the input tensor `s`, and creates an output tensor `z` of the same\
    \ shape. It sets the computation grid based on tensor dimensions and block size.\
    \ The kernel is called with this grid, using dimensions like `stride` to correctly\
    \ address tensor slices.\n\n    The kernel uses Triton's `autotune` decorator\
    \ to choose optimal execution configurations (combinations of block size and number\
    \ of warps) based on input size `S`. This approach improves performance by adapting\
    \ to specific hardware characteristics.\n    \nThe test code is:\n\n\nimport torch\n\
    \n# Test for chunk_global_cumsum_vector with all possible branch coverage\ndef\
    \ test_chunk_global_cumsum_vector():\n    B, H, T, S = 2, 3, 4, 5  # Example dimensions\n\
    \    BS = 32\n    s = torch.rand((B, H, T, S), dtype=torch.float32).cuda()\n\n\
    \    result_dict = {}\n    \n    # First case: Default configuration\n    result_dict[\"\
    test_case_1\"] = chunk_global_cumsum_vector(s)\n    \n    # Third case: Testing\
    \ with larger tensor dimensions\n    s_large = torch.rand((B, H, T * 2, S * 2),\
    \ dtype=torch.float32).cuda()\n    result_dict[\"test_case_2\"] = chunk_global_cumsum_vector(s_large)\n\
    \    \n    # Fourth case: Testing with smaller tensor dimensions\n    s_small\
    \ = torch.rand((B, H, 1, S), dtype=torch.float32).cuda()\n    result_dict[\"test_case_3\"\
    ] = chunk_global_cumsum_vector(s_small)\n    \n    return result_dict\n\n# Run\
    \ all tests\nresult_gold = test_chunk_global_cumsum_vector()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py chunk_cumsum_vector.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_cumsum_vector
