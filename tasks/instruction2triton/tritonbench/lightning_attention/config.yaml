compile_command:
- python lightning_attention.py
correctness_command:
- python lightning_attention_perf.py
performance_command:
- tb_eval -f lightning_attention.py -o lightning_attention_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton code defines a custom attention mechanism in PyTorch using\
    \ the Triton library. This attention mechanism is implemented as a custom autograd\
    \ function `LightningAttention2NoDecay` with `forward` and `backward` methods.\
    \ The forward method computes the attention output given input tensors Q (queries),\
    \ K (keys), and V (values), while the backward method computes gradients for Q,\
    \ K, and V given the gradient of the output.\n\n            The `_fwd_kernel`\
    \ is responsible for the forward pass computation. It calculates the attention\
    \ output by processing Q, K, and V in blocks of size `BLOCK` (64). It uses `NUM_BLOCK`\
    \ to determine how many such blocks exist along the sequence dimension. The kernel\
    \ loads segments of Q, K, and V, computes their dot product, and uses the result\
    \ to calculate the output by combining intra-block (within the block) and inter-block\
    \ (between blocks) interactions.\n\n            The `_bwd_intra_kernel` is used\
    \ in the backward pass to compute gradients within each block. It processes the\
    \ gradient of the output (`DO`) and calculates the gradients `DQ`, `DK`, and `DV`\
    \ for each of the input tensors. It uses a block size of `CBLOCK` (32) for sub-block\
    \ computations, iterating over `NUM_BLOCK` blocks.\n\n            The `_bwd_inter_kernel`\
    \ computes gradients involving interactions between blocks. It iteratively updates\
    \ the accumulated gradients for the entire input sequence. It uses the computed\
    \ values from the `_bwd_intra_kernel` to adjust gradients for keys (K) and values\
    \ (V).\n\n            The code uses a grid launch strategy for parallel computation\
    \ across batches and heads, defined by `b * h`, and sequence dimension divided\
    \ into blocks.\n\n            Important parameters and settings include:\n   \
    \         - `BLOCK`: Main block size (64) used in computations.\n            -\
    \ `NUM_BLOCK`: Number of blocks along the sequence dimension.\n            - `CBLOCK`:\
    \ Sub-block size (32) used for intra-block gradient calculations.\n          \
    \  - `NUM_CBLOCK`: Number of sub-blocks within each block for intra operations.\n\
    \n            These kernels are called using a grid defined by `(b * h, cdiv(e,\
    \ BLOCK_MODEL))` for the forward pass and intra-block backward pass, and `(b *\
    \ h,)` for the inter-block backward pass. The context saves Q, K, and V during\
    \ the forward pass to facilitate efficient gradient computation during the backward\
    \ pass.\n            \nThe test code is:\n\n\nimport torch\n\ndef test_lightning_attention2_no_decay():\n\
    \    # \u8F93\u5165\u5F20\u91CF\u7684\u5F62\u72B6\n    b, h, n, d, e = 2, 8, 128,\
    \ 64, 128  # batch_size, num_heads, seq_len, embed_dim, value_dim\n\n    # \u521B\
    \u5EFA\u968F\u673A\u7684 q, k, v \u5F20\u91CF\n    q = torch.randn((b, h, n, d),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n    k = torch.randn((b,\
    \ h, n, d), dtype=torch.float32, device='cuda', requires_grad=True)\n    v = torch.randn((b,\
    \ h, n, e), dtype=torch.float32, device='cuda', requires_grad=True)\n\n    # \u524D\
    \u5411\u4F20\u64AD\n    o = lightning_attn2_no_decay(q, k, v)\n\n    # \u53CD\u5411\
    \u4F20\u64AD\n    o.sum().backward()  # \u8BA1\u7B97\u603B\u548C\u7684\u68AF\u5EA6\
    \n\n    # \u8FD4\u56DEresults\n    results = {\n        'test_case_1': (\n   \
    \         o.cpu().detach().numpy(),  # \u76F4\u63A5\u8FD4\u56DE\u524D\u5411\u4F20\
    \u64AD\u7684\u8F93\u51FA\n            q.grad.cpu().detach().numpy(),  # q\u7684\
    \u68AF\u5EA6\n            k.grad.cpu().detach().numpy(),  # k\u7684\u68AF\u5EA6\
    \n            v.grad.cpu().detach().numpy()   # v\u7684\u68AF\u5EA6\n        )\n\
    \    }\n    \n    return results\n\n# Run the test\nresult_gold = test_lightning_attention2_no_decay()\n\
    # print(result_gold)\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ lightning_attention.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- lightning_attention
