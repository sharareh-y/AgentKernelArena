compile_command:
- python bgmv_expand_slice.py
correctness_command:
- python bgmv_expand_slice_perf.py
performance_command:
- tb_eval -f bgmv_expand_slice.py -o bgmv_expand_slice_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel `_bgmv_expand_slice_kernel` is designed to optimize\
    \ the matrix-vector multiplication by splitting the N dimension into multiple\
    \ blocks, improving performance for large hidden sizes. It operates on inputs\
    \ from `input_ptr` (input matrix), `lora_ptr` (LoRA weight matrix), and `out_ptr`\
    \ (output matrix), considering the batch and index information provided by `lora_indices`.\
    \ The kernel computes matrix-vector products for each block and optionally accumulates\
    \ these products into the output tensor. Key constants like `BLOCK_N`, `BLOCK_K`,\
    \ `SPLIT_N`, `EVEN_K`, `ADD_INPUTS`, and `CAST_TYPE` control the block dimensions,\
    \ input data handling, and type casting behavior. The `_bgmv_expand_slice` function,\
    \ marked with `@torch.inference_mode()`, initializes tensor properties, validates\
    \ shapes, and prepares the grid configuration before invoking the Triton kernel,\
    \ ensuring data and operational integrity.\n        \nThe test code is:\n\n\n\
    import torch\n\ndef test_bgmv_expand_slice():\n    # Define test inputs\n    batch_size\
    \ = 4\n    hidden_size = 128\n    rank = 64\n    lora_num = 3\n\n    inputs =\
    \ torch.randn(batch_size, hidden_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    lora_b_weights = torch.randn(lora_num, rank, hidden_size, dtype=torch.float16,\
    \ device='cuda').contiguous()\n    output_tensor = torch.zeros(batch_size, rank,\
    \ dtype=torch.float16, device='cuda').contiguous()\n    lora_indices_tensor =\
    \ torch.tensor([0, 1, -1, 2], dtype=torch.int32, device='cuda')\n    slice_offset\
    \ = 0\n    slice_size = 64\n\n    results = {}\n\n    # Test case 1: Basic functionality\
    \ with add_inputs=True\n    _bgmv_expand_slice(\n        inputs=inputs,\n    \
    \    lora_b_weights=lora_b_weights,\n        output_tensor=output_tensor,\n  \
    \      lora_indices_tensor=lora_indices_tensor,\n        slice_offset=slice_offset,\n\
    \        slice_size=slice_size,\n        add_inputs=True\n    )\n    results['test_case_1']\
    \ = output_tensor.clone()\n\n    # Test case 2: Basic functionality with add_inputs=False\n\
    \    output_tensor_zero = torch.zeros_like(output_tensor)\n    _bgmv_expand_slice(\n\
    \        inputs=inputs,\n        lora_b_weights=lora_b_weights,\n        output_tensor=output_tensor_zero,\n\
    \        lora_indices_tensor=lora_indices_tensor,\n        slice_offset=slice_offset,\n\
    \        slice_size=slice_size,\n        add_inputs=False\n    )\n    results['test_case_2']\
    \ = output_tensor_zero.clone()\n\n    # Test case 3: With casting from float32\
    \ to float16\n    inputs_float32 = inputs.to(torch.float32)\n    _bgmv_expand_slice(\n\
    \        inputs=inputs_float32,\n        lora_b_weights=lora_b_weights,\n    \
    \    output_tensor=output_tensor,\n        lora_indices_tensor=lora_indices_tensor,\n\
    \        slice_offset=slice_offset,\n        slice_size=slice_size,\n        add_inputs=True\n\
    \    )\n    results['test_case_3'] = output_tensor.clone()\n\n    return results\n\
    \nresult_gold = test_bgmv_expand_slice()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ bgmv_expand_slice.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- bgmv_expand_slice
