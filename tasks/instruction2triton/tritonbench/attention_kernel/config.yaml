compile_command:
- python attention_kernel.py
correctness_command:
- python attention_kernel_perf.py
performance_command:
- tb_eval -f attention_kernel.py -o attention_kernel_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel _fwd_kernel_aligned performs scaled dot-product attention\
    \ using queries (Q), keys (K), values (V), and a bias matrix (B0) to compute the\
    \ output (Out). The kernel operates using blocks of size defined by BLOCK_M, BLOCK_N,\
    \ and BLOCK_DMODEL, enabling efficient memory usage and computation. The kernel\
    \ processes data by dividing the input matrices Q, K, and V into smaller blocks,\
    \ loading these into shared memory, and iteratively processing them using dot\
    \ products and bias additions. The computation involves scaling the dot products\
    \ by sm_scale, adding biases from B0, and applying a numerically stable softmax\
    \ operation using tl.math.exp2. Accumulated results are normalized before being\
    \ written back to the output matrix Out. The kernel is parallelized using Triton's\
    \ grid, spreading tasks across the Q's head dimension and batch size.\n\n    \
    \    The function _attention_rel_h_rel_w_kernel_aligned_device sets up and launches\
    \ the Triton kernel. It first validates that input tensor shapes and types are\
    \ consistent, particularly ensuring q, k, v, and rel_h_w are compatible and have\
    \ expected dimensions. It configures a computation grid based on the third dimension\
    \ of Q and its batch size. The function calculates necessary constants like OUT_DTYPE\
    \ for the precision and BIAS_LAST_SIZE for bias processing. It then invokes the\
    \ _fwd_kernel_aligned kernel with specified parameters such as tensor pointers,\
    \ strides, block sizes, and tuning parameters (num_warps, num_stages) to optimize\
    \ the execution.\n    \nThe test code is:\n\n\nimport torch\n\ndef test_attention_rel_h_rel_w_kernel_aligned_device():\n\
    \    # Define the input parameters\n    BATCH_SIZE = 2\n    HEADS = 4\n    N_CTX\
    \ = 128\n    BLOCK_M = 64\n    BLOCK_N = 64\n    D_MODEL = 128\n    SM_SCALE =\
    \ 0.1\n\n    # Create input tensors with appropriate shapes and data types\n \
    \   q = torch.randn((BATCH_SIZE, HEADS, N_CTX, D_MODEL), dtype=torch.float16,\
    \ device='cuda')\n    k = torch.randn((BATCH_SIZE, HEADS, N_CTX, D_MODEL), dtype=torch.float16,\
    \ device='cuda')\n    v = torch.randn((BATCH_SIZE, HEADS, N_CTX, D_MODEL), dtype=torch.float16,\
    \ device='cuda')\n    rel_h_w = torch.randn((BATCH_SIZE, HEADS, N_CTX, 128), dtype=torch.float16,\
    \ device='cuda')\n    o = torch.empty((BATCH_SIZE, HEADS, N_CTX, D_MODEL), dtype=torch.float16,\
    \ device='cuda')\n\n    # Create a dictionary to store the results of different\
    \ test cases\n    test_case_results = {}\n\n    # Test case 1: Default case with\
    \ P_SEQ = 0\n    P_SEQ = 0\n    _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, SM_SCALE, o,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n\
    \        num_warps=4,\n        num_stages=2\n    )\n    test_case_results['test_case_1']\
    \ = o.clone()\n\n    # Test case 2: Change P_SEQ to a non-zero value\n    P_SEQ\
    \ = 10  # Arbitrary non-zero value\n    _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, SM_SCALE, o,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n\
    \        num_warps=4,\n        num_stages=2\n    )\n    test_case_results['test_case_2']\
    \ = o.clone()\n\n    # Test case 3: Change number of warps\n    num_warps = 8\
    \  # Arbitrary non-zero value\n    _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, SM_SCALE, o,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n\
    \        num_warps=num_warps,\n        num_stages=2\n    )\n    test_case_results['test_case_3']\
    \ = o.clone()\n\n    # Test case 4: Change number of stages\n    num_stages =\
    \ 1  # Arbitrary non-zero value\n    _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, SM_SCALE, o,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n\
    \        num_warps=4,\n        num_stages=num_stages\n    )\n    test_case_results['test_case_4']\
    \ = o.clone()\n\n    return test_case_results\n\n\n# Execute the test function\n\
    result_gold = test_attention_rel_h_rel_w_kernel_aligned_device()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py attention_kernel.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attention_kernel
