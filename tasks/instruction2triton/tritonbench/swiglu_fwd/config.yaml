compile_command:
- python swiglu_fwd.py
correctness_command:
- python swiglu_fwd_perf.py
performance_command:
- tb_eval -f swiglu_fwd.py -o swiglu_fwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel `_swiglu_fwd_kernel` is designed to perform the Swiglu\
    \ operation for the forward pass, where each element of the input `X` is multiplied\
    \ by its sigmoid activation and then multiplied by the corresponding element in\
    \ `Y`. The kernel is implemented with `@triton.jit`, allowing it to run on the\
    \ GPU efficiently. It is autotuned over various configurations to handle different\
    \ block sizes, optimizing performance based on the `ncols` parameter. The kernel\
    \ uses Triton's `program_id` to determine which row and block of columns to process.\
    \ The `load` function fetches data from the input tensors, and the `store` function\
    \ writes the result to the `OUT` tensor. The function `_swiglu_fwd` prepares input\
    \ tensors `x` and `y`, ensuring they are contiguous and splitting them from the\
    \ input `xy`. It also reshapes the tensors as needed and initializes the output\
    \ tensor. The grid is defined based on the number of rows (`M`) and columns (`N`)\
    \ using `triton.cdiv` to calculate blocks per column. The kernel is then launched\
    \ with the specified grid configuration.\n    \nThe test code is:\n\n\n# Test\
    \ the forward function with different configurations\ndef test_swiglu_fwd():\n\
    \    results = {}\n    # Test case 1\n    batch_size = 4\n    ncols = 128\n  \
    \  xy = torch.randn(batch_size, 2 * ncols, device='cuda', dtype=torch.float32)\n\
    \    out = _swiglu_fwd(xy)\n    results['test_case_1'] = out.detach().cpu()\n\n\
    \    # Test case 2\n    batch_size = 8\n    ncols = 256\n    xy = torch.randn(batch_size,\
    \ 2 * ncols, device='cuda', dtype=torch.float32)\n    out = _swiglu_fwd(xy)\n\
    \    results['test_case_2'] = out.detach().cpu()\n\n    # Test case 3\n    batch_size\
    \ = 16\n    ncols = 512\n    xy = torch.randn(batch_size, 2 * ncols, device='cuda',\
    \ dtype=torch.float32)\n    out = _swiglu_fwd(xy)\n    results['test_case_3']\
    \ = out.detach().cpu()\n\n    # Test case 4\n    batch_size = 32\n    ncols =\
    \ 1024\n    xy = torch.randn(batch_size, 2 * ncols, device='cuda', dtype=torch.float32)\n\
    \    out = _swiglu_fwd(xy)\n    results['test_case_4'] = out.detach().cpu()\n\n\
    \    return results\n\n# Run the tests\nresult_gold = test_swiglu_fwd()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py swiglu_fwd.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- swiglu_fwd
