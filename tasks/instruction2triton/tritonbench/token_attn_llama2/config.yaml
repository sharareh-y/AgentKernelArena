compile_command:
- python token_attn_llama2.py
correctness_command:
- python token_attn_llama2_perf.py
performance_command:
- tb_eval -f token_attn_llama2.py -o token_attn_llama2_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The Triton kernel function '_fwd_kernel_token_att1' performs a forward pass for\
    \ a token attention mechanism, often used in transformer models for calculating\
    \ the attention scores between query (Q) and key (K) tensors. This kernel accepts\
    \ various input tensors, including 'Q' and 'K', which represent the query and\
    \ key matrices, 'B_Loc', which provides positional information about the keys,\
    \ 'B_Start_Loc' and 'B_Seqlen', which give sequence start indices and lengths,\
    \ and 'max_input_len' as the maximum sequence length within a batch. The output\
    \ 'Att_Out' stores the computed attention values. Internally, the kernel uses\
    \ several predefined strides to navigate through the tensor dimensions. For each\
    \ batch, head, and block of sequences defined by the block size 'BLOCK_N', it\
    \ loads segments of the query and key tensors, computes their dot product, scales\
    \ it by 'sm_scale', and stores the result back. The grid configuration ensures\
    \ coverage across all required dimensions with specific 'num_warps' for parallel\
    \ processing. The wrapper function 'token_att_fwd' initializes and launches this\
    \ kernel, checking the compatibility of input dimensions and adjusting the scaling\
    \ factor based on the key dimension.\n    \nThe test code is:\n\n\nimport torch\n\
    \ndef test_token_att_fwd():\n    # Define the input parameters\n    batch_size\
    \ = 2\n    head_num = 4\n    max_input_len = 64\n    d_model = 32  # This should\
    \ be one of {16, 32, 64, 128}\n\n    # Create random input tensors\n    q = torch.randn((batch_size,\
    \ head_num, max_input_len, d_model), dtype=torch.float32, device='cuda')\n   \
    \ k = torch.randn((batch_size, head_num, max_input_len, d_model), dtype=torch.float32,\
    \ device='cuda')\n    att_out = torch.zeros((batch_size, head_num, max_input_len),\
    \ dtype=torch.float32, device='cuda')\n\n    # Create B_Loc, B_Start_Loc, B_Seqlen\n\
    \    B_Loc = torch.randint(0, max_input_len, (batch_size, max_input_len), dtype=torch.int32,\
    \ device='cuda')\n    B_Start_Loc = torch.randint(0, max_input_len, (batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    B_Seqlen = torch.randint(1, max_input_len\
    \ + 1, (batch_size,), dtype=torch.int32, device='cuda')\n\n    # Dictionary to\
    \ store results for each test case\n    results = {}\n\n    # Test case 1\n  \
    \  token_att_fwd(q, k, att_out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len)\n\
    \    results['test_case_1'] = att_out.clone()\n\n    # Additional test cases to\
    \ cover more branches\n    # Test case 2: Different max_input_len\n    max_input_len_2\
    \ = 32\n    att_out_2 = torch.zeros((batch_size, head_num, max_input_len_2), dtype=torch.float32,\
    \ device='cuda')\n    token_att_fwd(q, k, att_out_2, B_Loc, B_Start_Loc, B_Seqlen,\
    \ max_input_len_2)\n    results['test_case_2'] = att_out_2.clone()\n\n    # Test\
    \ case 3: Different d_model\n    d_model_3 = 64\n    q_3 = torch.randn((batch_size,\
    \ head_num, max_input_len, d_model_3), dtype=torch.float32, device='cuda')\n \
    \   k_3 = torch.randn((batch_size, head_num, max_input_len, d_model_3), dtype=torch.float32,\
    \ device='cuda')\n    att_out_3 = torch.zeros((batch_size, head_num, max_input_len),\
    \ dtype=torch.float32, device='cuda')\n    token_att_fwd(q_3, k_3, att_out_3,\
    \ B_Loc, B_Start_Loc, B_Seqlen, max_input_len)\n    results['test_case_3'] = att_out_3.clone()\n\
    \n    # Test case 4: Different batch size\n    batch_size_4 = 4\n    q_4 = torch.randn((batch_size_4,\
    \ head_num, max_input_len, d_model), dtype=torch.float32, device='cuda')\n   \
    \ k_4 = torch.randn((batch_size_4, head_num, max_input_len, d_model), dtype=torch.float32,\
    \ device='cuda')\n    att_out_4 = torch.zeros((batch_size_4, head_num, max_input_len),\
    \ dtype=torch.float32, device='cuda')\n    B_Loc_4 = torch.randint(0, max_input_len,\
    \ (batch_size_4, max_input_len), dtype=torch.int32, device='cuda')\n    B_Start_Loc_4\
    \ = torch.randint(0, max_input_len, (batch_size_4,), dtype=torch.int32, device='cuda')\n\
    \    B_Seqlen_4 = torch.randint(1, max_input_len + 1, (batch_size_4,), dtype=torch.int32,\
    \ device='cuda')\n    token_att_fwd(q_4, k_4, att_out_4, B_Loc_4, B_Start_Loc_4,\
    \ B_Seqlen_4, max_input_len)\n    results['test_case_4'] = att_out_4.clone()\n\
    \n    return results\n\n# Execute the test function\nresult_gold = test_token_att_fwd()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py token_attn_llama2.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- token_attn_llama2
