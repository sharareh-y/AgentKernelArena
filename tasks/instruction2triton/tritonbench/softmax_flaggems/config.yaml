compile_command:
- python softmax_flaggems.py
correctness_command:
- python softmax_flaggems_perf.py
performance_command:
- tb_eval -f softmax_flaggems.py -o softmax_flaggems_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    This Triton script provides a specialized implementation of the softmax function,\
    \ optimized for GPU computation. It includes both forward and backward passes\
    \ for inner and non-inner dimensions, using Triton's kernel execution framework\
    \ with tunable parameters to achieve optimal performance.\n\n### Key Components:\n\
    \n- **Kernel Functions**:\n  - `softmax_kernel_non_inner`: This function computes\
    \ softmax over non-inner dimensions of a tensor. It uses the `TILE_K` and `TILE_N`\
    \ parameters to define tile sizes, with `ONE_TILE_PER_CTA` indicating if a single\
    \ tile is processed per CTA.\n  - `softmax_kernel_inner`: Similar to the non-inner\
    \ version, but optimized for inner dimension processing, focusing on maximizing\
    \ the efficient use of shared memory.\n  - `softmax_backward_kernel_non_inner`\
    \ & `softmax_backward_kernel_inner`: These handle the backpropagation phase, computing\
    \ the gradient based on the forward pass results. \n\n- **Heuristic Functions**:\n\
    \  - `heur_tile_k`, `heur_tile_n_non_inner`, etc., determine optimal tile sizes\
    \ (`TILE_K`, `TILE_N`) based on input dimensions M, N, K. These heuristics help\
    \ balance the workload across available SMs (Streaming Multiprocessors) on the\
    \ GPU.\n\n- **Softmax Class**:\n  - Provides an autograd function with a custom\
    \ forward and backward pass for PyTorch integration. The forward method executes\
    \ the softmax operation using a Triton kernel, and the backward method computes\
    \ the gradient.\n\n### Execution:\n- For forward pass, `softmax_kernel_non_inner`\
    \ is invoked when K > 1, otherwise `softmax_kernel_inner` is used. Similar logic\
    \ applies to the backward pass with the respective backward kernels.\n- Grid execution\
    \ parameters are dynamically configured using the `grid` lambda functions.\n\n\
    ### Inputs/Outputs:\n- **Inputs**: `output_ptr`, `input_ptr`, and dimensions M,\
    \ N, K. \n- **Outputs**: Computed softmax or gradient stored in `output_ptr` or\
    \ `in_grad_ptr` respectively.\n\nBy splitting the softmax calculation into tiles\
    \ and using Triton's parallel execution capabilities, this implementation leverages\
    \ GPU resources effectively to provide scalable and high-performance softmax operations.\n\
    \nThe test code is:\n\n\ndef test_softmax():\n    # \u521B\u5EFA\u4E00\u4E2A\u5B57\
    \u5178\u7528\u4E8E\u4FDD\u5B58\u6BCF\u4E2A\u5206\u652F\u7684\u7ED3\u679C\n   \
    \ result = {}\n\n    # Test case 1: 1D tensor, float32, default dim=-1\n    x_1d\
    \ = torch.rand((10,), device='cuda', dtype=torch.float32)\n    out_1d = softmax(x_1d)\n\
    \    result[\"test_case_1\"] = out_1d\n\n    # Test case 2: 2D tensor, float32,\
    \ dim=1\n    x_2d = torch.rand((4, 5), device='cuda', dtype=torch.float32)\n \
    \   out_2d = softmax(x_2d, dim=1)\n    result[\"test_case_2\"] = out_2d\n\n  \
    \  # Test case 3: 2D tensor, float16, dim=0\n    x_2d_fp16 = torch.rand((4, 5),\
    \ device='cuda', dtype=torch.float16)\n    out_2d_fp16 = softmax(x_2d_fp16, dim=0)\n\
    \    result[\"test_case_3\"] = out_2d_fp16\n\n    # Test case 4: 3D tensor, float32,\
    \ default dim=-1\n    x_3d = torch.rand((2, 3, 4), device='cuda', dtype=torch.float32)\n\
    \    out_3d = softmax(x_3d)\n    result[\"test_case_4\"] = out_3d\n\n    # Test\
    \ case 5: 3D tensor, float64, dim=1\n    x_3d_fp64 = torch.rand((2, 3, 4), device='cuda',\
    \ dtype=torch.float64)\n    out_3d_fp64 = softmax(x_3d_fp64, dim=1)\n    result[\"\
    test_case_5\"] = out_3d_fp64\n\n    # Test case 6: 4D tensor, float32, with large\
    \ K dimension\n    x_4d_large_k = torch.rand((2, 3, 4, 1024), device='cuda', dtype=torch.float32)\n\
    \    out_4d_large_k = softmax(x_4d_large_k, dim=-1)\n    result[\"test_case_6\"\
    ] = out_4d_large_k\n\n    # Test case 7: Single-element tensor, float32\n    x_single\
    \ = torch.tensor([1.0], device='cuda', dtype=torch.float32)\n    out_single =\
    \ softmax(x_single)\n    result[\"test_case_7\"] = out_single\n\n    # Test case\
    \ 8: Large tensor, float32, with large N dimension\n    x_large = torch.rand((1024,\
    \ 1024), device='cuda', dtype=torch.float32)\n    out_large = softmax(x_large,\
    \ dim=1)\n    result[\"test_case_8\"] = out_large\n\n    # Test case 9: Tensor\
    \ with Inf and -Inf values, checking numerical stability\n    x_inf = torch.tensor([float('inf'),\
    \ -float('inf')], device='cuda', dtype=torch.float32)\n    out_inf = softmax(x_inf)\n\
    \    result[\"test_case_9\"] = out_inf\n\n    # Test case 10: Tensor with NaN\
    \ values, checking if the output is NaN\n    x_nan = torch.tensor([float('nan')],\
    \ device='cuda', dtype=torch.float32)\n    out_nan = softmax(x_nan)\n    result[\"\
    test_case_10\"] = out_nan\n\n    # Test case 11: Tensor with specific shape (non-square),\
    \ float32, dim=-1\n    x_shape1 = torch.rand((3, 7), device='cuda', dtype=torch.float32)\n\
    \    out_shape1 = softmax(x_shape1)\n    result[\"test_case_11\"] = out_shape1\n\
    \n    # Test case 12: Tensor with small shape, float16, checking precision and\
    \ sum\n    x_small_fp16 = torch.rand((2, 2), device='cuda', dtype=torch.float16)\n\
    \    out_small_fp16 = softmax(x_small_fp16)\n    result[\"test_case_12\"] = out_small_fp16\n\
    \n    # Test case 13: Large tensor with float16, checking performance and sum\n\
    \    x_large_fp16 = torch.rand((512, 512), device='cuda', dtype=torch.float16)\n\
    \    out_large_fp16 = softmax(x_large_fp16)\n    result[\"test_case_13\"] = out_large_fp16\n\
    \n    # Test case 14: Tensor with extreme values, checking overflow handling\n\
    \    x_extreme = torch.tensor([1e5, -1e5], device='cuda', dtype=torch.float32)\n\
    \    out_extreme = softmax(x_extreme)\n    result[\"test_case_14\"] = out_extreme\n\
    \n    # Test case 15: Very large tensor with float32, testing memory and performance\n\
    \    x_very_large = torch.rand((2048, 2048), device='cuda', dtype=torch.float32)\n\
    \    out_very_large = softmax(x_very_large, dim=1)\n    result[\"test_case_15\"\
    ] = out_very_large\n\n    return result\n\n# \u6267\u884C\u6D4B\u8BD5\nresult_gold\
    \ = test_softmax()\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ softmax_flaggems.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- softmax_flaggems
