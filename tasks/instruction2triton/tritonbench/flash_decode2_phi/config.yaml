compile_command:
- python flash_decode2_phi.py
correctness_command:
- python flash_decode2_phi_perf.py
performance_command:
- tb_eval -f flash_decode2_phi.py -o flash_decode2_phi_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_fwd_kernel_flash_decode_stage2` Triton kernel is a parallel\
    \ computation designed for processing sequences in a neural network context, specifically\
    \ dealing with batches, heads, and sequence blocks. This kernel receives several\
    \ inputs: `B_Seqlen`, `Mid_O`, `Mid_O_LogExpSum`, and `Out`, along with strides\
    \ for indexing. `B_Seqlen` contains sequence lengths per batch, `Mid_O` contains\
    \ intermediate outputs, `Mid_O_LogExpSum` holds log-exp sum values, and `Out`\
    \ will store the final output. The kernel operates over a 2D grid defined by batch\
    \ size and head count (`grid = (batch, head_num)`), with constants `BLOCK_SEQ`\
    \ and `BLOCK_DMODEL` indicating sequence block size and dimension alignment respectively.\n\
    \n            The kernel function operates as follows:\n            - Identifies\
    \ the current batch and head using `tl.program_id`.\n            - Initializes\
    \ accumulators: `sum_exp`, `max_logic`, and `acc` to accumulate exponential logic\
    \ and values.\n            - Loads the current sequence length and calculates\
    \ the number of sequence blocks (`block_n_size`).\n            - Iterates over\
    \ each block, where:\n                - It loads values (`tv`) from `Mid_O` and\
    \ logic sums (`tlogic`) from `Mid_O_LogExpSum`.\n                - Computes the\
    \ maximum logic value across blocks and scales previous accumulations.\n     \
    \           - Updates the accumulators by computing the exponential of adjusted\
    \ logic values and scaling/accumulating.\n            - Stores the final normalized\
    \ result into `Out`, scaling accumulated values by the sum of exponentials.\n\n\
    \            The `flash_decode_stage2` function sets up and invokes this kernel,\
    \ determining dimensions and grid setup based on input tensor shapes. It ensures\
    \ efficient computation by using Triton's parallel execution framework, specifying\
    \ warp and stage numbers.\n            \nThe test code is:\n\n\nimport torch\n\
    \n# Define the test function\ndef test_flash_decode_stage2():\n    # Define the\
    \ parameters for different test cases\n    batch_size = 2\n    head_num = 4\n\
    \    seq_block_num = 3\n    head_dim = 64\n    block_seq = 16\n\n    test_cases\
    \ = {\n        \"test_case_1\": {\n            \"B_Seqlen\": torch.randint(1,\
    \ seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n\
    \            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim),\
    \ dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size,\
    \ head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n           \
    \ \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32,\
    \ device='cuda'),\n            \"block_seq\": block_seq\n        },\n        \"\
    test_case_2\": {\n            \"B_Seqlen\": torch.randint(1, seq_block_num * block_seq,\
    \ (batch_size,), dtype=torch.int32, device='cuda'),\n            \"mid_out\":\
    \ torch.randn((batch_size, head_num, seq_block_num, head_dim), dtype=torch.float32,\
    \ device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size,\
    \ head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n           \
    \ \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32,\
    \ device='cuda'),\n            \"block_seq\": block_seq + 1  # Different block\
    \ size\n        },\n        \"test_case_3\": {\n            \"B_Seqlen\": torch.randint(1,\
    \ seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n\
    \            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim),\
    \ dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size,\
    \ head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n           \
    \ \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32,\
    \ device='cuda'),\n            \"block_seq\": block_seq // 2  # Different block\
    \ size\n        },\n        \"test_case_4\": {\n            \"B_Seqlen\": torch.randint(1,\
    \ seq_block_num * block_seq, (batch_size,), dtype=torch.int32, device='cuda'),\n\
    \            \"mid_out\": torch.randn((batch_size, head_num, seq_block_num, head_dim),\
    \ dtype=torch.float32, device='cuda'),\n            \"mid_out_logexpsum\": torch.randn((batch_size,\
    \ head_num, seq_block_num), dtype=torch.float32, device='cuda'),\n           \
    \ \"Out\": torch.zeros((batch_size, head_num, head_dim), dtype=torch.float32,\
    \ device='cuda'),\n            \"block_seq\": block_seq * 2  # Different block\
    \ size\n        }\n    }\n\n    # Execute the function for all test cases\n  \
    \  results = {}\n    for key, test_case in test_cases.items():\n        flash_decode_stage2(test_case[\"\
    mid_out\"], test_case[\"mid_out_logexpsum\"], test_case[\"B_Seqlen\"], test_case[\"\
    Out\"], test_case[\"block_seq\"])\n        results[key] = test_case[\"Out\"]\n\
    \n    return results\n\n# Run the test\nresult_gold = test_flash_decode_stage2()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py flash_decode2_phi.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- flash_decode2_phi
