compile_command:
- python matmul_triton2.py
correctness_command:
- python matmul_triton2_perf.py
performance_command:
- tb_eval -f matmul_triton2.py -o matmul_triton2_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton matrix multiplication code is organized around the `matmul_kernel`\
    \ function, designed for GPU execution. \n        This function is auto-tuned\
    \ for efficiency, adapting to different matrix shapes with several block configurations\
    \ specified via the `@triton.autotune` decorator. Each configuration varies in\
    \ block sizes, stages, and warps, optimizing for a given MxNxK problem size. \n\
    \        The kernel itself operates as follows:\n        - `pid` determines the\
    \ unique program ID within the grid, dividing the workload among GPU units.\n\
    \        - The grid's structure is defined such that the matrix is divided into\
    \ tiles of size BLOCK_SIZE_M x BLOCK_SIZE_N.\n        - `num_pid_m` and `num_pid_n`\
    \ calculate how many blocks fit in each matrix dimension.\n        - Each program\
    \ computes part of the result matrix C for a given (M, N) tile by iterating over\
    \ the K dimension in blocks of BLOCK_SIZE_K.\n        - Pointers `a_ptrs` and\
    \ `b_ptrs` are established to access relevant submatrices.\n        - The kernel\
    \ loops over `K` in chunks of BLOCK_SIZE_K, loading submatrices from A and B,\
    \ performing a dot product, and accumulating the result.\n        - Finally, results\
    \ are stored into matrix C with appropriate masking to handle edge cases. \n \
    \       The `triton_matmul` function provides a Pythonic interface for calling\
    \ the kernel, performing dimension checks, setting up the output matrix, and configuring\
    \ the execution grid based on input dimensions and kernel metadata.\n        \n\
    The test code is:\n\n\nimport torch\n\n# Function to compare results of Triton\
    \ and PyTorch matmul\ndef test_matmul():\n    results = {}\n    \n    # Test case\
    \ 1\n    M, K, N = 256, 256, 256\n    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n    c_triton_1\
    \ = triton_matmul(a, b)\n    results['test_case_1'] = c_triton_1\n\n    # Test\
    \ case 2\n    M, K, N = 64, 64, 64\n    a = torch.randn((M, K), device='cuda',\
    \ dtype=torch.float32)\n    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n\
    \    c_triton_2 = triton_matmul(a, b)\n    results['test_case_2'] = c_triton_2\n\
    \n    # Test case 3\n    M, K, N = 16, 16, 16\n    a = torch.randn((M, K), device='cuda',\
    \ dtype=torch.float32)\n    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n\
    \    c_triton_3 = triton_matmul(a, b)\n    results['test_case_3'] = c_triton_3\n\
    \n    return results\n\n# Run the comparison\nresult_gold = test_matmul()\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matmul_triton2.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_triton2
