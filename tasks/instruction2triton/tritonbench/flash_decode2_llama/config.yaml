compile_command:
- python flash_decode2_llama.py
correctness_command:
- python flash_decode2_llama_perf.py
performance_command:
- tb_eval -f flash_decode2_llama.py -o flash_decode2_llama_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_fwd_kernel_flash_decode_stage2` is a Triton JIT-compiled kernel\
    \ function designed to perform efficient tensor calculations for each batch and\
    \ head within a defined grid. It processes two main input tensors, `Mid_O` and\
    \ `Mid_O_LogExpSum`, using their strides and other inputs to compute and store\
    \ results in the output tensor `O`. Key parameters include:\n            - `B_Seqlen`:\
    \ Provides sequence lengths for the batches.\n            - `Mid_O`: A tensor\
    \ of shape [batch, head, seq_block_num, head_dim] used in weighted sum calculations.\n\
    \            - `Mid_O_LogExpSum`: A tensor [batch, head, seq_block_num] storing\
    \ log-exp sum values for each sequence block.\n            - `O`: A tensor [batch,\
    \ head, head_dim] for storing the final accumulated and normalized output.\n \
    \           - Stride parameters for tensors ensure correct indexing: `stride_mid_ob`,\
    \ `stride_mid_oh`, `stride_mid_os`, `stride_mid_od` for `Mid_O`, and similar for\
    \ others.\n            - Constants `BLOCK_SEQ` and `BLOCK_DMODEL` define the block\
    \ sizes for sequences and model dimensions, which guide iteration within the kernel.\n\
    \            The kernel uses a double loop: over sequence blocks to accumulate\
    \ weighted values and a vectorized computation for efficiency. Inside the loop,\
    \ it computes new logic values, scales existing accumulations, and adjusts for\
    \ numerical stability. Finally, it normalizes the accumulation by the sum of exponentials\
    \ and writes the result back to `O`. \n            The wrapper function `flash_decode_stage2`\
    \ is a PyTorch-compatible function that sets up the computation grid and launches\
    \ this kernel. It checks the compatibility of model dimensions (`Lk`) and passes\
    \ the appropriate arguments to the kernel.\n            \nThe test code is:\n\n\
    \nimport torch\n\n# Define the test function\ndef test_flash_decode_stage2():\n\
    \    # Define the parameters\n    batch_size = 2\n    head_num = 4\n    seq_block_num\
    \ = 3\n    head_dim = 32  # This should be one of {16, 32, 64, 128}\n    block_seq\
    \ = 8\n\n    results = {}\n\n    # Create input tensors for test case 1\n    B_Seqlen_1\
    \ = torch.tensor([24, 16], dtype=torch.int32, device='cuda')\n    mid_out_1 =\
    \ torch.randn(batch_size, head_num, seq_block_num, head_dim, dtype=torch.float32,\
    \ device='cuda')\n    mid_out_logexpsum_1 = torch.randn(batch_size, head_num,\
    \ seq_block_num, dtype=torch.float32, device='cuda')\n    O_1 = torch.empty(batch_size,\
    \ head_num, head_dim, dtype=torch.float32, device='cuda')\n    # Call the wrapper\
    \ function\n    flash_decode_stage2(mid_out_1, mid_out_logexpsum_1, B_Seqlen_1,\
    \ O_1, block_seq)\n    results['test_case_1'] = O_1.clone().cpu()\n\n    # Create\
    \ input tensors for test case 2\n    B_Seqlen_2 = torch.tensor([0, 0], dtype=torch.int32,\
    \ device='cuda')  # Edge case: zero sequence lengths\n    mid_out_2 = torch.randn(batch_size,\
    \ head_num, seq_block_num, head_dim, dtype=torch.float32, device='cuda')\n   \
    \ mid_out_logexpsum_2 = torch.randn(batch_size, head_num, seq_block_num, dtype=torch.float32,\
    \ device='cuda')\n    O_2 = torch.empty(batch_size, head_num, head_dim, dtype=torch.float32,\
    \ device='cuda')\n    # Call the wrapper function\n    flash_decode_stage2(mid_out_2,\
    \ mid_out_logexpsum_2, B_Seqlen_2, O_2, block_seq)\n    results['test_case_2']\
    \ = O_2.clone().cpu()\n\n    # Create input tensors for test case 3\n    B_Seqlen_3\
    \ = torch.tensor([8, 8], dtype=torch.int32, device='cuda')  # Edge case: minimum\
    \ non-zero sequence lengths\n    mid_out_3 = torch.randn(batch_size, head_num,\
    \ seq_block_num, head_dim, dtype=torch.float32, device='cuda')\n    mid_out_logexpsum_3\
    \ = torch.randn(batch_size, head_num, seq_block_num, dtype=torch.float32, device='cuda')\n\
    \    O_3 = torch.empty(batch_size, head_num, head_dim, dtype=torch.float32, device='cuda')\n\
    \    # Call the wrapper function\n    flash_decode_stage2(mid_out_3, mid_out_logexpsum_3,\
    \ B_Seqlen_3, O_3, block_seq)\n    results['test_case_3'] = O_3.clone().cpu()\n\
    \n    # Create input tensors for test case 4\n    B_Seqlen_4 = torch.tensor([32,\
    \ 64], dtype=torch.int32, device='cuda')  # Larger sequence lengths\n    mid_out_4\
    \ = torch.randn(batch_size, head_num, seq_block_num, head_dim, dtype=torch.float32,\
    \ device='cuda')\n    mid_out_logexpsum_4 = torch.randn(batch_size, head_num,\
    \ seq_block_num, dtype=torch.float32, device='cuda')\n    O_4 = torch.empty(batch_size,\
    \ head_num, head_dim, dtype=torch.float32, device='cuda')\n    # Call the wrapper\
    \ function\n    flash_decode_stage2(mid_out_4, mid_out_logexpsum_4, B_Seqlen_4,\
    \ O_4, block_seq)\n    results['test_case_4'] = O_4.clone().cpu()\n\n    return\
    \ results\n\n# Execute the test function\nresult_gold = test_flash_decode_stage2()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py flash_decode2_llama.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- flash_decode2_llama
