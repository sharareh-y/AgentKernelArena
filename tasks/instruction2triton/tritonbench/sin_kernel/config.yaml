compile_command:
- python sin_kernel.py
correctness_command:
- python sin_kernel_perf.py
performance_command:
- tb_eval -f sin_kernel.py -o sin_kernel_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The code implements a Triton kernel named `kernel_function`, which\
    \ processes input data using block-wise operations. \n            The kernel takes\
    \ pointers to input and output data (`x_ptr` and `output_ptr`), the total number\
    \ of elements to process (`n_elements`), and a constant block size (`BLOCK_SIZE`).\
    \ \n            Inside the kernel, each program instance calculates its starting\
    \ point (`block_start`) and creates an `offsets` tensor for element indexing.\
    \ \n            A mask ensures operations only occur on valid indices within the\
    \ input bounds. The kernel loads data from `x_ptr`, computes the sine using `tl.math.sin`,\
    \ and stores the result in `output_ptr`. \n            The `call_kernel` function\
    \ prepares to execute the kernel by calculating the total number of elements (`n_elements`)\
    \ and creates an output tensor. \n            It defines a grid configuration\
    \ function using lambda to handle thread block calculations based on `BLOCK_SIZE`,\
    \ ensuring the entire input is processed. \n            The kernel is then launched\
    \ with the grid configuration, input, output, and element count.\n           \
    \ \nThe test code is:\n\n\nimport torch\n\n# Function to test the Triton kernel\n\
    def test_call_kernel():\n    results = {}\n    \n    # Test case 1: Small input\
    \ tensor\n    x1 = torch.tensor([0.0, 1.0, 2.0, 3.0], dtype=torch.float32).cuda()\n\
    \    output1 = call_kernel(x1)\n    results['test_case_1'] = output1\n    \n \
    \   # Test case 2: Larger input tensor\n    x2 = torch.linspace(0, 10, steps=1024,\
    \ dtype=torch.float32).cuda()\n    output2 = call_kernel(x2)\n    results['test_case_2']\
    \ = output2\n\n    # Test case 3: Edge case with zero elements\n    x3 = torch.tensor([],\
    \ dtype=torch.float32).cuda()\n    output3 = call_kernel(x3)\n    results['test_case_3']\
    \ = output3\n\n    # Test case 4: Input tensor with negative values\n    x4 =\
    \ torch.tensor([-1.0, -2.0, -3.0, -4.0], dtype=torch.float32).cuda()\n    output4\
    \ = call_kernel(x4)\n    results['test_case_4'] = output4\n    \n    return results\n\
    \n# Run the test function\nresult_gold = test_call_kernel()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py sin_kernel.py {kernel_path}` to check\
    \ the correctness and performance.The kernel_path is where you stored the generated\
    \ code.\nCall Status means whether the code can be executed, Exec Status means\
    \ whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- sin_kernel
