compile_command:
- python streamk_matmul.py
correctness_command:
- python streamk_matmul_perf.py
performance_command:
- tb_eval -f streamk_matmul.py -o streamk_matmul_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This Triton-based operator is a matrix multiplication kernel designed\
    \ to optimize computational efficiency using techniques like memory hierarchy\
    \ optimization and tile swizzling. It consists of several key functions:\n\n \
    \           - `swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)`:\
    \ This function calculates 2D tile coordinates from a given linear tile ID using\
    \ a swizzling pattern. It improves L2 cache performance by changing the order\
    \ in which tiles are accessed.\n            - `linear_tile(tile_id, M, N, K, BLOCK_M,\
    \ BLOCK_N, BLOCK_K, GROUP_M)`: Converts a linear tile ID into 2D tile coordinates\
    \ without reordering.\n            - `mac_loop(A, B, C, M, N, K, locks, stride_am,\
    \ stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, iters_per_tile, start_iter,\
    \ end_iter, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE, GROUP_M)`: Computes a portion\
    \ of the matrix multiplication for the given range of iterations (start_iter to\
    \ end_iter). It accumulates results in a local accumulator and handles synchronization\
    \ using locks.\n            - `first_wave(A, B, C, M, N, K, locks, stride_am,\
    \ stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_full_tiles_streamk,\
    \ total_partial_tiles_streamk, iters_per_tile, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE,\
    \ GROUP_M)`: Manages the first set of work-items executed on the hardware, handling\
    \ a batch of tiles efficiently by leveraging Stream-K techniques.\n          \
    \  - `full_tiles(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn,\
    \ stride_cm, stride_cn, total_tiles_streamk, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE,\
    \ GROUP_M)`: Computes tiles left after the initial \"first wave,\" managing the\
    \ remaining work through classical blocking.\n\n            The `matmul` class\
    \ orchestrates the execution, setting up parameters and calling the necessary\
    \ functions. The `_call` method manages grid setup, memory allocation, and kernel\
    \ execution, while `forward` exposes the operation as a PyTorch-compatible function.\
    \ Key input parameters include the dimensions of matrices A and B (M, N, K), block\
    \ sizes (BLK_M, BLK_N, BLK_K), and parallelization configuration (number of stages/warps).\
    \ Output is the product matrix C, calculated from the input matrices A and B.\n\
    \            \nThe test code is:\n\n\nimport torch\n\ndef test_matmul():\n   \
    \ # \u751F\u6210\u968F\u673A\u77E9\u9635\n    M, K, N = 512, 512, 512\n    A =\
    \ torch.randn(M, K, device='cuda', dtype=torch.float32)\n    B = torch.randn(K,\
    \ N, device='cuda', dtype=torch.float32)\n\n    # \u5206\u652F1\n    c_triton_1\
    \ = matmul.forward(None, A, B, grid=16, BLK_M=128, BLK_N=128, BLK_K=32)\n\n  \
    \  # \u5206\u652F2\n    c_triton_2 = matmul.forward(None, A, B, grid=32, BLK_M=64,\
    \ BLK_N=64, BLK_K=16)\n\n    # \u5206\u652F3\n    c_triton_3 = matmul.forward(None,\
    \ A, B, grid=8, BLK_M=32, BLK_N=32, BLK_K=32)\n\n    # \u5206\u652F4\n    c_triton_4\
    \ = matmul.forward(None, A, B, grid=4, BLK_M=256, BLK_N=256, BLK_K=32)\n\n\n \
    \   results = {\n        \"test_case_1\": c_triton_1,\n        \"test_case_2\"\
    : c_triton_2,\n        \"test_case_3\": c_triton_3,\n        \"test_case_4\":\
    \ c_triton_4,\n    }\n    return results\n\nresult_gold = test_matmul()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py streamk_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- streamk_matmul
