compile_command:
- python matrix_reduction.py
correctness_command:
- python matrix_reduction_perf.py
performance_command:
- tb_eval -f matrix_reduction.py -o matrix_reduction_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The kernel 'load_reduce_kernel' is implemented using the Triton library\
    \ to perform an element-wise reduction operation. It reads from an input matrix\
    \ 'x_ptr', computes the maximum value across each row (axis=1), and writes the\
    \ result to an output vector 'y_ptr'. The function accepts several parameters:\
    \ 'x_ptr' and 'y_ptr' are pointers to the input matrix and output vector, respectively;\
    \ 'stride_xm' and 'stride_xn' define the leading and secondary dimension strides\
    \ for accessing the matrix; 'stride_y' is the stride for the output vector. 'BLOCK_M'\
    \ and 'BLOCK_N' are compile-time constants defining the block dimensions. The\
    \ kernel creates a block pointer to manage memory access efficiently, loads a\
    \ block of data, computes the row-wise maxima using tl.max, and stores the output\
    \ using tl.store. The 'load_reduce' function tests this kernel by generating a\
    \ random matrix 'x' and an empty output vector 'y', invoking the kernel with these\
    \ parameters, and comparing the result against PyTorch's max function for correctness\
    \ using 'assert_close'.\n    \nThe test code is:\n\n\nimport torch\n\ndef test_reduce():\n\
    \    # \u6D4B\u8BD5\u53C2\u6570\u8BBE\u7F6E\n    test_cases = [\n        {\"BLOCK_M\"\
    : 16, \"BLOCK_N\": 16, \"dtype_str\": \"float16\"},\n        {\"BLOCK_M\": 32,\
    \ \"BLOCK_N\": 32, \"dtype_str\": \"float16\"},\n        {\"BLOCK_M\": 64, \"\
    BLOCK_N\": 64, \"dtype_str\": \"float32\"},\n        {\"BLOCK_M\": 128, \"BLOCK_N\"\
    : 128, \"dtype_str\": \"float32\"},\n    ]\n\n    results = {}\n    for i, case\
    \ in enumerate(test_cases):\n        BLOCK_M = case[\"BLOCK_M\"]\n        BLOCK_N\
    \ = case[\"BLOCK_N\"]\n        dtype_str = case[\"dtype_str\"]\n\n        try:\n\
    \            load_reduce(BLOCK_M, BLOCK_N, dtype_str)\n            results[f\"\
    test_case_{i+1}\"] = \"passed\"\n        except Exception as e:\n            results[f\"\
    test_case_{i+1}\"] = f\"failed: {e}\"\n\n    return results\n\nresult_gold = test_reduce()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matrix_reduction.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matrix_reduction
