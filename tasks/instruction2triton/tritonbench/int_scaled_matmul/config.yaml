compile_command:
- python int_scaled_matmul.py
correctness_command:
- python int_scaled_matmul_perf.py
performance_command:
- tb_eval -f int_scaled_matmul.py -o int_scaled_matmul_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton operator code showcases the implementation of\
    \ efficient GPU kernels for performing integer matrix multiplication and scaled\
    \ integer matrix multiplication using the Triton framework. The main constructs\
    \ in the code are two kernel functions and two corresponding launch functions.\n\
    \n            1. `@triton.jit def matmul_kernel_with_block_pointers(...)`: This\
    \ kernel computes the matrix product C = A x B for matrices A (MxK) and B (KxN)\
    \ with result C (MxN). It uses block pointers to access matrix elements in chunks\
    \ defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_K`, promoting L2 cache reuse and\
    \ minimizing global memory access latency. The kernel organizes the computation\
    \ through program IDs, which are mapped to blocks of matrix C in a grouped ordering,\
    \ enhancing data locality. The accumulation of results is done in an `accumulator`\
    \ of type `tl.int32` for precision.\n\n            2. `@triton.jit def scaled_matmul_kernel_with_block_pointers(...)`:\
    \ Extending the basic matrix multiplication, this kernel incorporates scaling\
    \ of the output matrix by a scale matrix `scales1`. The approach is similar to\
    \ the previous kernel, with the matrix multiplication results being scaled before\
    \ being stored. The `EVEN_K` parameter allows optimization if K is a multiple\
    \ of the block size.\n\n            3. `def int_matmul_kernel(a, b, c, config)`:\
    \ This is a host function that prepares and launches the `matmul_kernel_with_block_pointers`\
    \ kernel. It calculates the grid size needed for the kernel execution based on\
    \ the dimensions of the matrices A, B, and C, then invokes the kernel with this\
    \ configuration, passing the matrix data pointers and stride information alongside\
    \ execution parameters from the `config` object.\n\n            4. `def int_scaled_matmul_kernel(a,\
    \ b, scales1, c, config)`: Similar to `int_matmul_kernel`, this function sets\
    \ up and launches the `scaled_matmul_kernel_with_block_pointers` kernel. It includes\
    \ handling for the scale matrix `scales1`, adjusting kernel parameters to account\
    \ for matrix sizes and execution configuration specified in the `config` object.\n\
    \n            5. `class Config`: This helper class encapsulates configuration\
    \ settings for the kernel launches, such as `num_warps`, `num_stages`, and `num_ctas`,\
    \ which dictate how the kernel is executed on the GPU. These settings can significantly\
    \ impact the performance and efficiency of the computation on different GPU architectures.\n\
    \n            Overall, the code leverages Triton's ability to generate efficient\
    \ GPU kernels with minimal user intervention by handling boundary checks, block\
    \ pointer manipulations, and execution configurations automatically, aiming to\
    \ achieve high-performance matrix operations suitable for integer arithmetic on\
    \ GPUs.\n            \nThe test code is:\n\n\ndef test_matmul_kernel():\n    M\
    \ = 256\n    K = 128\n    N = 256\n\n    a = torch.randint(-128, 128, (M, K),\
    \ dtype=torch.int8, device='cuda')\n    b = torch.randint(-128, 128, (K, N), dtype=torch.int8,\
    \ device='cuda')\n\n    # \u5206\u914D\u8F93\u51FA\u5F20\u91CF\n    c = torch.empty((M,\
    \ N), dtype=torch.int32, device='cuda')\n\n    # Triton kernel\u914D\u7F6E\u53C2\
    \u6570\n    config = Config(\n        BLOCK_M=64,\n        BLOCK_N=64,\n     \
    \   BLOCK_K=32,\n        GROUP_M=8\n    )\n\n    # \u8C03\u7528\u81EA\u5B9A\u4E49\
    \u7684\u77E9\u9635\u4E58\u6CD5\u5185\u6838\n    c_triton = int_matmul_kernel(a,\
    \ b, c, config)\n\n    scales1 = torch.rand((M, 1), dtype=torch.float32, device='cuda')\
    \  # \u5047\u8BBE\u662F\u6309\u884C\u7F29\u653E\n    c = torch.empty((M, N), dtype=torch.int32,\
    \ device='cuda')\n\n    # Triton kernel\u914D\u7F6E\u53C2\u6570\n    config =\
    \ Config(\n        BLOCK_M=64,\n        BLOCK_N=64,\n        BLOCK_K=32,\n   \
    \     GROUP_M=8\n    )\n\n    # \u8C03\u7528\u5E26\u6709scales\u7684\u77E9\u9635\
    \u4E58\u6CD5\u5185\u6838\n    c_triton_scaled = int_scaled_matmul_kernel(a, b,\
    \ scales1, c, config)\n\n    # Return results in a dictionary\n    results = {\n\
    \        \"test_case_1\": c_triton,\n        \"test_case_2\": c_triton_scaled\n\
    \    }\n    return results\n\nresult_gold = test_matmul_kernel()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py int_scaled_matmul.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int_scaled_matmul
