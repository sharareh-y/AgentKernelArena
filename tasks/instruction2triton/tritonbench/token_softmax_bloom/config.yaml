compile_command:
- python token_softmax_bloom.py
correctness_command:
- python token_softmax_bloom_perf.py
performance_command:
- tb_eval -f token_softmax_bloom.py -o token_softmax_bloom_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton code defines a custom forward-pass kernel function\
    \ `_fwd_kernel_token_softmax` designed for computing the softmax of token logits\
    \ with variable sequence lengths within a batch and multiple attention heads.\
    \ The kernel is launched through the `token_softmax_fwd` function, which orchestrates\
    \ the setup of the kernel's execution parameters based on the input data. The\
    \ main task of `_fwd_kernel_token_softmax` is to handle each batch and head independently\
    \ using two-dimensional parallelism. It loads the appropriate segment of logits\
    \ for each token sequence, applies a numerically stable softmax by subtracting\
    \ the maximum logit value from each element, and normalizes by the total exponentiated\
    \ sum. This operation efficiently computes softmax for padded sequences by masking\
    \ invalid positions using Triton's `mask` mechanism, which replaces them with\
    \ negative infinity. The kernel parameters such as `num_warps` are dynamically\
    \ adjusted based on the block size, enhancing performance across various sequence\
    \ lengths.\n            \nThe test code is:\n\n\nimport torch\n\ndef test_token_softmax_fwd():\n\
    \    results = {}\n    \n    # Test case 1\n    B, N_CTX, H, D = 4, 1025, 12,\
    \ 128\n    dtype = torch.float16\n    Logics = torch.empty((H, B * N_CTX), dtype=dtype,\
    \ device=\"cuda\").normal_(mean=0.1, std=10)\n    ProbOut = torch.empty((H, B\
    \ * N_CTX), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2)\n    b_start_loc\
    \ = torch.zeros((B,), dtype=torch.int32, device=\"cuda\")\n    b_seq_len = torch.zeros((B,),\
    \ dtype=torch.int32, device=\"cuda\")\n    for i in range(B):\n        b_start_loc[i]\
    \ = i * N_CTX\n        b_seq_len[i] = N_CTX\n    token_softmax_fwd(Logics, b_start_loc,\
    \ b_seq_len, ProbOut, N_CTX)\n    results['test_case_1'] = ProbOut.clone()\n\n\
    \    # Test case 2\n    B, N_CTX, H, D = 3, 1025, 12, 128\n    dtype = torch.float16\n\
    \    Logics = torch.empty((H, B * N_CTX), dtype=dtype, device=\"cuda\").normal_(mean=0.1,\
    \ std=10)\n    ProbOut = torch.empty((H, B * N_CTX), dtype=dtype, device=\"cuda\"\
    ).normal_(mean=0.4, std=0.2)\n    B = 4\n    b_start_loc = torch.zeros((B,), dtype=torch.int32,\
    \ device=\"cuda\")\n    b_seq_len = torch.zeros((B,), dtype=torch.int32, device=\"\
    cuda\")\n    b_seq_len[0] = 513\n    b_seq_len[1] = 1025\n    b_seq_len[2] = 513\n\
    \    b_seq_len[3] = 1024\n    for i in range(1, B):\n        b_start_loc[i] =\
    \ b_start_loc[i - 1] + b_seq_len[i - 1]\n    token_softmax_fwd(Logics, b_start_loc,\
    \ b_seq_len, ProbOut, N_CTX)\n    results['test_case_2'] = ProbOut.clone()\n\n\
    \    return results\n\nresult_gold = test_token_softmax_fwd()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py token_softmax_bloom.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- token_softmax_bloom
