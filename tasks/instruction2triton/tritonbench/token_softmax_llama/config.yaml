compile_command:
- python token_softmax_llama.py
correctness_command:
- python token_softmax_llama_perf.py
performance_command:
- tb_eval -f token_softmax_llama.py -o token_softmax_llama_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided code implements a custom Triton kernel for calculating\
    \ softmax probabilities from logits in token sequences. The kernel function '_fwd_kernel_token_softmax'\
    \ is designed to process each batch and head independently, leveraging Triton's\
    \ just-in-time (JIT) compilation capabilities. Inputs include 'Logics', a tensor\
    \ representing the logits; 'B_Start_Loc', indicating the starting indices of each\
    \ sequence in the batch; 'B_Seqlen', giving the length of each sequence; and 'Prob_Out',\
    \ an output tensor where the computed probabilities are stored.\n\n          \
    \  The kernel works by first determining the current batch and head using 'tl.program_id',\
    \ then loading the relevant portion of the logits using these indices. It uses\
    \ the 'BLOCK_SIZE' to handle data in chunks, ensuring it respects sequence boundaries\
    \ with a mask based on 'B_Seqlen'. Softmax computation is done in a numerically\
    \ stable way by subtracting the max value from each element in the row before\
    \ exponentiation, followed by a normalization step with the sum of the exponentials.\n\
    \n            The wrapper function 'token_softmax_fwd' is responsible for setting\
    \ up the kernel launch. It calculates an appropriate 'BLOCK_SIZE' based on 'max_input_len',\
    \ adapts the number of warps used based on 'BLOCK_SIZE' to optimize performance,\
    \ and orchestrates the batch and head dimension handling by launching the Triton\
    \ kernel with these parameters. This function does not compute gradients, indicated\
    \ by '@torch.no_grad()', making it suitable for inference tasks where gradients\
    \ are not needed.\n            \nThe test code is:\n\n\nimport torch\n\n# Define\
    \ the test function\ndef test_token_softmax_fwd():\n    results = {}\n\n    #\
    \ Test case 1: Small input size\n    batch_size = 2\n    head_num = 2\n    max_input_len\
    \ = 8\n\n    # Create random input tensors\n    Logics = torch.randn((head_num,\
    \ batch_size * max_input_len), dtype=torch.float32, device='cuda')\n    B_Start_Loc\
    \ = torch.tensor([0, max_input_len], dtype=torch.int32, device='cuda')\n    B_Seqlen\
    \ = torch.tensor([max_input_len, max_input_len], dtype=torch.int32, device='cuda')\n\
    \    Prob_Out = torch.empty_like(Logics)\n\n    # Call the Triton softmax function\n\
    \    token_softmax_fwd(Logics, B_Start_Loc, B_Seqlen, Prob_Out, max_input_len)\n\
    \n    # Store the output\n    results['test_case_1'] = Prob_Out.clone()\n\n  \
    \  # Test case 2: Larger input size\n    batch_size = 1\n    head_num = 1\n  \
    \  max_input_len = 16\n\n    # Create random input tensors\n    Logics = torch.randn((head_num,\
    \ batch_size * max_input_len), dtype=torch.float32, device='cuda')\n    B_Start_Loc\
    \ = torch.tensor([0], dtype=torch.int32, device='cuda')\n    B_Seqlen = torch.tensor([max_input_len],\
    \ dtype=torch.int32, device='cuda')\n    Prob_Out = torch.empty_like(Logics)\n\
    \n    # Call the Triton softmax function\n    token_softmax_fwd(Logics, B_Start_Loc,\
    \ B_Seqlen, Prob_Out, max_input_len)\n\n    # Store the output\n    results['test_case_2']\
    \ = Prob_Out.clone()\n\n    return results\n\n# Run the test function\nresult_gold\
    \ = test_token_softmax_fwd()\n\n\nDon't append test code to the kernel code or\
    \ edit test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ token_softmax_llama.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- token_softmax_llama
