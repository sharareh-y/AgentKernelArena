compile_command:
- python matmul_dequant_int4.py
correctness_command:
- python matmul_dequant_int4_perf.py
performance_command:
- tb_eval -f matmul_dequant_int4.py -o matmul_dequant_int4_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided code implements two optimized GPU kernels using Triton:\
    \ `matmul4_kernel` and `dequantize_kernel`. Both kernels leverage Triton's JIT\
    \ compilation and autotuning capabilities to efficiently handle specific matrix\
    \ operations commonly used in neural network computations, especially those involving\
    \ quantized weights.\n\n        1. `matmul4_kernel`:\n            - Purpose: Perform\
    \ a quantized matrix multiplication where matrix `B` is in a 4-bit integer format,\
    \ while `A` and `C` are in `float16`.\n            - Inputs: \n              \
    \  - `a_ptr`, `b_ptr`, `c_ptr`: Pointers to matrices `A`, `B`, and `C`.\n    \
    \            - `scales_ptr`, `zeros_ptr`: Pointers to scale and zero-point matrices\
    \ for quantization.\n                - `M`, `N`, `K`: Dimensions of the matrices.\n\
    \                - `stride_am`, `stride_ak`, `stride_bk`, `stride_bn`, `stride_cm`,\
    \ `stride_cn`: Strides for memory access.\n                - `stride_scales_g`,\
    \ `stride_scales_n`, `stride_zeros_g`, `stride_zeros_n`: Strides for scales and\
    \ zeros.\n                - `groupsize`, `NO_GROUPS`: Meta-parameters for group\
    \ handling.\n            - Operation: Multiplies `A` with a dequantized version\
    \ of `B`, using provided scales and zeros to adjust the values. It operates on\
    \ blocks of size `BLOCK_SIZE_M x BLOCK_SIZE_N x BLOCK_SIZE_K`.\n            -\
    \ Characteristics: Handles grouped scaling efficiently by preloading scales and\
    \ zero-points, and decomposes the operation into several small matrix multiplies\
    \ over the specified block sizes.\n\n        2. `dequantize_kernel`:\n       \
    \     - Purpose: Unpacks and dequantizes a 4-bit quantized matrix `B` to full\
    \ precision.\n            - Inputs:\n                - `b_ptr`: Pointer to the\
    \ int4 quantized weights.\n                - `b_scale_ptr`, `b_zp_ptr`: Pointers\
    \ to scale and zero-point matrices.\n                - `fpb_ptr`: Pointer to store\
    \ the dequantized full precision result.\n                - `K`, `N`, `group_size`:\
    \ Dimensions and grouping size.\n                - `stride_bk`, `stride_bn`, `stride_bsk`,\
    \ `stride_bsn`, `stride_bzpk`, `stride_bzpn`, `stride_fpbk`, `stride_fpbn`: Strides\
    \ for respective matrices.\n            - Operation: Converts packed 4-bit integers\
    \ into full-precision floats using group-wise scales and zero-points, processing\
    \ each tile of size `BLOCK_SIZE_K x BLOCK_SIZE_N`.\n\n        Additional Functions:\n\
    \        - `dequantize_int4`: A helper function using `dequantize_kernel` to convert\
    \ an int4 matrix to float.\n        - `matmul_dequantize_int4_s1`: Dequantizes\
    \ matrix `B` using `dequantize_int4` and performs a matrix multiplication with\
    \ `A`.\n        - `quantize_int4`: Quantizes a given weight matrix into a 4-bit\
    \ format, preparing it for use in the above kernels. It calculates scales and\
    \ zero-points based on groups and packs values into `int32`.\n\n        The code\
    \ is structured to facilitate the handling of quantized matrices, optimizing for\
    \ modern hardware's parallel execution capabilities. The kernels are auto-tuned\
    \ for varying hardware specifications, ensuring efficient execution.\n       \
    \ \nThe test code is:\n\n\nimport torch\n\ndef test_correct_int4_s1(M=32, K=4096,\
    \ N=4096):\n    group_size = 128\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale,\
    \ b_zero_point, _ = quantize_int4(b, group_size=group_size)\n    results = {}\n\
    \    \n    # Test case 1\n    triton_output_1 = matmul_dequantize_int4_s1(a, int_b,\
    \ b_scale, b_zero_point, group_size)\n    results['test_case_1'] = triton_output_1\n\
    \    \n    # Test case 2\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale,\
    \ b_zero_point, _ = quantize_int4(b, group_size=256)\n    triton_output_2 = matmul_dequantize_int4_s1(a,\
    \ int_b, b_scale, b_zero_point, 256)\n    results['test_case_2'] = triton_output_2\n\
    \    \n    # Test case 3\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale,\
    \ b_zero_point, _ = quantize_int4(b, group_size=64)\n    triton_output_3 = matmul_dequantize_int4_s1(a,\
    \ int_b, b_scale, b_zero_point, 64)\n    results['test_case_3'] = triton_output_3\n\
    \    \n    # Test case 4\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale,\
    \ b_zero_point, _ = quantize_int4(b, group_size=32)\n    triton_output_4 = matmul_dequantize_int4_s1(a,\
    \ int_b, b_scale, b_zero_point, 32)\n    results['test_case_4'] = triton_output_4\n\
    \    \n    return results\n\nresult_gold = test_correct_int4_s1()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py matmul_dequant_int4.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_dequant_int4
