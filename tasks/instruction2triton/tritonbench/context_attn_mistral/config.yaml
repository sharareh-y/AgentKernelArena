compile_command:
- python context_attn_mistral.py
correctness_command:
- python context_attn_mistral_perf.py
performance_command:
- tb_eval -f context_attn_mistral.py -o context_attn_mistral_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The Triton kernel `_fwd_kernel` implements a scalable sliding window mechanism\
    \ for performing attention operations in a Transformer architecture. It takes\
    \ tensors `Q`, `K`, and `V` as inputs along with other parameters like `sm_scale`\
    \ (to scale query-key scores), and additional metadata (`B_Start_Loc`, `B_Seqlen`)\
    \ for handling batch sequences efficiently.\n\nKey aspects of `_fwd_kernel`:\n\
    - It operates on a grid defined by `(batch, head, sequence blocks)`, processed\
    \ in parallel using Triton.\n- `BLOCK_M`, `BLOCK_DMODEL`, and `BLOCK_N` are compile-time\
    \ constants defining the dimensions of processing blocks.\n- The algorithm iteratively\
    \ processes `K` and `V` blocks, computes attention scores (`qk`) with a sliding\
    \ window constraint to focus attention within a localized sequence window.\n-\
    \ Uses dynamic memory access patterns with offsets to load and store data efficiently.\n\
    - Employs numerically stable operations for computing exponential weights, preventing\
    \ underflows/overflows.\n- Constructs final output via scaling and accumulating\
    \ partial results, stored to the output tensor `Out`.\n\nThe `context_attention_fwd`\
    \ function serves as the interface, defining the grid dimensions, ensuring correct\
    \ input data preparation, and invoking the Triton kernel with proper execution\
    \ parameters for optimized performance.\n    \nThe test code is:\n\n\ndef test_context_attention_fwd():\n\
    \    Z, H, N_CTX, D_HEAD = 4, 6, 1024, 128\n    dtype = torch.float16\n    q =\
    \ torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1,\
    \ std=0.2)\n    k = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.4, std=0.2)\n    v = torch.empty((Z * N_CTX, H, D_HEAD),\
    \ dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n    o = torch.empty((Z\
    \ * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n\
    \n    max_input_len = N_CTX\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32,\
    \ device=\"cuda\")\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"\
    cuda\")\n\n    b_seq_len[0] = 512\n    b_seq_len[1] = 1024\n    b_seq_len[2] =\
    \ 512\n    b_seq_len[3] = 1024\n\n    for i in range(1, Z):\n        b_start_loc[i]\
    \ = b_start_loc[i - 1] + b_seq_len[i - 1]\n\n    results = {}\n    \n    # Test\
    \ case 1\n    context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len,\
    \ 10)\n    results['test_case_1'] = o.clone()\n\n    # Test case 2: Different\
    \ sliding window\n    context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len,\
    \ max_input_len, 20)\n    results['test_case_2'] = o.clone()\n\n    # Test case\
    \ 3: Different max_input_len\n    context_attention_fwd(q, k, v, o, b_start_loc,\
    \ b_seq_len, max_input_len // 2, 10)\n    results['test_case_3'] = o.clone()\n\
    \n    # Test case 4: Different batch size\n    Z = 2\n    q = torch.empty((Z *\
    \ N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n\
    \    k = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4,\
    \ std=0.2)\n    v = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.3, std=0.2)\n    o = torch.empty((Z * N_CTX, H, D_HEAD),\
    \ dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n    b_start_loc =\
    \ torch.zeros((Z,), dtype=torch.int32, device=\"cuda\")\n    b_seq_len = torch.ones((Z,),\
    \ dtype=torch.int32, device=\"cuda\")\n    b_seq_len[0] = 512\n    b_seq_len[1]\
    \ = 1024\n    for i in range(1, Z):\n        b_start_loc[i] = b_start_loc[i -\
    \ 1] + b_seq_len[i - 1]\n    context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len,\
    \ max_input_len, 10)\n    results['test_case_4'] = o.clone()\n\n    return results\n\
    \nresult_gold = test_context_attention_fwd()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ context_attn_mistral.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- context_attn_mistral
