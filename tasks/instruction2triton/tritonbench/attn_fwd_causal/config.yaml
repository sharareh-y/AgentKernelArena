compile_command:
- python attn_fwd_causal.py
correctness_command:
- python attn_fwd_causal_perf.py
performance_command:
- tb_eval -f attn_fwd_causal.py -o attn_fwd_causal_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton-based kernel performs a forward pass of a blockwise attention\
    \ mechanism, specifically designed to handle the query (Q), key (K), and value\
    \ (V) matrices in blocks for efficient computation on GPUs.\n\n            - Function\
    \ `_attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs, start_m,\
    \ BLOCK_M, HEAD_DIM, BLOCK_N, STAGE, offs_m, offs_n, N_CTX)`:\n              This\
    \ function executes the core attention mechanism in two main stages depending\
    \ on the STAGE parameter. In STAGE 1, it initializes by loading key vectors (k)\
    \ and their scaling factors (k_scale). It computes the dot product of Q and K,\
    \ applies scaling, and manages masking for handling causal attention. In both\
    \ stages, it uses exponential operations to perform the softmax calculation in\
    \ a numerically stable manner by leveraging maximum value adjustment (m_ij). It\
    \ accumulates the weighted value vectors to compute the attention output.\n\n\
    \            - Function `_attn_fwd(Q, K, V, Q_scale, K_scale, Out, stride_qz,\
    \ stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\
    \ stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\
    \ stride_on, Z, H, N_CTX, HEAD_DIM, BLOCK_M, BLOCK_N, STAGE)`:\n             \
    \ This function sets up the parameters for block processing and handles the execution\
    \ logic of the forward pass by allocating memory pointers, computing required\
    \ indices, and setting up the grid for parallel execution using the Triton programming\
    \ model. It computes linear indices for each block based on offsets calculated\
    \ from tensor strides, invokes `_attn_fwd_inner` to perform blockwise computation\
    \ of attention scores, and stores the results back into the output tensor.\n\n\
    \            - Function `forward(q, k, v, q_scale, k_scale)`:\n              This\
    \ acts as a wrapper function to manage inputs and invoke the Triton kernel `_attn_fwd`.\
    \ It determines the dimensions of the input tensors, configures the grid for kernel\
    \ launch, and calls `_attn_fwd` with the appropriate settings. It ensures that\
    \ inputs are appropriately batched and shaped to match the expected dimensions,\
    \ manages output allocation, and performs assertions to guarantee consistency\
    \ across input feature dimensions.\n\n            The kernel is optimized for\
    \ parallel execution by leveraging Triton\u2019s capabilities to handle complex\
    \ memory access patterns and perform efficient tensor computations in a blockwise\
    \ manner. Key parameters such as BLOCK_M and BLOCK_N determine the block size,\
    \ while HEAD_DIM specifies the feature dimensionality in attention computation.\
    \ The entire setup enables efficient processing of sequences within attention\
    \ mechanisms typically used in transformer models.\n            \nThe test code\
    \ is:\n\n\nimport torch\n\n# Define the test function\ndef test_forward():\n \
    \   # Define the dimensions\n    batch_size = 2\n    num_heads = 4\n    seq_len\
    \ = 128\n    head_dim = 128\n\n    # Create random input tensors\n    q = torch.randn((batch_size,\
    \ num_heads, seq_len, head_dim), dtype=torch.float16, device='cuda')\n    k =\
    \ torch.randn((batch_size, num_heads, seq_len, head_dim), dtype=torch.float16,\
    \ device='cuda')\n    v = torch.randn((batch_size, num_heads, seq_len, head_dim),\
    \ dtype=torch.float16, device='cuda')\n    q_scale = torch.ones((batch_size, num_heads,\
    \ seq_len), dtype=torch.float32, device='cuda')\n    k_scale = torch.ones((batch_size,\
    \ num_heads, seq_len), dtype=torch.float32, device='cuda')\n\n    # Dictionary\
    \ to store results\n    results = {}\n\n    # Test case 1\n    output1 = forward(q,\
    \ k, v, q_scale, k_scale)\n    results['test_case_1'] = output1\n\n    # Test\
    \ case 2: Different scaling factors\n    q_scale = torch.full((batch_size, num_heads,\
    \ seq_len), 0.5, dtype=torch.float32, device='cuda')\n    k_scale = torch.full((batch_size,\
    \ num_heads, seq_len), 0.5, dtype=torch.float32, device='cuda')\n    output2 =\
    \ forward(q, k, v, q_scale, k_scale)\n    results['test_case_2'] = output2\n\n\
    \    # Test case 3: Different input sizes\n    q = torch.randn((batch_size, num_heads,\
    \ seq_len, head_dim), dtype=torch.float16, device='cuda')\n    k = torch.randn((batch_size,\
    \ num_heads, seq_len, head_dim), dtype=torch.float16, device='cuda')\n    v =\
    \ torch.randn((batch_size, num_heads, seq_len, head_dim), dtype=torch.float16,\
    \ device='cuda')\n    output3 = forward(q, k, v, q_scale, k_scale)\n    results['test_case_3']\
    \ = output3\n\n    # Test case 4: Edge case with zero scaling\n    q_scale = torch.zeros((batch_size,\
    \ num_heads, seq_len), dtype=torch.float32, device='cuda')\n    k_scale = torch.zeros((batch_size,\
    \ num_heads, seq_len), dtype=torch.float32, device='cuda')\n    output4 = forward(q,\
    \ k, v, q_scale, k_scale)\n    results['test_case_4'] = output4\n\n    return\
    \ results\n\n# Run the test\nresult_gold = test_forward()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py attn_fwd_causal.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attn_fwd_causal
