compile_command:
- python reversed_cumsum_scalar.py
correctness_command:
- python reversed_cumsum_scalar_perf.py
performance_command:
- tb_eval -f reversed_cumsum_scalar.py -o reversed_cumsum_scalar_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `chunk_global_reversed_cumsum_scalar_kernel` is a Triton kernel\
    \ that computes a reversed cumulative sum for a given 3D tensor `s` of shape (B,\
    \ H, T) where B is the batch dimension, H is the number of heads, and T is the\
    \ sequence length. The kernel executes for each (B, H) pair independently, by\
    \ utilizing a grid where the grid size is (B * H). For each block, it initializes\
    \ an accumulation variable `b_z` to zero. It then iterates over the dimension\
    \ T in blocks of size `BT`, moving backwards from the end to the start. In each\
    \ iteration, it computes the sum of the current block, updates the accumulation\
    \ variable, calculates the cumulative sum by subtracting it from the sum, and\
    \ stores the result in the output tensor `o`. The function `chunk_global_reversed_cumsum_scalar`\
    \ serves as a wrapper to handle PyTorch tensor input and output, and it defines\
    \ the kernel configuration parameters such as grid size and tensor data types.\n\
    \            \nThe test code is:\n\n\nimport torch\n\n# Test for chunk_global_reversed_cumsum_scalar\n\
    def test_chunk_global_reversed_cumsum_scalar():\n    B, H, T = 2, 3, 4  # Example\
    \ dimensions\n    results = {}\n    \n    # Test case 1\n    s1 = torch.rand((B,\
    \ H, T), dtype=torch.float32).cuda()\n    result1 = chunk_global_reversed_cumsum_scalar(s1)\n\
    \    results['test_case_1'] = result1\n    \n    # Test case 2\n    s2 = torch.rand((B,\
    \ H, T), dtype=torch.float32).cuda()\n    result2 = chunk_global_reversed_cumsum_scalar(s2)\n\
    \    results['test_case_2'] = result2\n    \n    # Test case 3\n    s3 = torch.rand((B,\
    \ H, T), dtype=torch.float32).cuda()\n    result3 = chunk_global_reversed_cumsum_scalar(s3)\n\
    \    results['test_case_3'] = result3\n    \n    # Test case 4\n    s4 = torch.rand((B,\
    \ H, T), dtype=torch.float32).cuda()\n    result4 = chunk_global_reversed_cumsum_scalar(s4)\n\
    \    results['test_case_4'] = result4\n    \n    return results\n\n# Run all tests\n\
    result_gold = test_chunk_global_reversed_cumsum_scalar()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py reversed_cumsum_scalar.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- reversed_cumsum_scalar
