compile_command:
- python kldiv_ops.py
correctness_command:
- python kldiv_ops_perf.py
performance_command:
- tb_eval -f kldiv_ops.py -o kldiv_ops_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    This Triton implementation provides a specialized operator for computing\
    \ the Kullback-Leibler divergence between predicted and target distributions,\
    \ with support for various reduction modes and optimized gradient computation.\n\
    \n    The `kldiv_forward_triton` function orchestrates the forward computation\
    \ of the KL divergence. Inputs to this function include `y_pred` (a tensor of\
    \ predicted log-probabilities with shape [BT, V]), `y_true` (ground truth tensor\
    \ of the same shape), `log_target` (a boolean flag indicating whether `y_true`\
    \ is in log-space), `reduction` (a string determining how the results are aggregated:\
    \ \"none\", \"sum\", \"mean\", or \"batchmean\"), and `eps` (a small value added\
    \ for numerical stability in logarithmic operations).\n\n    The core computation\
    \ is executed by the `_kldiv_kernel_forward` kernel. This kernel processes each\
    \ batch independently. It calculates the KL divergence using the formula `KL(y_true\
    \ || y) = y_true * (log(y_true) - log(y))` when `log_target` is False, and uses\
    \ `loss = exp(y_true) * (y_true - y)` otherwise. The kernel iterates over columns\
    \ in blocks of size `BLOCK_SIZE`. If `reduction` is \"none\", the computed losses\
    \ for each position are stored directly in `loss_ptr`. For other reduction modes,\
    \ the losses are summed over the batch or entire tensor as specified.\n\n    In\
    \ the backward pass, handled by `kldiv_backward_triton`, the gradients with respect\
    \ to the inputs are calculated. The inputs are `target` (ground truth values),\
    \ `grad_output` (incoming gradient from the network), `new_grads` (a tensor to\
    \ hold the computed gradients), and `log_target`. The `_kldiv_kernel_backward`\
    \ kernel performs the differentiation step, outputting gradients adjusted according\
    \ to whether `log_target` is true (where the exponential form is used).\n\n  \
    \  Parameters such as `BLOCK_SIZE` and `num_warps` are crucial for tuning the\
    \ computation's performance, dictating the block of data processed per iteration\
    \ and parallelism level, respectively.\n    \nThe test code is:\n\n\nimport torch\n\
    \n# Test cases for kldiv_forward_triton\ndef test_kldiv():\n    # Define input\
    \ tensors\n    y_pred = torch.tensor([[0.2, 0.3, 0.5], [0.1, 0.6, 0.3]], device='cuda',\
    \ dtype=torch.float32).log()\n    y_true = torch.tensor([[0.1, 0.4, 0.5], [0.2,\
    \ 0.5, 0.3]], device='cuda', dtype=torch.float32)\n    eps = 1e-6\n\n    # Test\
    \ different reduction modes\n    results = {}\n    for i, reduction in enumerate([\"\
    none\", \"sum\", \"mean\", \"batchmean\"]):\n        output = kldiv_forward_triton(y_pred,\
    \ y_true, log_target=False, reduction=reduction, eps=eps)\n        results[f\"\
    test_case_{i+1}\"] = output\n\n    # Test with log_target=True\n    y_true_log\
    \ = y_true.log()\n    output_log_target = kldiv_forward_triton(y_pred, y_true_log,\
    \ log_target=True, reduction=\"sum\", eps=eps)\n    results[\"test_case_5\"] =\
    \ output_log_target\n\n    # Define input tensors\n    target = torch.tensor([[0.1,\
    \ 0.4, 0.5], [0.2, 0.5, 0.3]], device='cuda', dtype=torch.float32)\n    grad_output\
    \ = torch.tensor(1.0, device='cuda', dtype=torch.float32)\n    new_grads = torch.zeros_like(target)\n\
    \n    # Test with log_target=False\n    backward_output = kldiv_backward_triton(target,\
    \ grad_output, new_grads, log_target=False)\n    results[\"test_case_6\"] = backward_output\n\
    \n    # Test with log_target=True\n    target_log = target.log()\n    backward_output_log_target\
    \ = kldiv_backward_triton(target_log, grad_output, new_grads, log_target=True)\n\
    \    results[\"test_case_7\"] = backward_output_log_target\n\n    return results\n\
    \n# Run tests\nresult_gold = test_kldiv()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ kldiv_ops.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- kldiv_ops
