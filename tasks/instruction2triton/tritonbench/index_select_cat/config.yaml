compile_command:
- python index_select_cat.py
correctness_command:
- python index_select_cat_perf.py
performance_command:
- tb_eval -f index_select_cat.py -o index_select_cat_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.The\
    \ `index_select_cat_fwd` function is implemented using Triton to accelerate the\
    \ process of selecting and concatenating specific rows from a 2D source tensor\
    \ based on a 1D index tensor, both residing on the GPU. This function requires\
    \ that `source` and `index` be CUDA tensors, with `source` being 2-dimensional\
    \ and `index` 1-dimensional. If the number of indices exceeds the number of rows\
    \ in the source, a warning is printed, and the indices are truncated. The function\
    \ extracts strides for source tensor dimension traversal. The `index_select_cat_fwd_kernel`,\
    \ decorated with `@triton.jit`, uses a 2D grid to distribute workload across blocks,\
    \ where each block handles a block of indices (`BLOCK_SIZE_INDEX`) and a block\
    \ of columns (`BLOCK_SIZE_COL`). Within the kernel, program ids `pid0` and `pid1`\
    \ determine the starting indices and columns for the block. It computes offsets\
    \ for source and output tensors, applying masks to ensure bounds. `tl.load` is\
    \ used to fetch data from the source, and `tl.store` writes the selected data\
    \ into the output tensor. The `grid` function computes the dimensions of the launch\
    \ grid based on the number of indices and columns, utilizing `triton.cdiv` for\
    \ ceil division.\nThe test code is:\n\n\n# Test for index_select_cat_fwd\ndef\
    \ test_index_select_cat_fwd():\n    results = {}\n    \n    # Test case 1: Standard\
    \ case\n    source = torch.randn(10, 512, device='cuda')\n    index = torch.tensor([0,\
    \ 2, 4, 6, 8], device='cuda')\n    output = torch.empty(len(index), source.size(1),\
    \ device='cuda')\n    index_select_cat_fwd(output, source, index)\n    results['test_case_1']\
    \ = output.clone()\n\n    # Test case 2: Edge case with index covering full range\n\
    \    index = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda')\n   \
    \ output = torch.empty(len(index), source.size(1), device='cuda')\n    index_select_cat_fwd(output,\
    \ source, index)\n    results['test_case_2'] = output.clone()\n\n    # Test case\
    \ 3: Edge case with single index\n    index = torch.tensor([0], device='cuda')\n\
    \    output = torch.empty(len(index), source.size(1), device='cuda')\n    index_select_cat_fwd(output,\
    \ source, index)\n    results['test_case_3'] = output.clone()\n\n    # Test case\
    \ 4: Index in reverse order\n    index = torch.tensor([9, 7, 5, 3, 1], device='cuda')\n\
    \    output = torch.empty(len(index), source.size(1), device='cuda')\n    index_select_cat_fwd(output,\
    \ source, index)\n    results['test_case_4'] = output.clone()\n\n    return results\n\
    \nresult_gold = test_index_select_cat_fwd()\n# \u5206\u652F\u8986\u76D6\u7387\uFF1A\
    [4/4]\n\n\nDon't append test code to the kernel code or edit test function.\n\n\
    The generated code should be written into a python file.\nIf you have already\
    \ created a file and wrote the code into it, edit the code directly in the file.\n\
    Test the code by running `python python_bindings/tritonbench.py index_select_cat.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- index_select_cat
