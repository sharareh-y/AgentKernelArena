compile_command:
- python embedding_triton_kernel.py
correctness_command:
- python embedding_triton_kernel_perf.py
performance_command:
- tb_eval -f embedding_triton_kernel.py -o embedding_triton_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton-accelerated function embedding_kernel is specialized for\
    \ extracting and storing embedding vectors from a weight matrix for a sequence\
    \ of token IDs. It uses program IDs to determine processing offsets and handles\
    \ iteration over sequences with BLOCK_N and BLOCK_NN stride sizes. For each sequence,\
    \ it computes token IDs and uses masks to ensure only valid data is loaded and\
    \ processed. The weight matrix is addressed using a combination of token IDs and\
    \ dimension offsets, facilitated by the stride of the weight tensor. The processed\
    \ vectors are then stored into the 'out' tensor using calculated strides and masks,\
    \ ensuring each output sequence position receives the correct embedding vector.\
    \ The wrapping function, embedding, configures and invokes the kernel with appropriate\
    \ grid settings, aligning BLOCK_DMODEL to the next power of two based on weight\
    \ dimensions and leveraging constant memory settings to optimize the embedding\
    \ extraction process.\n            \nThe test code is:\n\n\nimport torch\n\ndef\
    \ test_embedding():\n    # \u53C2\u6570\u5B9A\u4E49\n    vocab_size = 1000   \
    \      # \u8BCD\u6C47\u8868\u5927\u5C0F\n    embedding_dim = 512       # \u5D4C\
    \u5165\u7EF4\u5EA6\n    sequence_length = 128     # \u8F93\u5165\u5E8F\u5217\u957F\
    \u5EA6\n    vob_start_id = 10         # \u8BCD\u6C47\u8868\u8D77\u59CB ID\n  \
    \  vob_end_id = 1000         # \u8BCD\u6C47\u8868\u7ED3\u675F ID\n\n    # \u521B\
    \u5EFA\u6D4B\u8BD5\u8F93\u5165\u5F20\u91CF\n    input_ids = torch.randint(\n \
    \       vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n\
    \    )\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32,\
    \ device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim,\
    \ dtype=torch.float32, device='cuda'\n    )\n\n    # \u8C03\u7528\u5D4C\u5165\u51FD\
    \u6570\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n\n  \
    \  # \u4FDD\u5B58\u7ED3\u679C\n    results = {}\n    results['test_case_1'] =\
    \ out.clone()\n\n    # \u6D4B\u8BD5\u4E0D\u540C\u7684\u8F93\u5165\n    input_ids\
    \ = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32,\
    \ device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id,\
    \ out)\n    results['test_case_2'] = out.clone()\n\n    # \u6D4B\u8BD5\u4E0D\u540C\
    \u7684\u8BCD\u6C47\u8868\u8303\u56F4\n    vob_start_id = 0\n    vob_end_id = 500\n\
    \    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,),\
    \ dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id,\
    \ vob_end_id, out)\n    results['test_case_3'] = out.clone()\n\n    # \u6D4B\u8BD5\
    \u4E0D\u540C\u7684\u5D4C\u5165\u7EF4\u5EA6\n    embedding_dim = 256\n    weight\
    \ = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n\
    \    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32,\
    \ device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id,\
    \ out)\n    results['test_case_4'] = out.clone()\n\n    return results\n\nresult_gold\
    \ = test_embedding()\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ embedding_triton_kernel.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- embedding_triton_kernel
