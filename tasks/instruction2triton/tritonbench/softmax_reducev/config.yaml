compile_command:
- python softmax_reducev.py
correctness_command:
- python softmax_reducev_perf.py
performance_command:
- tb_eval -f softmax_reducev.py -o softmax_reducev_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The '_fwd_kernel' Triton kernel performs a forward computation for\
    \ a token-wise softmax reduction involving several key steps. The kernel operates\
    \ on three-dimensional data structured by 'Logics', 'V', and 'Out', together with\
    \ indices and sequence metadata ('B_Loc', 'B_Start_Loc', 'B_Seqlen') which are\
    \ crucial for handling variable-length sequences within a batch. The primary purpose\
    \ is to apply a scaled dot-product attention mechanism, which includes a softmax\
    \ computation on logits and a weighted summation. Within the function, the grid\
    \ is defined over batch and head dimensions, indicated by 'program_id(0)' and\
    \ 'program_id(1)'. The kernel iteratively processes blocks of the sequence, as\
    \ determined by 'BLOCK_N', calculating maximum exponentials 'e_max', probabilities\
    \ 'p', and an accumulated weighted sum 'acc' over the sequence. This is achieved\
    \ by carefully loading indices with 'tl.load', performing exponentiation and normalization,\
    \ and ensuring numerical stability through max-shifted exponentials. Post-loop,\
    \ results in 'acc' are normalized by the sum of exponentials and stored into 'Out'.\
    \ The helper function 'token_softmax_reducev_fwd' is responsible for launching\
    \ '_fwd_kernel' across all combinations of batch and head via the grid, setting\
    \ up strides and block sizes using input tensor strides and the model dimension\
    \ 'BLOCK_DMODEL'. Execution parameters include a configurable number of warps\
    \ and stages for performance tuning.\n            \nThe test code is:\n\n\nimport\
    \ torch\n\ndef test_token_softmax_reducev_fwd():\n    # Define the input parameters\n\
    \    batch_size = 2\n    num_heads = 2\n    max_input_len = 128\n    d_model =\
    \ 64\n    BLOCK = 64\n\n    # Create random tensors for inputs\n    logics = torch.randn((num_heads,\
    \ batch_size * max_input_len), dtype=torch.float32, device='cuda')\n    v = torch.randn((num_heads,\
    \ max_input_len, d_model), dtype=torch.float32, device='cuda')\n    o = torch.empty((batch_size,\
    \ num_heads, d_model), dtype=torch.float32, device='cuda')\n\n    # Create auxiliary\
    \ tensors\n    b_loc = torch.randint(0, max_input_len, (batch_size, max_input_len),\
    \ dtype=torch.int32, device='cuda')\n    b_start_loc = torch.randint(0, max_input_len,\
    \ (batch_size,), dtype=torch.int32, device='cuda')\n    b_seq_len = torch.randint(1,\
    \ max_input_len + 1, (batch_size,), dtype=torch.int32, device='cuda')\n    other_kv_index\
    \ = -1  # Assuming -1 is used to avoid reading NaN data\n\n    # First branch\
    \ execution\n    token_softmax_reducev_fwd(logics, v, o, b_loc, b_start_loc, b_seq_len,\
    \ max_input_len, other_kv_index)\n    result_1 = o.clone()\n\n    # Modify inputs\
    \ to test second branch\n    b_seq_len = torch.tensor([max_input_len, max_input_len],\
    \ dtype=torch.int32, device='cuda')\n    token_softmax_reducev_fwd(logics, v,\
    \ o, b_loc, b_start_loc, b_seq_len, max_input_len, other_kv_index)\n    result_2\
    \ = o.clone()\n\n    # Modify inputs to test third branch\n    b_start_loc = torch.tensor([0,\
    \ 0], dtype=torch.int32, device='cuda')\n    token_softmax_reducev_fwd(logics,\
    \ v, o, b_loc, b_start_loc, b_seq_len, max_input_len, other_kv_index)\n    result_3\
    \ = o.clone()\n\n    # Modify inputs to test fourth branch\n    other_kv_index\
    \ = 0\n    token_softmax_reducev_fwd(logics, v, o, b_loc, b_start_loc, b_seq_len,\
    \ max_input_len, other_kv_index)\n    result_4 = o.clone()\n\n    results = {\n\
    \        \"test_case_1\": result_1,\n        \"test_case_2\": result_2,\n    \
    \    \"test_case_3\": result_3,\n        \"test_case_4\": result_4\n    }\n\n\
    \    return results\n\nresult_gold = test_token_softmax_reducev_fwd()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py softmax_reducev.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- softmax_reducev
