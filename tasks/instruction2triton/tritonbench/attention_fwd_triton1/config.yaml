compile_command:
- python attention_fwd_triton1.py
correctness_command:
- python attention_fwd_triton1_perf.py
performance_command:
- tb_eval -f attention_fwd_triton1.py -o attention_fwd_triton1_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton operator implements a forward-pass kernel for\
    \ the attention mechanism, a central part of transformer models. The `attention_fwd_kernel`\
    \ function is compiled using Triton\u2019s `@triton.jit`, allowing the kernel\
    \ to execute on the GPU. It computes the scaled dot-product attention, which involves\
    \ taking queries (q), keys (k), and values (v) as inputs. The calculations are\
    \ performed over batched multi-head inputs, where each batch and head are processed\
    \ independently.\n\n            Inside the kernel, `b_h` is initialized as a zero\
    \ matrix and is used to accumulate intermediate results. The loop iterates over\
    \ divided blocks of the sequence length (`tl.cdiv(T, BT)`). In each iteration,\
    \ memory pointers `p_q`, `p_k`, `p_v`, `p_h`, and `p_o` are set up to point to\
    \ the corresponding slices of the q, k, v, h, and o tensors. The dimensions and\
    \ strides are set to efficiently load and store tensor blocks.\n\n           \
    \ The core computation begins with loading blocks of the query and key tensors,\
    \ followed by a scaled dot-product to compute attention scores (`b_s`). The block\
    \ of the value tensor is then weighted by these scores to produce the output block\
    \ (`b_o`). Depending on the `IFCOND` condition, either a conditional update or\
    \ standard update is applied to the intermediate `b_h` tensor, which may also\
    \ be optionally stored when the `STORE` flag is set.\n\n            The class\
    \ `AttentionFunction` provides a user interface for invoking the Triton kernel.\
    \ It initializes the output tensor `o` and an intermediate storage tensor `h`.\
    \ The forward method sets the kernel launch configurations, like grid size, number\
    \ of warps, and stages, tailored for different input dimensions. The method also\
    \ computes scaling based on the head dimension and calls the Triton kernel with\
    \ these parameters. The function returns the computed output tensor.\n\n     \
    \       \nThe test code is:\n\n\nimport torch\n\ndef test_attention_fwd():\n \
    \   # Test 1: Basic Forward Pass (no special conditions)\n    B, H, T, D = 2,\
    \ 8, 1024, 128\n    q = torch.randn((B, H, T, D), dtype=torch.float, device='cuda')\n\
    \    k = torch.randn((B, H, T, D), dtype=torch.float, device='cuda')\n    v =\
    \ torch.randn((B, H, T, D), dtype=torch.float, device='cuda')\n\n    result =\
    \ AttentionFunction.apply(q, k, v)\n\n    # Test 2: Forward Pass with STORE enabled\n\
    \    result_store = AttentionFunction.apply(q, k, v, True)\n\n    # Test 3: Forward\
    \ Pass with IFCOND enabled\n    result_ifcond = AttentionFunction.apply(q, k,\
    \ v, False, True)\n\n    # Test 4: Forward Pass with STORE and IFCOND enabled\n\
    \    result_store_ifcond = AttentionFunction.apply(q, k, v, True, True)\n\n  \
    \  # Test 5: Forward Pass with large head size (d_head > 64)\n    B, H, T, D =\
    \ 2, 8, 1024, 128\n    q_large_head = torch.randn((B, H, T, D), dtype=torch.float,\
    \ device='cuda')\n    k_large_head = torch.randn((B, H, T, D), dtype=torch.float,\
    \ device='cuda')\n    v_large_head = torch.randn((B, H, T, D), dtype=torch.float,\
    \ device='cuda')\n\n    result_large_head = AttentionFunction.apply(q_large_head,\
    \ k_large_head, v_large_head)\n\n    # Test 6: Edge case with smallest sequence\
    \ length (T = 1)\n    B, H, T, D = 2, 8, 1, 128\n    q_small_seq = torch.randn((B,\
    \ H, T, D), dtype=torch.float, device='cuda')\n    k_small_seq = torch.randn((B,\
    \ H, T, D), dtype=torch.float, device='cuda')\n    v_small_seq = torch.randn((B,\
    \ H, T, D), dtype=torch.float, device='cuda')\n\n    result_small_seq = AttentionFunction.apply(q_small_seq,\
    \ k_small_seq, v_small_seq)\n\n    return {\n        \"test_case_1\": result,\n\
    \        \"test_case_2\": result_store,\n        \"test_case_3\": result_ifcond,\n\
    \        \"test_case_4\": result_store_ifcond,\n        \"test_case_5\": result_large_head,\n\
    \        \"test_case_6\": result_small_seq\n    }\n\n# Run all tests\nresult_gold\
    \ = test_attention_fwd()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ attention_fwd_triton1.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attention_fwd_triton1
