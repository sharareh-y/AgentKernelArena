compile_command:
- python attention_kernel_aligned.py
correctness_command:
- python attention_kernel_aligned_perf.py
performance_command:
- tb_eval -f attention_kernel_aligned.py -o attention_kernel_aligned_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_fwd_kernel_aligned` function is a Triton JIT-compiled kernel\
    \ designed to perform attention operations incorporating relative positional embeddings\
    \ between query (Q), key (K), and value (V) tensors. It uses block-wise loading\
    \ and processing for efficiency, leveraging blocks defined by dimensions `BLOCK_M`,\
    \ `BLOCK_N`, and `BLOCK_DMODEL`. The kernel applies a scaled dot-product mechanism\
    \ with a temperature scale `sm_scale`, accounting for additional bias from the\
    \ B0 tensor. The forward pass of the attention mechanism iterates over the sequence\
    \ dimension (N_CTX + P_SEQ), calculating softmax scores and accumulating weighted\
    \ values into `acc`. The outer function `_attention_rel_h_rel_w_kernel_aligned_device`\
    \ configures the execution environment, specifying block dimensions, warps, and\
    \ stages. It verifies shape and type consistency of inputs and invokes the kernel\
    \ using a 3D grid setup that parallels the batch, head, and sequence dimensions.\
    \ The resulting output is computed in `OUT_DTYPE` format, ensuring the data type\
    \ compatibility based on input precision (float16 or bfloat16).\n            \n\
    The test code is:\n\n\nimport torch\n\n# Define the test function\ndef test_attention_rel_h_rel_w_kernel_aligned_device():\n\
    \    # Parameters\n    BATCH_SIZE = 2\n    HEADS = 4\n    N_CTX = 128\n    D_MODEL\
    \ = 64\n    BLOCK_M = 32\n    BLOCK_N = 64\n    num_warps = 4\n    num_stages\
    \ = 2\n    sm_scale = 1.0\n\n    # Create random input tensors\n    q = torch.randn((BATCH_SIZE,\
    \ HEADS, N_CTX, D_MODEL), dtype=torch.float16, device='cuda')\n    k = torch.randn((BATCH_SIZE,\
    \ HEADS, N_CTX, D_MODEL), dtype=torch.float16, device='cuda')\n    v = torch.randn((BATCH_SIZE,\
    \ HEADS, N_CTX, D_MODEL), dtype=torch.float16, device='cuda')\n    rel_h_w = torch.randn((BATCH_SIZE,\
    \ HEADS, N_CTX, 128), dtype=torch.float16, device='cuda')\n    o = torch.empty((BATCH_SIZE,\
    \ HEADS, N_CTX, D_MODEL), dtype=torch.float16, device='cuda')\n\n    # Prepare\
    \ test cases for each branch\n    test_cases = {}\n\n    # Case 1: Check when\
    \ rel_h_w.size(-1) == 128\n    test_cases[\"test_case_1\"] = _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, sm_scale, o, BLOCK_M, BLOCK_N, num_warps, num_stages\n\
    \    )\n\n    # Case 2: Check when q.shape[-2] == k.shape[-2] (P_SEQ == 0)\n \
    \   P_SEQ = 0\n    rel_h_w2 = torch.randn((BATCH_SIZE, HEADS, N_CTX, 128), dtype=torch.float16,\
    \ device='cuda')\n    test_cases[\"test_case_2\"] = _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w2, sm_scale, o, BLOCK_M, BLOCK_N, num_warps, num_stages\n\
    \    )\n\n    # Case 4: Check with a different sm_scale\n    sm_scale2 = 0.5\n\
    \    test_cases[\"test_case_4\"] = _attention_rel_h_rel_w_kernel_aligned_device(\n\
    \        q, k, v, rel_h_w, sm_scale2, o, BLOCK_M, BLOCK_N, num_warps, num_stages\n\
    \    )\n\n    return test_cases\n\n# Run the test\nresult_gold = test_attention_rel_h_rel_w_kernel_aligned_device()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py attention_kernel_aligned.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attention_kernel_aligned
