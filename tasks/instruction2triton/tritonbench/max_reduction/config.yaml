compile_command:
- python max_reduction.py
correctness_command:
- python max_reduction_perf.py
performance_command:
- tb_eval -f max_reduction.py -o max_reduction_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The code comprises multiple GPU-accelerated kernels using Triton,\
    \ facilitating efficient max value computation in tensors.\n            - `max_kernel_1`:\
    \ This Triton JIT-compiled kernel computes the maximum values within a large 1D\
    \ input tensor across predefined blocks (BLOCK_SIZE). The program index (pid)\
    \ determines the starting offset for each block, and values are conditionally\
    \ loaded using a mask to handle out-of-bound reads. The maximum value in each\
    \ block is calculated using `tl.max()` and stored in the `mid` intermediate tensor\
    \ at the corresponding position.\n            - `max_kernel_2`: A second kernel\
    \ that consolidates results from `max_kernel_1`. It calculates the overall maximum\
    \ from the intermediate results stored in `mid` and writes the final maximum value\
    \ to the `out` tensor.\n            - `max_kernel`: An advanced kernel for multi-dimensional\
    \ max computation, which calculates the maximum values along a specified dimension.\
    \ It employs a dual-program index strategy: `pid_m` for blocks along the selected\
    \ dimension and `pid_k` for other dimensions. The `tl.load` function pulls data\
    \ with masking to ensure correct indexing, and `tl.max` derives the maximum value\
    \ and corresponding index across dimensions.\n            - `max`: A wrapper function\
    \ that executes the sequential execution of `max_kernel_1` and `max_kernel_2`.\
    \ It prepares necessary intermediate tensors (`mid` and `out`) and determines\
    \ optimal block sizes (BLOCK_SIZE) through Triton's utility functions.\n     \
    \       - `max_dim`: Extends max computation to operate on any dimension specified\
    \ by `dim`, which is checked for validity against the tensor's rank. It reshapes\
    \ outputs accordingly and executes `max_kernel` using calculated grid dimensions,\
    \ determined by tensor dimensions M (product of all sizes before `dim`) and K\
    \ (sizes after `dim`).\n            Each function is meticulously annotated for\
    \ Triton's JIT compilation, facilitating optimal resource allocation for various\
    \ GPU architectures. This design aims at reducing computational time by maximizing\
    \ parallel execution potential of Triton kernels.\n            \nThe test code\
    \ is:\n\n\ndef test_max():\n    # \u6D4B\u8BD51\uFF1A1\u7EF4Tensor\uFF0C\u9A8C\
    \u8BC1max\u51FD\u6570\n    # \u4F7F\u7528\u968F\u673A\u751F\u6210\u7684\u957F\u5EA6\
    \u4E3A1024\u7684\u4E00\u7EF4Tensor\n    inp1d = torch.randn(1024, device=\"cuda\"\
    )\n    # \u4F7F\u7528\u81EA\u5B9A\u4E49max\u51FD\u6570\n    out1d_custom = max(inp1d)\n\
    \n    # \u6D4B\u8BD52\uFF1A2\u7EF4Tensor\uFF0C\u9A8C\u8BC1max_dim\u51FD\u6570\n\
    \    # \u4F7F\u7528\u968F\u673A\u751F\u6210\u76841024x1024\u7684\u4E8C\u7EF4Tensor\n\
    \    inp2d = torch.randn(1024, 1024, device=\"cuda\")\n    # \u4F7F\u7528\u81EA\
    \u5B9A\u4E49max_dim\u51FD\u6570\uFF0C\u6CBF\u7740dim=1\u8BA1\u7B97\u6700\u5927\
    \u503C\n    out2d_custom = max_dim(inp2d, dim=1)\n\n    # \u6D4B\u8BD53\uFF1A\
    3\u7EF4Tensor\uFF0C\u9A8C\u8BC1max_dim\u51FD\u6570\n    # \u4F7F\u7528\u968F\u673A\
    \u751F\u6210\u7684128x64x32\u7684\u4E09\u7EF4Tensor\n    inp3d = torch.randn(128,\
    \ 64, 32, device=\"cuda\")\n    # \u4F7F\u7528\u81EA\u5B9A\u4E49max_dim\u51FD\u6570\
    \uFF0C\u6CBF\u7740dim=2\u8BA1\u7B97\u6700\u5927\u503C\n    out3d_custom = max_dim(inp3d,\
    \ dim=2)\n\n    # \u6D4B\u8BD54\uFF1A\u4FDD\u6301\u7EF4\u5EA6\u7684\u6D4B\u8BD5\
    \n    # \u4F7F\u7528\u968F\u673A\u751F\u6210\u7684512x256\u7684\u4E8C\u7EF4Tensor\n\
    \    inp2d_keepdim = torch.randn(512, 256, device=\"cuda\")\n    # \u4F7F\u7528\
    \u81EA\u5B9A\u4E49max_dim\u51FD\u6570\uFF0C\u4FDD\u6301\u7EF4\u5EA6\u7684\u60C5\
    \u51B5\u4E0B\u8BA1\u7B97\u6700\u5927\u503C\n    out2d_custom_keepdim = max_dim(inp2d_keepdim,\
    \ dim=1, keepdim=True)\n\n    # \u6D4B\u8BD55\uFF1A\u8D1F\u7EF4\u5EA6\u6D4B\u8BD5\
    \n    # \u4F7F\u7528\u968F\u673A\u751F\u6210\u768464x128x256\u7684\u4E09\u7EF4\
    Tensor\n    inp3d_neg_dim = torch.randn(64, 128, 256, device=\"cuda\")\n    #\
    \ \u4F7F\u7528\u81EA\u5B9A\u4E49max_dim\u51FD\u6570\uFF0C\u6CBF\u7740\u8D1F\u7684\
    \u7EF4\u5EA6\u8BA1\u7B97\u6700\u5927\u503C\uFF08\u7B49\u4EF7\u4E8Edim=1\uFF09\n\
    \    out3d_custom_neg_dim = max_dim(inp3d_neg_dim, dim=-2)\n\n    # \u8BB0\u5F55\
    \u6BCF\u4E2A\u6D4B\u8BD5\u7528\u4F8B\u7684\u7ED3\u679C\n    results = {\n    \
    \    \"test_case_1\": out1d_custom,\n        \"test_case_2\": out2d_custom,\n\
    \        \"test_case_3\": out3d_custom,\n        \"test_case_4\": out2d_custom_keepdim,\n\
    \        \"test_case_5\": out3d_custom_neg_dim,\n    }\n\n    return results\n\
    \nresult_gold = test_max()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ max_reduction.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- max_reduction
