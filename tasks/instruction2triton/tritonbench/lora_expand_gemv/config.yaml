compile_command:
- python lora_expand_gemv.py
correctness_command:
- python lora_expand_gemv_perf.py
performance_command:
- tb_eval -f lora_expand_gemv.py -o lora_expand_gemv_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel `_bgmv_expand_kernel` is designed to perform a batched\
    \ generalized matrix-vector multiplication (GEMV) using LoRA weights, with options\
    \ to split the N dimension for performance optimization and conditionally add\
    \ input values to the output. Key parameters include:\n    \n    - `input_ptr`:\
    \ Pointer to the input matrix.\n    - `lora_ptr`: Pointer to the LoRA weights\
    \ matrix.\n    - `out_ptr`: Pointer to the output matrix.\n    - `N, K`: Dimensions\
    \ of the matrices involved.\n    - `lora_indices`: Tensor indicating which LoRA\
    \ weights to use for each batch.\n    - Various stride parameters for correct\
    \ memory addressing.\n    \n    The kernel checks the `lora_index` for each batch\
    \ and skips computation if the index is -1. It then performs the matrix-vector\
    \ multiplication in blocks, controlled by `BLOCK_N` and `BLOCK_K` constants, with\
    \ potential type casting and input addition.\n    \n    The wrapper function `_bgmv_expand`\
    \ sets up this kernel execution. It ensures that the input tensors are contiguous\
    \ and have compatible dimensions. It selects the appropriate block sizes (`BLOCK_K`)\
    \ and determines if type casting (`CAST_TYPE`) is needed based on the input and\
    \ LoRA weights' data types. The function then launches the Triton kernel with\
    \ a grid configuration based on the split N parameter, controlling how the work\
    \ is distributed among kernel instances.\n    \nThe test code is:\n\n\nimport\
    \ torch\n\n# Define the test function\ndef test_bgmv_expand():\n    # Define input\
    \ parameters\n    batch_size = 4\n    hidden_size = 128\n    rank = 64\n    lora_num\
    \ = 3\n\n    # Create input tensors\n    inputs = torch.randn(batch_size, hidden_size,\
    \ dtype=torch.float16, device='cuda')\n    lora_b_weights = torch.randn(lora_num,\
    \ rank, hidden_size, dtype=torch.float16, device='cuda')\n    lora_indices_tensor\
    \ = torch.tensor([0, 1, -1, 2], dtype=torch.int32, device='cuda')\n\n    # Initialize\
    \ a dictionary to store results\n    results = {}\n\n    # Test case 1: add_inputs=True\n\
    \    output_tensor_1 = torch.zeros(batch_size, rank, dtype=torch.float16, device='cuda')\n\
    \    _bgmv_expand(\n        inputs=inputs,\n        lora_b_weights=lora_b_weights,\n\
    \        output_tensor=output_tensor_1,\n        lora_indices_tensor=lora_indices_tensor,\n\
    \        add_inputs=True\n    )\n    results['test_case_1'] = output_tensor_1\n\
    \n    # Test case 2: add_inputs=False\n    output_tensor_2 = torch.zeros(batch_size,\
    \ rank, dtype=torch.float16, device='cuda')\n    _bgmv_expand(\n        inputs=inputs,\n\
    \        lora_b_weights=lora_b_weights,\n        output_tensor=output_tensor_2,\n\
    \        lora_indices_tensor=lora_indices_tensor,\n        add_inputs=False\n\
    \    )\n    results['test_case_2'] = output_tensor_2\n\n    return results\n\n\
    # Run the test\nresult_gold = test_bgmv_expand()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ lora_expand_gemv.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- lora_expand_gemv
