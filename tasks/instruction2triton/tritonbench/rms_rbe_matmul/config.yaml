compile_command:
- python rms_rbe_matmul.py
correctness_command:
- python rms_rbe_matmul_perf.py
performance_command:
- tb_eval -f rms_rbe_matmul.py -o rms_rbe_matmul_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel `rms_matmul_rbe` executes a specialized matrix multiplication\
    \ that incorporates RMS normalization and optionally applies rotary embeddings.\
    \ The kernel takes in pointers to the input tensor `x`, the transposed weight\
    \ matrix `w`, and an auxiliary RMS weight `rms_w`. The operation proceeds in a\
    \ block-wise manner, defined by `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`,\
    \ iterating over batches and computing each element of the output matrix. RMS\
    \ normalization is applied to input rows before matrix multiplication, by computing\
    \ the sum of squares, deriving the mean, and applying the reciprocal square root\
    \ to normalize. If specified, rotary embeddings adjust results based on token\
    \ position and angle `THETA`. The wrapper function `rms_matmul_rbe_wrapper` sets\
    \ up parameters, manages data types, and launches the Triton kernel, ensuring\
    \ the input and output tensors are correctly formatted and aligned for transformer\
    \ layers, considering the model's dimensions like batch size, number of attention\
    \ heads, and head size.\n    \nThe test code is:\n\n\ndef test_rms_matmul_rbe():\n\
    \    batch, M, K = 2, 4, 1024\n    N = 64\n    n_heads = 8\n    assert N % n_heads\
    \ == 0\n    head_dim = N // n_heads\n\n    x = torch.randn((batch, M, K), dtype=torch.float16,\
    \ device='cuda')\n    weight = torch.randn((N, K), dtype=torch.float16, device='cuda')\n\
    \    rms_w = torch.randn((K,), dtype=torch.float16, device='cuda')\n\n    test_results\
    \ = {}\n\n    # Test case 1: use_rbe = False, weight dtype = float16\n    use_rbe\
    \ = False\n    start_pos = 0\n    out = rms_matmul_rbe_wrapper(x, weight, rms_w,\
    \ use_rbe, start_pos, n_heads, head_dim)\n    test_results['test_case_1'] = out\n\
    \n    # Test case 2: use_rbe = True, weight dtype = float16\n    use_rbe = True\n\
    \    out = rms_matmul_rbe_wrapper(x, weight, rms_w, use_rbe, start_pos, n_heads,\
    \ head_dim)\n    test_results['test_case_2'] = out\n\n    # Test case 3: use_rbe\
    \ = False, weight dtype = int8\n    weight_int8 = weight.to(torch.int8)\n    use_rbe\
    \ = False\n    out = rms_matmul_rbe_wrapper(x, weight_int8, rms_w, use_rbe, start_pos,\
    \ n_heads, head_dim)\n    test_results['test_case_3'] = out\n\n    # Test case\
    \ 4: use_rbe = True, weight dtype = int8\n    use_rbe = True\n    out = rms_matmul_rbe_wrapper(x,\
    \ weight_int8, rms_w, use_rbe, start_pos, n_heads, head_dim)\n    test_results['test_case_4']\
    \ = out\n\n    return test_results\n\nresult_gold = test_rms_matmul_rbe()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py rms_rbe_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rms_rbe_matmul
