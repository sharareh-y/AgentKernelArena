compile_command:
- python layernorm_fwd_triton.py
correctness_command:
- python layernorm_fwd_triton_perf.py
performance_command:
- tb_eval -f layernorm_fwd_triton.py -o layernorm_fwd_triton_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The `_layer_norm_fwd_kernel` function in Triton computes the forward pass\
    \ of a layer normalization operation on a 3D input tensor `X`. The kernel normalizes\
    \ the elements of `X` along the last dimension for each 2D slice, using a specified\
    \ weight matrix `W` for scaling. \n\n    The kernel operates with blocks of size\
    \ `BLOCK_SIZE` and uses loop iterations to handle each segment along the normalization\
    \ axis. The mean and variance are computed first, using floating point precision\
    \ for stability, and then applied to obtain the normalized output `Y`.\n\n   \
    \ Input `X` is a pointer to a 3D tensor with shape `(D1, D2, N)`, and `W` is a\
    \ pointer to weights with shape `(D2, N)`. `Y` is the output tensor of the same\
    \ shape as `X`. The kernel uses provided strides to traverse the input tensor\
    \ efficiently.\n\n    The function `layernorm_forward` prepares the grid dimensions\
    \ for kernel execution, computes the memory strides, and ensures the dimensions\
    \ are compatible between `X` and `W`. It then invokes the Triton kernel `_layer_norm_fwd_kernel`\
    \ with computed parameters such as grid size, and block size.\n    \nThe test\
    \ code is:\n\n\nimport torch\n\n# Test function for layernorm_forward\ndef test_layernorm_forward():\n\
    \    results = {}\n    \n    # Test case 1: Basic functionality\n    X = torch.randn(2,\
    \ 3, 128, dtype=torch.float32, device='cuda')\n    W = torch.randn(3, 128, dtype=torch.float32,\
    \ device='cuda')\n    eps = 1e-5\n    y = layernorm_forward(X, W, eps)\n    results['test_case_1']\
    \ = y\n\n    # Test case 2: Different batch size\n    X = torch.randn(4, 3, 128,\
    \ dtype=torch.float32, device='cuda')\n    W = torch.randn(3, 128, dtype=torch.float32,\
    \ device='cuda')\n    y = layernorm_forward(X, W, eps)\n    results['test_case_2']\
    \ = y\n\n    # Test case 3: Different feature size\n    X = torch.randn(2, 3,\
    \ 256, dtype=torch.float32, device='cuda')\n    W = torch.randn(3, 256, dtype=torch.float32,\
    \ device='cuda')\n    y = layernorm_forward(X, W, eps)\n    results['test_case_3']\
    \ = y\n\n    # Test case 4: Different number of heads\n    X = torch.randn(2,\
    \ 4, 128, dtype=torch.float32, device='cuda')\n    W = torch.randn(4, 128, dtype=torch.float32,\
    \ device='cuda')\n    y = layernorm_forward(X, W, eps)\n    results['test_case_4']\
    \ = y\n\n    return results\n\n# Run the test function\nresult_gold = test_layernorm_forward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py layernorm_fwd_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- layernorm_fwd_triton
