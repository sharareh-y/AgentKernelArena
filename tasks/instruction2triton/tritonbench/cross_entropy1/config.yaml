compile_command:
- python cross_entropy1.py
correctness_command:
- python cross_entropy1_perf.py
performance_command:
- tb_eval -f cross_entropy1.py -o cross_entropy1_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    This script provides a highly optimized cross-entropy loss function using\
    \ Triton for efficient GPU execution, particularly suitable for large-scale distributed\
    \ machine learning tasks. It introduces two main kernels, `cross_entropy_fwd_kernel`\
    \ and `cross_entropy_bwd_kernel`, written in Triton language that enable high-performance\
    \ computation with fine-grained control over execution.\n\n    The forward kernel,\
    \ `cross_entropy_fwd_kernel`, calculates the LSE and the smoothed cross-entropy\
    \ loss. It uses block-level parallelism and supports label smoothing, where a\
    \ proportion of the true label probability is distributed among all classes. The\
    \ kernel uses Triton\u2019s `program_id` to identify the executing thread block\
    \ and manage access to specific data partitions. It includes heuristics to determine\
    \ whether label smoothing is applied and whether the dataset is split across multiple\
    \ devices (useful for tensor parallelism).\n\n    The backward kernel, `cross_entropy_bwd_kernel`,\
    \ computes the gradient of the cross-entropy loss with respect to the logits.\
    \ This involves calculating the gradient of LSE and adjusting probabilities based\
    \ on the presence of label smoothing. It uses similar block-level indexing to\
    \ parallelize this computation efficiently across GPU cores.\n\n    The `CrossEntropyLoss`\
    \ class encapsulates these kernels within a PyTorch autograd function. Its static\
    \ `forward` method sets up the context, computes losses, manages optional distributed\
    \ operations, and saves necessary tensors for the backward pass. The `backward`\
    \ method applies the backward kernel to compute gradients.\n\n    The auxiliary\
    \ function `cross_entropy_loss` is a user-friendly wrapper around the `CrossEntropyLoss.apply`\
    \ method. It handles parameter passing for common usage patterns, such as enabling/disabling\
    \ label smoothing and configuring tensor parallelism.\n\n    Parameters for these\
    \ functions include:\n    - `logits`: a 2D tensor containing model predictions\
    \ before softmax.\n    - `labels`: a 1D tensor with the actual class labels.\n\
    \    - `smoothing`: a float controlling label smoothing intensity.\n    - `lse_square_scale`:\
    \ controls LSE regularization.\n    - `ignored_index`: specifies label indices\
    \ to ignore in loss computation.\n    - `process_group`: defines the communication\
    \ group for distributed settings.\n\n    This module ensures efficient computation\
    \ and gradient propagation in neural networks, especially when working with large\
    \ vocabularies or extensive multi-GPU setups.\n    \nThe test code is:\n\n\nimport\
    \ torch\n\ndef test_cross_entropy_loss():\n    results = {}\n    # Test case 1:\
    \ Basic test without label smoothing\n    logits = torch.tensor([[2.0, 1.0, 0.1],\
    \ [0.5, 2.5, 0.3]], device='cuda')\n    labels = torch.tensor([0, 1], device='cuda')\n\
    \    loss, _ = cross_entropy_loss(logits, labels)\n    results['test_case_1']\
    \ = loss\n\n    # Test case 2: Test with label smoothing\n    label_smoothing\
    \ = 0.1\n    loss, _ = cross_entropy_loss(logits, labels, label_smoothing=label_smoothing)\n\
    \    results['test_case_2'] = loss\n\n    # Test case 3: Test with ignored index\n\
    \    ignored_index = 1\n    labels_with_ignored = torch.tensor([0, ignored_index],\
    \ device='cuda')\n    loss, _ = cross_entropy_loss(logits, labels_with_ignored,\
    \ ignored_index=ignored_index)\n    results['test_case_3'] = loss\n\n    # Test\
    \ case 4: Test with tensor parallelism (simulated)\n    # Assuming a process group\
    \ is set up for distributed training\n    # For simplicity, we simulate this by\
    \ using a single process\n    process_group = None  # Replace with actual process\
    \ group in distributed setting\n    loss, _ = cross_entropy_loss(logits, labels,\
    \ process_group=process_group)\n    results['test_case_4'] = loss\n\n    return\
    \ results\n\nresult_gold = test_cross_entropy_loss()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ cross_entropy1.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- cross_entropy1
