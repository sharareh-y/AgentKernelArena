compile_command:
- python kldiv_triton.py
correctness_command:
- python kldiv_triton_perf.py
performance_command:
- tb_eval -f kldiv_triton.py -o kldiv_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The provided Triton operator computes the forward and backward passes of the Kullback-Leibler\
    \ divergence (KL divergence). It includes the kernel definitions `_kldiv_kernel_forward`\
    \ and `_kldiv_kernel_backward` and their corresponding wrapper functions `kldiv_forward_triton`\
    \ and `kldiv_backward_triton`. Here is a detailed breakdown:\n\n1. Constants:\n\
    \   - `MAX_FUSED_SIZE`: Set to 65536 divided by 4, determining the maximum block\
    \ size for tensor fusion.\n   - Reduction mode constants: `_REDUCTION_MODE_NONE`,\
    \ `_REDUCTION_MODE_SUM`, `_REDUCTION_MODE_MEAN`, and `_REDUCTION_MODE_BATCHMEAN`\
    \ are defined to manage different reduction strategies.\n   - `_str_to_reduction_mode`:\
    \ A dictionary mapping string representations of reduction modes to their constant\
    \ values.\n\n2. Function `get_num_warps(BLOCK_SIZE)`:\n   - Determines the optimal\
    \ number of warps based on the provided `BLOCK_SIZE`. It returns 4 by default\
    \ but increases for larger block sizes, up to 32 for `BLOCK_SIZE` >= 32768.\n\n\
    3. `_kldiv_kernel_forward` function:\n   - Takes pointers to the prediction tensor\
    \ `y_ptr`, ground truth tensor `gt_ptr`, and a loss tensor `loss_ptr`, along with\
    \ their strides.\n   - Iterates over blocks of `BLOCK_SIZE` columns, computing\
    \ the KL divergence using the formula `KL(y_true || y) = y_true * (log(y_true)\
    \ - log(y))`.\n   - Supports different reduction modes, handling each mode within\
    \ the loop and storing results accordingly.\n\n4. `_kldiv_kernel_backward` function:\n\
    \   - Similar in structure to its forward counterpart, this kernel computes gradients\
    \ for the backward pass.\n   - For non-log targets, it computes the negative of\
    \ the target. For log targets, it multiplies the negative of the exponential of\
    \ the target by the target.\n\n5. `kldiv_forward_triton` function:\n   - Configures\
    \ and launches the `_kldiv_kernel_forward` with appropriate grid, block size,\
    \ and warp count based on the input tensor shape.\n   - Outputs a tensor reduced\
    \ based on the specified `reduction` mode, returning summed or mean values if\
    \ applicable.\n\n6. `kldiv_backward_triton` function:\n   - Configures and launches\
    \ the `_kldiv_kernel_backward` for computing gradients.\n   - If the `grad_output`\
    \ is a scalar tensor equal to one, it returns the computed gradient directly;\
    \ otherwise, it scales the gradient by `grad_output`.\n\nThe operations leverage\
    \ Triton's capabilities for efficient parallel execution, particularly suited\
    \ for large-scale tensor computations often found in deep learning tasks.\n\n\
    The test code is:\n\n\ndef test_kldiv_triton():\n    # Initialize random inputs\n\
    \    B, S = 4, 8\n    y_pred_np = np.random.rand(B, S).astype(np.float32)\n  \
    \  y_true_np = np.random.rand(B, S).astype(np.float32)\n\n    # Parameters\n \
    \   log_target = False\n    reduction_modes = ['none', 'sum', 'mean', 'batchmean']\n\
    \n    # Ensure input tensors have requires_grad=True\n    y_pred_torch = torch.tensor(y_pred_np,\
    \ requires_grad=True, device=\"cuda\")\n    y_true_torch = torch.tensor(y_true_np,\
    \ device=\"cuda\")\n\n    results = {}\n\n    for i, reduction in enumerate(reduction_modes):\n\
    \        # Triton forward\n        triton_loss = kldiv_forward_triton(y_pred_torch,\
    \ y_true_torch, log_target, reduction)\n\n        # Reset PyTorch gradient\n \
    \       y_pred_torch.grad = None\n\n        # Triton backward\n        grad_output_triton\
    \ = torch.ones_like(triton_loss)  # Ensure shape consistency\n        triton_grad\
    \ = kldiv_backward_triton(y_pred_torch, y_true_torch, grad_output_triton, log_target)\n\
    \n        # Store results\n        results[f'test_case_{i+1}'] = triton_grad.detach().cpu().numpy()\n\
    \n    return results\n\nresult_gold = test_kldiv_triton()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py kldiv_triton.py {kernel_path}` to check\
    \ the correctness and performance.The kernel_path is where you stored the generated\
    \ code.\nCall Status means whether the code can be executed, Exec Status means\
    \ whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- kldiv_triton
