compile_command:
- python rope_transform.py
correctness_command:
- python rope_transform_perf.py
performance_command:
- tb_eval -f rope_transform.py -o rope_transform_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The Triton kernel '_triton_rope' is a JIT-compiled function designed for efficient\
    \ application of rotary position embeddings to input matrices q (queries) and\
    \ k (keys). The function takes pointers to these matrices, their row strides,\
    \ cosine and sine rotation vectors, and several configuration parameters as inputs.\
    \ Inside the kernel, program_id (pid) is used to distribute work across elements\
    \ in a batch-sequence space.\n\nThe kernel reads slices of the q and k matrices,\
    \ applies rotary transformations using cosine and sine components, and stores\
    \ the transformed slices back. The transformation is effectively a 2D rotation\
    \ in each head's dimensional space, achieved through vectorized operations. The\
    \ choice of forward or backward transformation is determined by the BACKWARD_PASS\
    \ constant, allowing flexible operations depending on the training phase.\n\n\
    The function 'rope_forward' serves as a wrapper around '_triton_rope'. It first\
    \ transposes the query and key matrices to the appropriate format, computes the\
    \ necessary paddings using Triton's utility functions to ensure efficient processing,\
    \ and calls the kernel on each batch-sequence pair. It configures the execution\
    \ grid based on the number of rows (batch_size * seq_len) and ensures the inputs\
    \ are contiguous, optimizing memory access patterns. Finally, it returns the matrices\
    \ to their original shapes and provides the updated q, k, cos, and sin matrices.\n\
    \nThe test code is:\n\n\nimport torch\n\ndef test_rope_forward():\n    # Define\
    \ the test parameters\n    batch_size = 2\n    seq_len = 4\n    n_q_head = 8\n\
    \    n_kv_head = 8\n    head_dim = 16\n\n    # Create random input tensors\n \
    \   q = torch.randn(batch_size, n_q_head, seq_len, head_dim, dtype=torch.float32,\
    \ device='cuda')\n    k = torch.randn(batch_size, n_kv_head, seq_len, head_dim,\
    \ dtype=torch.float32, device='cuda')\n    cos = torch.randn(seq_len, head_dim\
    \ // 2, dtype=torch.float32, device='cuda')\n    sin = torch.randn(seq_len, head_dim\
    \ // 2, dtype=torch.float32, device='cuda')\n\n    # Dictionary to store results\
    \ for each test case\n    results = {}\n\n    # Test case 1: Forward pass\n  \
    \  q_out_1, k_out_1, cos_out_1, sin_out_1 = rope_forward(q, k, cos, sin)\n   \
    \ results['test_case_1'] = (q_out_1, k_out_1, cos_out_1, sin_out_1)\n\n    # Test\
    \ case 2: Backward pass\n    q_out_2, k_out_2, cos_out_2, sin_out_2 = rope_forward(q,\
    \ k, cos, sin)\n    results['test_case_2'] = (q_out_2, k_out_2, cos_out_2, sin_out_2)\n\
    \n    return results\n\nresult_gold = test_rope_forward()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py rope_transform.py {kernel_path}` to check\
    \ the correctness and performance.The kernel_path is where you stored the generated\
    \ code.\nCall Status means whether the code can be executed, Exec Status means\
    \ whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rope_transform
