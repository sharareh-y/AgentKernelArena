compile_command:
- python dropout_triton.py
correctness_command:
- python dropout_triton_perf.py
performance_command:
- tb_eval -f dropout_triton.py -o dropout_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `dropout` function is implemented using a Triton kernel named\
    \ `_dropout`. The purpose is to apply dropout to an input tensor `x`, controlled\
    \ by a mask tensor `x_keep`, and store the result in `output`. The kernel operates\
    \ on blocks of elements (`BLOCK_SIZE=1024`) and for each block, calculates an\
    \ offset based on the program ID. The elements are loaded from `x_ptr` and `x_keep_ptr`\
    \ using these offsets and a mask to ensure within bounds. The core operation modifies\
    \ the elements of `x` based on the mask `x_keep`, using the formula `tl.where(x_keep,\
    \ x / (1 - p), 0.0)` to scale retained elements and zero-out others. Results are\
    \ stored back to `output_ptr`. The host function `dropout` ensures input tensor\
    \ `x` is contiguous, computes the grid size for dispatching the Triton kernel,\
    \ and calls `_dropout` with necessary pointers and parameters, enabling parallel\
    \ execution over the elements of `x`.\n            \nThe test code is:\n\n\n#\
    \ Test for the dropout function\ndef test_dropout():\n    # Dictionary to store\
    \ test case results\n    results = {}\n    \n    # Test case 1\n    x = torch.randn(size=(10,)).cuda()\n\
    \    p = 0.5\n    x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n\
    \    output = dropout(x, x_keep=x_keep, p=p)\n    results['test_case_1'] = output\n\
    \n    # Test case 2: p = 0 (no dropout)\n    p = 0.0\n    x_keep = (torch.rand(size=(10,))\
    \ > p).to(torch.int32).cuda()\n    output = dropout(x, x_keep=x_keep, p=p)\n \
    \   results['test_case_2'] = output\n\n    # Test case 3: p = 1 (full dropout)\n\
    \    p = 1.0\n    x_keep = (torch.rand(size=(10,)) > p).to(torch.int32).cuda()\n\
    \    output = dropout(x, x_keep=x_keep, p=p)\n    results['test_case_3'] = output\n\
    \n    # Test case 4: different block size\n    p = 0.5\n    x_keep = (torch.rand(size=(10,))\
    \ > p).to(torch.int32).cuda()\n    output = dropout(x, x_keep=x_keep, p=p)\n \
    \   results['test_case_4'] = output\n    \n    return results\n\n# Run tests and\
    \ store result\nresult_gold = test_dropout()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ dropout_triton.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- dropout_triton
