compile_command:
- python fast_rope_embedding.py
correctness_command:
- python fast_rope_embedding_perf.py
performance_command:
- tb_eval -f fast_rope_embedding.py -o fast_rope_embedding_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton kernel, `_rope_embedding`, and its associated\
    \ Python wrapper function, `Fast_RoPE_Embedding`, are designed to compute the\
    \ Rotary Positional Encoding (RoPE) on input matrices, particularly for query\
    \ (`Q`) and key (`K`) matrices in transformer architectures. The `_rope_embedding`\
    \ kernel is written in Triton, a language for writing efficient GPU compute kernels,\
    \ and it leverages parallel execution to accelerate the RoPE computation. \n\n\
    \            The kernel takes multiple parameters, including:\n            - `Q`:\
    \ the query tensor to be transformed.\n            - `Q_row_stride`: the stride\
    \ between rows in the `Q` tensor.\n            - `cos`, `sin`: tensors containing\
    \ precomputed cosine and sine values for rotation.\n            - `cos_row_stride`,\
    \ `sin_row_stride`: respective strides for cosine and sine tensors.\n        \
    \    - `seqlen`: sequence length constraint.\n            - `head_dim`: dimension\
    \ of each head in the multi-head attention mechanism.\n            - `n_heads`:\
    \ number of attention heads.\n            - `BACKWARD_PASS`: a constant indicating\
    \ if the backward pass is computed.\n            - `BLOCK_SIZE`: the block size\
    \ for Triton kernel execution, determining the parallel execution granularity.\n\
    \n            Within the kernel, each thread computes parts of the RoPE transformation\
    \ for segments of the input tensor. The operation involves computing `Q * cos\
    \ + rotate_half(Q) * sin`, where `rotate_half(Q)` denotes a transformation involving\
    \ half-head dimension offsets.\n\n            The `Fast_RoPE_Embedding` class\
    \ manages the forward and backward passes for autograd in PyTorch. In the `forward`\
    \ method, input tensors are reshaped, processed by `_rope_embedding`, and reshaped\
    \ back to their original form. The backward method processes gradients, passing\
    \ them through the RoPE transformation for backpropagation.\n\n            Finally,\
    \ the `fast_rope_embedding` function applies this transformation to both query\
    \ and key matrices, ensuring compatibility with transformer-based models. It utilizes\
    \ the `transpose` operation to ensure correct dimensions for the RoPE computation.\n\
    \            \nThe test code is:\n\n\nimport torch\n\ndef test_fast_rope_embedding_with_backward():\n\
    \    # Define the test parameters\n    batch_size = 2\n    seq_len = 4\n    n_heads\
    \ = 8\n    head_dim = 16\n\n    # Create random input tensors with requires_grad=True\
    \ for gradient computation\n    Q = torch.randn(batch_size, n_heads, seq_len,\
    \ head_dim, dtype=torch.float32, device='cuda', requires_grad=True)\n    K = torch.randn(batch_size,\
    \ n_heads, seq_len, head_dim, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \n    # Create cos and sin tensors\n    cos = torch.randn(seq_len, head_dim //\
    \ 2, dtype=torch.float32, device='cuda')\n    sin = torch.randn(seq_len, head_dim\
    \ // 2, dtype=torch.float32, device='cuda')\n\n    # Forward pass using fast_rope_embedding\n\
    \    Q_out, K_out = fast_rope_embedding(Q, K, cos, sin)\n\n    # Compute a dummy\
    \ loss function (mean of the outputs)\n    loss = Q_out.mean() + K_out.mean()\n\
    \n    # Perform backward propagation\n    loss.backward()\n\n    # Collect gradients\n\
    \    result = {\n        \"Q_grad\": Q.grad,\n        \"K_grad\": K.grad\n   \
    \ }\n\n    return result\n\n\n# Run the backward test\nresult_gold = test_fast_rope_embedding_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fast_rope_embedding.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fast_rope_embedding
