compile_command:
- python dequantize_rowwise.py
correctness_command:
- python dequantize_rowwise_perf.py
performance_command:
- tb_eval -f dequantize_rowwise.py -o dequantize_rowwise_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel '_dequantize_rowwise' is designed for row-wise dequantization\
    \ of the input tensor 'x'. The kernel takes pointers to the input tensor 'x_ptr',\
    \ the state tensor 'state_x', the output tensor 'output_ptr', a precomputed inverse\
    \ of 127, the total number of elements 'n_elements', a block size 'BLOCK_SIZE',\
    \ and 'P2', which is the nearest power of two of the number of columns. Each kernel\
    \ instance processes a single row identified by 'pid', the program ID in axis\
    \ 0. The 'block_start' variable calculates the starting index for the current\
    \ block, and 'arange' creates an index range up to 'P2'. 'offsets' computes the\
    \ global memory offsets for loading the input tensor. The input values are loaded\
    \ with 'tl.load' using these offsets and a mask to ensure out-of-bound accesses\
    \ are ignored. The maximum value for the row is loaded from 'state_x', and each\
    \ element is dequantized by multiplying with the maximum value and 'inv_127'.\
    \ The results are stored back to 'output_ptr'. The Python function 'dequantize_rowwise'\
    \ is a wrapper that prepares the output tensor, computes 'P2' for efficient memory\
    \ alignment, and sets up the execution grid to invoke the Triton kernel. It assumes\
    \ the input tensor is on CUDA, ensuring compatibility with Triton's GPU-based\
    \ execution.\n    \nThe test code is:\n\n\n# Test function for dequantize_rowwise\n\
    def test_dequantize_rowwise():\n    results = {}\n\n    # Test case 1: Simple\
    \ case\n    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.int8, device='cuda')\n\
    \    state_x = torch.tensor([4.0, 8.0], dtype=torch.float32, device='cuda')\n\
    \    output = dequantize_rowwise(x, state_x)\n    results['test_case_1'] = output\n\
    \n    # Test case 2: Larger input\n    x = torch.randint(-128, 127, (10, 16),\
    \ dtype=torch.int8, device='cuda')\n    state_x = torch.rand(10, dtype=torch.float32,\
    \ device='cuda') * 10\n    output = dequantize_rowwise(x, state_x)\n    results['test_case_2']\
    \ = output\n\n    # Test case 3: Edge case with zeros\n    x = torch.zeros((5,\
    \ 8), dtype=torch.int8, device='cuda')\n    state_x = torch.ones(5, dtype=torch.float32,\
    \ device='cuda')\n    output = dequantize_rowwise(x, state_x)\n    results['test_case_3']\
    \ = output\n\n    # Test case 4: Different block size\n    x = torch.randint(-128,\
    \ 127, (3, 32), dtype=torch.int8, device='cuda')\n    state_x = torch.rand(3,\
    \ dtype=torch.float32, device='cuda') * 10\n    output = dequantize_rowwise(x,\
    \ state_x)\n    results['test_case_4'] = output\n\n    return results\n\n# Run\
    \ the test function\nresult_gold = test_dequantize_rowwise()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py dequantize_rowwise.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- dequantize_rowwise
