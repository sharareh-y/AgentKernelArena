compile_command:
- python int4_matmul.py
correctness_command:
- python int4_matmul_perf.py
performance_command:
- tb_eval -f int4_matmul.py -o int4_matmul_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The code defines a Triton-based kernel for matrix multiplication of INT4\
    \ quantized weights and provides Python functions to handle the quantization and\
    \ dequantization processes. \n        The 'matmul_kernel' function is a Triton\
    \ kernel using @triton.jit which performs matrix multiplication. It processes\
    \ the input matrices in tiles defined by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K,\
    \ and uses a loop to iterate over the K dimension to accumulate results in fp32\
    \ precision, subsequently stored in the output buffer, potentially employing atomic\
    \ add for SPLIT_K > 1.\n        The kernel is set up with a wide variety of configurations,\
    \ allowing Triton's autotuning capabilities to select optimal parameters based\
    \ on matrix dimensions M, N, and K. It utilizes quantized INT4 weights, reconstructing\
    \ them using scales and zero points to compute matrix products accurately. \n\
    \        The Python function 'matmul_dequantize_int4_s2' serves as an interface\
    \ to this kernel, preparing data and launching the computation on specified grid\
    \ dimensions.\n        The function 'quantize_int4' converts a floating-point\
    \ weight matrix into INT4 format, organizing the data by packing 8 INT4 values\
    \ into one INT32 and calculating scaling factors and zero points for each group\
    \ of elements. \n        The helper function 'unpack_int4' is intended for testing;\
    \ it unpacks the INT4 matrix back into a floating-point format to verify the quantization\
    \ process.\n    \nThe test code is:\n\n\ndef test_correct_int4_s2(M=32, K=4096,\
    \ N=4096):\n    group_size = 128\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n\
    \    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n    int_b, b_scale,\
    \ b_zero_point, _ = quantize_int4(b, group_size=group_size)\n    \n    # Test\
    \ case\n    triton_output = matmul_dequantize_int4_s2(a, int_b, b_scale, b_zero_point,\
    \ group_size)\n    \n    results = {\n        \"test_case_1\": triton_output\n\
    \    }\n    \n    return results\n\nresult_gold = test_correct_int4_s2()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py int4_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int4_matmul
