compile_command:
- python swiglu_triton.py
correctness_command:
- python swiglu_triton_perf.py
performance_command:
- tb_eval -f swiglu_triton.py -o swiglu_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton code defines custom forward and backward operations for\
    \ the SWiGLU (SwiGLU: Swish-Gated Linear Units) function using Triton kernels.\
    \ The main functions are `swiglu_forward` and `swiglu_backward`. \n          \
    \  `swiglu_forward` takes input tensors `a` and `b`, reshapes them, and prepares\
    \ an output tensor `c`. It computes the result using the `_swiglu_forward_kernel`,\
    \ which calculates the element-wise product of `b` and the SiLU-activated `a`.\
    \ The kernel uses `tl.load` to load data in blocks, applies the SiLU function\
    \ defined by `silu`, and stores the result using `tl.store`. The kernel launches\
    \ with a configurable grid size defined by the number of rows in the reshaped\
    \ input tensor and a calculated block size, `BLOCK_SIZE`, which is a power of\
    \ two based on the number of columns and limited by `MAX_FUSED_SIZE`.\n      \
    \      `swiglu_backward` computes gradients for `a` and `b` using the `_swiglu_backward_kernel`,\
    \ which utilizes recomputation to save memory. It recalculates the SiLU activation\
    \ and uses it to compute the gradients with respect to the input tensors. The\
    \ backward kernel also uses `tl.load` and `tl.store` for handling data and follows\
    \ a similar block configuration as the forward kernel.\n            The helper\
    \ function `calculate_settings` assists in determining the optimal `BLOCK_SIZE`\
    \ and `num_warps` based on the input size, with adjustments depending on the hardware\
    \ being used (HIP or otherwise). The SiLU activation is implemented as a Triton\
    \ JIT-compiled function, `silu`, using the Triton `tl.sigmoid` operation to define\
    \ the SiLU function.\n            \nThe test code is:\n\n\nimport torch\n\n# Test\
    \ the swiglu_forward function\ndef test_swiglu():\n    # Create input tensors\n\
    \    a = torch.randn(4, 8, dtype=torch.float32, device='cuda')  # Example shape\
    \ (4, 8)\n    b = torch.randn(4, 8, dtype=torch.float32, device='cuda')  # Same\
    \ shape as a\n    dc = torch.randn(4, 8, dtype=torch.float32, device='cuda') \
    \ # Gradient tensor\n\n    # Call the forward function\n    a_out, b_out, c_out\
    \ = swiglu_forward(a, b)\n\n    # Call the backward function\n    da_out, db_out\
    \ = swiglu_backward(a, b, dc)\n\n    # Store results in a dictionary\n    results\
    \ = {\n        \"test_case_1\": (a_out, b_out, c_out, da_out, db_out)\n    }\n\
    \n    return results\n\n# Run the tests\nresult_gold = test_swiglu()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py swiglu_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- swiglu_triton
