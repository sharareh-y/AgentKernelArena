compile_command:
- python var_len_copy.py
correctness_command:
- python var_len_copy_perf.py
performance_command:
- tb_eval -f var_len_copy.py -o var_len_copy_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `var_len_copy_kernel_triton` is a kernel designed to perform\
    \ variable-length copy operations from a source array to a destination array.\
    \ The kernel function accepts pointers to arrays `old_a_start`, `old_a_len`, `old_a_location`,\
    \ `new_a_start`, `new_a_location`, and a constant `BLOCK_SIZE`. The `a_id` is\
    \ the unique identifier for the current program instance, determining which segment\
    \ of data to copy. For each segment, defined by `a_id`, the function reads the\
    \ starting index and length from `old_a_start` and `old_a_len`, respectively,\
    \ and performs a looped copy using Triton's `tl.load` and `tl.store` commands.\
    \ The `old_offset` and `new_offset` are used to iterate over elements within the\
    \ block, while masks ensure that out-of-bound accesses are prevented. The function\
    \ `launch_var_len_copy_triton` prepares and launches the kernel with the grid\
    \ size equal to the number of segments to copy, and specifies `BLOCK_SIZE` as\
    \ 256.\n            \nThe test code is:\n\n\nimport torch\n\ndef test_launch_var_len_copy_kernel_triton():\n\
    \    # Define test input data\n    num_arrays = 3\n    BLOCK_SIZE = 256\n\n  \
    \  # Old array start indices\n    old_a_start = torch.tensor([0, 100, 300], dtype=torch.int32,\
    \ device='cuda')\n\n    # Lengths of each array\n    old_a_len = torch.tensor([50,\
    \ 150, 200], dtype=torch.int32, device='cuda')\n\n    # Flattened old array data\n\
    \    old_a_location = torch.arange(500, dtype=torch.float32, device='cuda')\n\n\
    \    # New array start indices\n    new_a_start = torch.tensor([0, 60, 260], dtype=torch.int32,\
    \ device='cuda')\n\n    # Target flattened array for copying\n    new_a_location\
    \ = torch.zeros(500, dtype=torch.float32, device='cuda')\n\n    # Launch the Triton\
    \ kernel\n    launch_var_len_copy_triton(old_a_start, old_a_len, old_a_location,\
    \ new_a_start, new_a_location)\n\n    # Store results in a dictionary\n    results\
    \ = {}\n    for i in range(num_arrays):\n        old_start = old_a_start[i].item()\n\
    \        new_start = new_a_start[i].item()\n        length = old_a_len[i].item()\n\
    \        # Store the result of each test case\n        results[f\"test_case_{i+1}\"\
    ] = torch.equal(\n            old_a_location[old_start:old_start + length],\n\
    \            new_a_location[new_start:new_start + length]\n        )\n    \n \
    \   return results\n\nresult_gold = test_launch_var_len_copy_kernel_triton()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py var_len_copy.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- var_len_copy
