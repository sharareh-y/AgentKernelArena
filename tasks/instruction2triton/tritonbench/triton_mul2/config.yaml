compile_command:
- python triton_mul2.py
correctness_command:
- python triton_mul2_perf.py
performance_command:
- tb_eval -f triton_mul2.py -o triton_mul2_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton implementation contains two kernels, `mul2_kernel` and `mul2_inplace_kernel`,\
    \ for multiplying elements of a tensor by 2 using parallel computation. Both kernels\
    \ use a block size (`BLOCK_SIZE`) to split the computation into manageable chunks.\
    \ They retrieve the program ID (`pid`) to identify the current block and calculate\
    \ `block_start`, the starting index for this block. Offsets are computed as `block_start\
    \ + tl.arange(0, BLOCK_SIZE)` to locate elements within the block. A mask is created\
    \ to ensure operations stay within `n_elements`, preventing out-of-bounds access.\
    \ The elements are loaded from memory, doubled, and written back to the specified\
    \ location using `tl.load` and `tl.store`. The `triton_mul2` function initializes\
    \ an output tensor, determines the grid size using `triton.cdiv(n_elements, BLOCK_SIZE)`,\
    \ and calls `mul2_kernel` with appropriate arguments. The `triton_mul2_inplace`\
    \ function directly modifies the input tensor, using a similar setup to call `mul2_inplace_kernel`.\
    \ These functions leverage the efficiency of GPU parallelism provided by Triton\
    \ to scale with the size of input data efficiently.\n    \nThe test code is:\n\
    \n\ndef test_mul():\n    N = 1024 * 1024\n    x = torch.randn(N, device='cuda')\n\
    \n    # \u5206\u652F1: triton_mul2 with BLOCK_SIZE=1024\n    triton_mul2_result\
    \ = triton_mul2(x, BLOCK_SIZE=1024)\n\n    # \u5206\u652F2: triton_mul2_inplace\
    \ with BLOCK_SIZE=1024\n    triton_mul2_inplace_result = triton_mul2_inplace(x.clone(),\
    \ BLOCK_SIZE=1024)\n\n    # \u5206\u652F3: triton_mul2 with a different BLOCK_SIZE\n\
    \    triton_mul2_result_case2 = triton_mul2(x, BLOCK_SIZE=512)\n\n    # \u5206\
    \u652F4: triton_mul2_inplace with a different BLOCK_SIZE\n    triton_mul2_inplace_result_case2\
    \ = triton_mul2_inplace(x.clone(), BLOCK_SIZE=512)\n\n    # \u8FD4\u56DE\u6D4B\
    \u8BD5\u7ED3\u679C\n    result_dict = {\n        \"test_case_1\": triton_mul2_result,\n\
    \        \"test_case_2\": triton_mul2_inplace_result,\n        \"test_case_3\"\
    : triton_mul2_result_case2,\n        \"test_case_4\": triton_mul2_inplace_result_case2,\n\
    \    }\n    \n    return result_dict\n\n# \u6267\u884C\u6D4B\u8BD5\u51FD\u6570\
    \nresult_gold = test_mul()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ triton_mul2.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- triton_mul2
