compile_command:
- python bmm_optimized.py
correctness_command:
- python bmm_optimized_perf.py
performance_command:
- tb_eval -f bmm_optimized.py -o bmm_optimized_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided Triton kernel `bmm_kernel` is designed to perform batched\
    \ matrix multiplication on input tensors A and B, producing an output tensor O.\
    \ This kernel is optimized for GPU execution, utilizing tiling and parallel processing.\n\
    \        The key parameters include:\n        - `A`, `B`: Input tensors with shapes\
    \ (batch, M, K) and (batch, K, N). These tensors are assumed to be contiguous\
    \ in memory.\n        - `O`: Output tensor with shape (batch, M, N) where the\
    \ result of the multiplication is stored.\n        - `M`, `N`, `K`: Dimensions\
    \ representing the sizes of the matrices to be multiplied.\n        - `TILE_M`,\
    \ `TILE_N`, `TILE_K`: Compile-time constants defining the tile sizes for the matrix\
    \ multiplication. This tiling strategy divides the matrices into smaller blocks\
    \ that can be processed in parallel.\n        - `GROUP_M`: Influences the order\
    \ of computation across different tiles.\n        - `DIVISIBLE_M`, `DIVISIBLE_N`,\
    \ `DIVISIBLE_K`: Compile-time booleans determining if the dimensions M, N, and\
    \ K are perfectly divisible by the respective tile sizes, optimizing boundary\
    \ handling.\n        \n        The kernel calculates grid indices and offsets\
    \ for processing each tile. Masking is used to handle partial tiles when dimensions\
    \ aren't perfectly divisible by tile sizes. The main computation involves loading\
    \ tiles of A and B, performing a matrix multiplication using `tl.dot`, and accumulating\
    \ the results into the output tile.\n        \n        The `bmm` function is a\
    \ wrapper that initializes the output tensor, determines grid dimensions based\
    \ on input sizes, and launches the `bmm_kernel` using Triton's `autotune` to find\
    \ optimal configurations for TILE_M, TILE_N, TILE_K, and other parameters. It\
    \ ensures the correct execution context on the GPU device and calls the kernel\
    \ with appropriate arguments.\n    \nThe test code is:\n\n\ndef test_bmm():\n\
    \    test_cases = [\n        (1, 32, 32, 32),\n        (8, 64, 64, 32),\n    \
    \    (16, 128, 128, 32),\n    ]\n\n    results = {}\n    for i, (batch_size, M,\
    \ N, K) in enumerate(test_cases):\n        A = torch.randn(batch_size, M, K, dtype=torch.float32,\
    \ device='cuda')\n        B = torch.randn(batch_size, K, N, dtype=torch.float32,\
    \ device='cuda')\n\n        triton_output = bmm(A, B)\n        results[f'test_case_{i+1}']\
    \ = triton_output.cpu().numpy()\n\n    return results\n\nresult_gold = test_bmm()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py bmm_optimized.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- bmm_optimized
