compile_command:
- python llama_ff_triton.py
correctness_command:
- python llama_ff_triton_perf.py
performance_command:
- tb_eval -f llama_ff_triton.py -o llama_ff_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The 'ff_llama' Triton kernel performs a complex fused operation involving matrix\
    \ multiplications and element-wise activation functions. The operation specifically\
    \ computes F.silu(w1(x)) * w3(x), where 'w1' and 'w3' are linear layer weights.\
    \ The kernel integrates root mean square (RMS) scaling using an auxiliary weight\
    \ 'rms_w', ensuring numerical stability with an epsilon parameter 'EPS' during\
    \ normalization.\n\nThe kernel's loop iteratively loads chunks of the input matrix\
    \ 'x', weights 'w1', 'w3', and RMS weights. It computes the accumulated sums for\
    \ two different matrix multiplications ('acc1' and 'acc2') within tiled regions\
    \ defined by 'BLOCK_SIZE_M', 'BLOCK_SIZE_N', and 'BLOCK_SIZE_K'. The kernel considers\
    \ whether to use FP8 precision based on weight data types.\n\nNormalization is\
    \ applied using L2-norm to stabilize the results, followed by combining both accumulators\
    \ with a scaled sigmoid activation to produce the final output. The output is\
    \ stored conditionally, ensuring it respects the tensor dimensions.\n\nThe 'kernel_ff'\
    \ function wraps this kernel, handling the reshaping and preparation of inputs.\
    \ It asserts the correct types and shapes, transposes weight matrices, and sets\
    \ up grid dimensions for parallel execution before invoking the Triton kernel.\
    \ The grid lambda function calculates the grid size required based on input sizes\
    \ and block sizes, ensuring efficient tiling for GPU execution.\n\nThe test code\
    \ is:\n\n\n# Test case for float16 weights\ndef test_ff_llama():\n    results\
    \ = {}\n    batch, seq_len, dim = 2, 4, 64\n    x = torch.randn((batch, seq_len,\
    \ dim), dtype=torch.float16, device='cuda')\n    w1 = torch.randn((dim, dim),\
    \ dtype=torch.float16, device='cuda')\n    w3 = torch.randn((dim, dim), dtype=torch.float16,\
    \ device='cuda')\n    rms_w = torch.randn((dim,), dtype=torch.float16, device='cuda')\n\
    \n    out = kernel_ff(x, w1, w3, rms_w)\n    results[\"test_case_1\"] = out\n\n\
    \    # Test case for different batch size\n    batch, seq_len, dim = 3, 4, 64\n\
    \    x = torch.randn((batch, seq_len, dim), dtype=torch.float16, device='cuda')\n\
    \    w1 = torch.randn((dim, dim), dtype=torch.float16, device='cuda')\n    w3\
    \ = torch.randn((dim, dim), dtype=torch.float16, device='cuda')\n    rms_w = torch.randn((dim,),\
    \ dtype=torch.float16, device='cuda')\n\n    out = kernel_ff(x, w1, w3, rms_w)\n\
    \    results[\"test_case_3\"] = out\n\n    # Test case for different sequence\
    \ length\n    batch, seq_len, dim = 2, 5, 64\n    x = torch.randn((batch, seq_len,\
    \ dim), dtype=torch.float16, device='cuda')\n    w1 = torch.randn((dim, dim),\
    \ dtype=torch.float16, device='cuda')\n    w3 = torch.randn((dim, dim), dtype=torch.float16,\
    \ device='cuda')\n    rms_w = torch.randn((dim,), dtype=torch.float16, device='cuda')\n\
    \n    out = kernel_ff(x, w1, w3, rms_w)\n    results[\"test_case_4\"] = out\n\
    \    return results\n\n# Run tests\nresult_gold = test_ff_llama()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py llama_ff_triton.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- llama_ff_triton
