compile_command:
- python chunk_retention.py
correctness_command:
- python chunk_retention_perf.py
performance_command:
- tb_eval -f chunk_retention.py -o chunk_retention_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This code implements chunk retention operations using Triton, targeting\
    \ high efficiency for transformer-like operations. It includes several kernels,\
    \ each focused on specific computation aspects, along with a PyTorch autograd\
    \ function to facilitate integration into neural network training pipelines.\n\
    \n        - `chunk_retention_fwd_kernel_h`: This Triton kernel calculates an intermediate\
    \ tensor 'h'. It accepts inputs 'k', 'v', and optional 'initial_state', manipulating\
    \ a buffer 'b_h' initialized to zeros. If `USE_INITIAL_STATE` is true, it loads\
    \ initial state data into 'b_h'. It performs several steps:\n            1. Computes\
    \ decay factors 'd_b' and 'd_i' based on a custom decay function.\n          \
    \  2. Iterates over time dimension 'NT', loading blocks of 'k' and 'v'.\n    \
    \        3. Updates 'b_h' with a dot product computation, applying decay.\n  \
    \          4. Optionally stores final state in 'final_state' if `STORE_FINAL_STATE`\
    \ is set.\n\n        - `chunk_retention_fwd_kernel_o`: This kernel computes the\
    \ output tensor 'o'. It integrates:\n            1. Loading blocks of 'q', 'k',\
    \ 'v', and precomputed 'h'.\n            2. Uses decay factors 'd_i' and precomputed\
    \ values to modulate contributions.\n            3. Aggregates contributions in\
    \ 'b_o' and 'b_s' from interactions between 'q', 'k', and 'v'.\n            4.\
    \ Final 'o' is calculated by scaling and storing the aggregated results.\n\n \
    \       - `chunk_retention_bwd_kernel_dh`: Part of the backward pass kernels,\
    \ it calculates 'dh', accumulating gradient contributions while iterating backwards\
    \ over time steps.\n\n        - `chunk_retention_bwd_kernel_dqkv`: This kernel\
    \ computes the gradients with respect to 'q', 'k', and 'v' (denoted as 'dq', 'dk',\
    \ 'dv'). It uses:\n            1. Loading blocks and calculating gradients using\
    \ Triton's matrix operations.\n            2. Applying decay and scaling to modulate\
    \ contributions.\n            3. Storing calculated gradients for 'q', 'k', 'v'\
    \ efficiently.\n\n        - `ChunkRetentionFunction`: A custom PyTorch autograd\
    \ function wrapping these kernels, providing an interface for forward and backward\
    \ passes. It automatically handles device operations and manages kernel launch\
    \ configuration.\n\n        The `chunk_retention` function acts as a user interface,\
    \ preparing inputs, invoking the autograd function, and handling optional initial\
    \ state and final state management.\n    \nThe test code is:\n\n\ndef test_chunk_retention():\n\
    \    # Define the dimensions for the test\n    B, H, T, K, V = 2, 4, 128, 64,\
    \ 64\n\n    # Create random input tensors with requires_grad=True for gradient\
    \ test\n    q = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \    k = torch.randn(B, H, K, T, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \    v = torch.randn(B, H, T, V, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \n    # Initial state if needed\n    initial_state = torch.randn(B, H, K, V, dtype=torch.float32,\
    \ device='cuda', requires_grad=True)\n\n    results = {}\n\n    # ========== Test\
    \ Case 1 ==========\n    # Without initial state, without final state output\n\
    \    q1, k1, v1 = q.detach().clone().requires_grad_(True), k.detach().clone().requires_grad_(True),\
    \ v.detach().clone().requires_grad_(True)\n\n    o, final_state = chunk_retention(q1,\
    \ k1, v1, initial_state=None, output_final_state=False)\n    assert final_state\
    \ is None, \"Final state should be None when output_final_state=False\"\n    #\
    \ Backward test\n    loss = o.sum()\n    loss.backward()\n    # Check gradients\n\
    \    assert q1.grad is not None, \"Gradient not calculated for q in Test Case\
    \ 1\"\n    assert k1.grad is not None, \"Gradient not calculated for k in Test\
    \ Case 1\"\n    assert v1.grad is not None, \"Gradient not calculated for v in\
    \ Test Case 1\"\n    results['test_case_1'] = o.detach().cpu()\n\n    # ==========\
    \ Test Case 2 ==========\n    # Without initial state, with final state output\n\
    \    q2, k2, v2 = q.detach().clone().requires_grad_(True), k.detach().clone().requires_grad_(True),\
    \ v.detach().clone().requires_grad_(True)\n\n    o, final_state = chunk_retention(q2,\
    \ k2, v2, initial_state=None, output_final_state=True)\n    assert final_state\
    \ is not None, \"Final state should not be None when output_final_state=True\"\
    \n    # Backward test\n    loss = o.sum()\n    loss.backward()\n    # Check gradients\n\
    \    assert q2.grad is not None, \"Gradient not calculated for q in Test Case\
    \ 2\"\n    assert k2.grad is not None, \"Gradient not calculated for k in Test\
    \ Case 2\"\n    assert v2.grad is not None, \"Gradient not calculated for v in\
    \ Test Case 2\"\n    results['test_case_2'] = o.detach().cpu()\n\n    # ==========\
    \ Test Case 3 ==========\n    # With initial state, without final state output\n\
    \    q3, k3, v3, is3 = q.detach().clone().requires_grad_(True), k.detach().clone().requires_grad_(True),\
    \ v.detach().clone().requires_grad_(True), initial_state.detach().clone().requires_grad_(True)\n\
    \n    o, final_state = chunk_retention(q3, k3, v3, initial_state=is3, output_final_state=False)\n\
    \    assert final_state is None, \"Final state should be None when output_final_state=False\
    \ and we have initial state\"\n    # Backward test\n    loss = o.sum()\n    loss.backward()\n\
    \    # Check gradients\n    assert q3.grad is not None, \"Gradient not calculated\
    \ for q in Test Case 3\"\n    assert k3.grad is not None, \"Gradient not calculated\
    \ for k in Test Case 3\"\n    assert v3.grad is not None, \"Gradient not calculated\
    \ for v in Test Case 3\"\n    results['test_case_3'] = o.detach().cpu()\n\n  \
    \  # ========== Test Case 4 ==========\n    # With initial state, with final state\
    \ output\n    q4, k4, v4, is4 = q.detach().clone().requires_grad_(True), k.detach().clone().requires_grad_(True),\
    \ v.detach().clone().requires_grad_(True), initial_state.detach().clone().requires_grad_(True)\n\
    \n    o, final_state = chunk_retention(q4, k4, v4, initial_state=is4, output_final_state=True)\n\
    \    assert final_state is not None, \"Final state should not be None when output_final_state=True\"\
    \n    # Backward test\n    loss = o.sum()\n    loss.backward()\n    # Check gradients\n\
    \    assert q4.grad is not None, \"Gradient not calculated for q in Test Case\
    \ 4\"\n    assert k4.grad is not None, \"Gradient not calculated for k in Test\
    \ Case 4\"\n    assert v4.grad is not None, \"Gradient not calculated for v in\
    \ Test Case 4\"\n    results['test_case_4'] = o.detach().cpu()\n\n    return results\n\
    \nresult_gold = test_chunk_retention()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ chunk_retention.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_retention
