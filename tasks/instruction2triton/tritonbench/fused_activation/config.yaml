compile_command:
- python fused_activation.py
correctness_command:
- python fused_activation_perf.py
performance_command:
- tb_eval -f fused_activation.py -o fused_activation_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This code provides a detailed example of a Triton kernel 'fused_add_mul_activation_kernel'\
    \ and its corresponding PyTorch wrapper function 'fused_add_mul_activation_torch'.\
    \ The kernel performs a fused operation of addition, multiplication, and activation\
    \ on tensors. It operates on the input tensor 'x_ptr' by adding elements from\
    \ 'bias_ptr' and a scaled version of 'in_ptr', then applies an activation function\
    \ ('sigmoid' or 'relu'). Inputs are accessed in blocks, controlled by 'BLOCK_SIZE',\
    \ which allows efficient memory handling and parallel execution. The function\
    \ 'fused_add_mul_activation_torch' configures the grid for executing the Triton\
    \ kernel based on the size of 'in_out_tensor' and invokes the kernel. The kernel\
    \ processes data in chunks using block-wise indexing, applies the specified mathematical\
    \ transformations, and stores the results back. It also manages memory through\
    \ specific eviction policies to optimize cache usage. The wrapper sets a default\
    \ multiplier and activation type, executes the kernel with these parameters, and\
    \ returns the processed tensor.\n        \nThe test code is:\n\n\ndef test_fused_add_mul_activation():\n\
    \    # \u8F93\u5165\u5F20\u91CF\u5F62\u72B6\n    num_elements = 8192  # \u603B\
    \u5143\u7D20\u6570\u91CF\n    num_weights = 64     # \u504F\u7F6E\u7684\u6570\u91CF\
    \n\n    # \u521B\u5EFA\u8F93\u5165\u5F20\u91CF\n    in_out_tensor = torch.randn(num_elements,\
    \ dtype=torch.float32, device='cuda')  # \u8F93\u5165\u8F93\u51FA\u5F20\u91CF\n\
    \    bias = torch.randn(num_weights, dtype=torch.float32, device='cuda')  # \u504F\
    \u7F6E\n    in_tensor = torch.randn(num_elements, dtype=torch.float32, device='cuda')\
    \  # \u989D\u5916\u8F93\u5165\u5F20\u91CF\n\n    # \u5206\u652F1: activation=\"\
    sigmoid\"\n    result_sigmoid = fused_add_mul_activation_torch(in_out_tensor.clone(),\
    \ bias, in_tensor)\n\n    # \u5206\u652F2: activation=\"relu\"\n    grid = lambda\
    \ meta: (triton.cdiv(in_out_tensor.numel(), meta['BLOCK_SIZE']),)\n    BLOCK_SIZE\
    \ = min(2048, in_out_tensor.numel())\n    fused_add_mul_activation_kernel[grid](in_out_tensor,\
    \ bias, in_tensor,\n                                          bias.numel(),\n\
    \                                          in_out_tensor.numel(),\n          \
    \                                multiplier=0.5,\n                           \
    \               activation=\"relu\",\n                                       \
    \   BLOCK_SIZE=BLOCK_SIZE)\n    result_relu = in_out_tensor.clone()\n\n    # \u5206\
    \u652F\u8986\u76D6\u7387\u30102/4\u3011\n    results = {\n        \"test_case_1\"\
    : result_sigmoid[:10].cpu().numpy(),\n        \"test_case_2\": result_relu[:10].cpu().numpy()\n\
    \    }\n    return results\n\nresult_gold = test_fused_add_mul_activation()\n\n\
    \nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fused_activation.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_activation
