compile_command:
- python bmm_chunk_bwd.py
correctness_command:
- python bmm_chunk_bwd_perf.py
performance_command:
- tb_eval -f bmm_chunk_bwd.py -o bmm_chunk_bwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel _bmm_chunk_bwd_kernel is designed to compute the\
    \ backward pass for a batched matrix multiplication operation, particularly in\
    \ scenarios involving chunking of one of the matrices. The kernel is decorated\
    \ with several configurations for autotuning, each specifying the number of stages,\
    \ warps, and block sizes (BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_CS) for optimal\
    \ execution.\n\n            Inputs include pointers to matrices (a_ptr, dout_ptr)\
    \ and their respective strides and dimensions, such as stride_a_batch, stride_dout_csize_m,\
    \ etc. The kernel calculates the partial dot product accumulation using tl.dot\
    \ and can add a residual from res_ptr if HAS_RESIDUAL is set to True. The result\
    \ is stored in db_ptr.\n\n            The auxiliary function _bmm_chunk_bwd prepares\
    \ tensors, ensuring they are contiguous and appropriately shaped, while determining\
    \ the computation grid size based on the meta-parameters. It invokes the Triton\
    \ kernel using triton.jit for execution on the CUDA device associated with the\
    \ input tensor. This function ensures alignment of tensor dimensions, strides,\
    \ and types, converting them as necessary for the underlying Triton operations.\n\
    \            \nThe test code is:\n\n\nimport torch\n\n# Test for _bmm_chunk_bwd\n\
    def test_bmm_chunk_bwd():\n    results = {}\n    \n    # Test case 1: Without\
    \ groups, no residual\n    a = torch.randn(2, 128, 64, device='cuda', dtype=torch.float16)\n\
    \    dout = torch.randn(2, 4, 32, 32, device='cuda', dtype=torch.float16)\n  \
    \  out = _bmm_chunk_bwd(a, dout)\n    results['test_case_1'] = out.shape\n\n \
    \   # Test case 2: With groups, with residual\n    a = torch.randn(2, 128, 4,\
    \ 64, device='cuda', dtype=torch.float16)\n    dout = torch.randn(2, 4, 4, 32,\
    \ 32, device='cuda', dtype=torch.float16)\n    residual = torch.randn(2, 128,\
    \ 4, 64, device='cuda', dtype=torch.float16)\n    out = _bmm_chunk_bwd(a, dout,\
    \ residual=residual)\n    results['test_case_2'] = out.shape\n\n    return results\n\
    \n# Run tests\nresult_gold = test_bmm_chunk_bwd()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ bmm_chunk_bwd.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- bmm_chunk_bwd
