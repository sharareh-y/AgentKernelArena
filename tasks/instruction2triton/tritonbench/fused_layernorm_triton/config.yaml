compile_command:
- python fused_layernorm_triton.py
correctness_command:
- python fused_layernorm_triton_perf.py
performance_command:
- tb_eval -f fused_layernorm_triton.py -o fused_layernorm_triton_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel `triton_red_fused_native_layer_norm_0` within `fused_native_layer_norm`\
    \ performs layer normalization on the input tensor `primals_3` (shape `(S, D)`).\
    \ The process involves calculating the mean and variance across the rows of the\
    \ input tensor using a parallel reduction technique known as the Welford algorithm,\
    \ facilitated by Triton's helper functions. Each thread block processes a segment\
    \ of the input (`RBLOCK` elements) and updates shared buffers for mean, variance,\
    \ and count (`tmp3_mean`, `tmp3_m2`, `tmp3_weight`). After the reduction, mean\
    \ (`tmp3`), variance (`tmp4`), and count (`tmp5`) are stored in buffer `out_ptr0`\
    \ and used to compute the normalization factor. The main normalization operation\
    \ involves centering the input by subtracting the mean and scaling with the reciprocal\
    \ of the variance's square root (`tmp10`). The kernel also applies optional affine\
    \ transformations using `primals_1` (scale) and `primals_2` (bias), storing the\
    \ final normalized outputs in `out_ptr1`. The function sets up the kernel execution\
    \ using Triton's grid and block configuration and manages CUDA streams for device\
    \ synchronization. The `fused_native_layer_norm` Python wrapper sets up these\
    \ inputs and invokes the Triton kernel, preparing auxiliary output tensors like\
    \ `buf0`, `buf3`, and the main result in `buf4`.\n    \nThe test code is:\n\n\n\
    import torch\n\ndef test_fused_native_layer_norm():\n    # Define the input shapes\n\
    \    S = 128  # Number of sequences\n    D = 4096  # Dimension of each sequence\n\
    \n    # Create input tensors with appropriate shapes and data types\n    primals_1\
    \ = torch.randn(D, dtype=torch.bfloat16, device='cuda')  # Weight tensor\n   \
    \ primals_2 = torch.randn(D, dtype=torch.bfloat16, device='cuda')  # Bias tensor\n\
    \    primals_3 = torch.randn(S, D, dtype=torch.bfloat16, device='cuda')  # Input\
    \ tensor\n\n    # Test the fused_native_layer_norm function\n    test_case_1 =\
    \ fused_native_layer_norm(primals_1, primals_2, primals_3)\n\n    # Additional\
    \ test cases to cover all branches\n    S = 256\n    D = 2048\n    primals_1 =\
    \ torch.randn(D, dtype=torch.bfloat16, device='cuda')\n    primals_2 = torch.randn(D,\
    \ dtype=torch.bfloat16, device='cuda')\n    primals_3 = torch.randn(S, D, dtype=torch.bfloat16,\
    \ device='cuda')\n    test_case_2 = fused_native_layer_norm(primals_1, primals_2,\
    \ primals_3)\n\n    S = 64\n    D = 8192\n    primals_1 = torch.randn(D, dtype=torch.bfloat16,\
    \ device='cuda')\n    primals_2 = torch.randn(D, dtype=torch.bfloat16, device='cuda')\n\
    \    primals_3 = torch.randn(S, D, dtype=torch.bfloat16, device='cuda')\n    test_case_3\
    \ = fused_native_layer_norm(primals_1, primals_2, primals_3)\n\n    S = 512\n\
    \    D = 1024\n    primals_1 = torch.randn(D, dtype=torch.bfloat16, device='cuda')\n\
    \    primals_2 = torch.randn(D, dtype=torch.bfloat16, device='cuda')\n    primals_3\
    \ = torch.randn(S, D, dtype=torch.bfloat16, device='cuda')\n    test_case_4 =\
    \ fused_native_layer_norm(primals_1, primals_2, primals_3)\n\n    return {\n \
    \       \"test_case_1\": test_case_1,\n        \"test_case_2\": test_case_2,\n\
    \        \"test_case_3\": test_case_3,\n        \"test_case_4\": test_case_4,\n\
    \    }\n\nresult_gold = test_fused_native_layer_norm()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py fused_layernorm_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_layernorm_triton
