source_file_path:
  - source/triton_linear_attn_decode.py

target_kernel_functions:
  - _linear_attn_decode_kernel

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton linear attention decode kernel `_linear_attn_decode_kernel`
    for maximum GPU throughput while maintaining numerical correctness.

    The kernel performs linear attention decoding for a single token step:
    1. Compute kv_outer = k[:, None] * v[None, :]
    2. Apply decay: kv_state = kv_outer + exp(-slope) * kv_cache_old
    3. Compute output = sum(q[:, None] * kv_state, axis=0)
    4. Update KV cache in-place

    Key parameters:
    - D: query/key dimension (constexpr)
    - BLOCK_SIZE: value dimension block size (default 32)
    - Uses slot_idx for batched KV cache indexing

    Constraints:
    - Must maintain the same function signature for `linear_attn_decode_forward`
    - Output must match reference within atol=1e-2, rtol=1e-2 for float16
    - The kernel must handle slot_idx=-1 for padding
  cheatsheet: null
