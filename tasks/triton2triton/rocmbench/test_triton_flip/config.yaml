compile_command:
- python -c "import ast; ast.parse(open('test_triton_flip.py').read())"
correctness_command:
- pytest -vv -x --maxfail=1 test_triton_flip.py -k "not test_performance and not test_save_performance_results"
performance_command:
- pytest -vv -x --maxfail=1 test_triton_flip.py -k "test_performance or test_save_performance_results"
task_type: triton2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "\nYou are an expert in triton programming language. You will be given\
    \ the function definition for the `flip_kernel`. Your task is to complete the\
    \ kernel code. Only optimize the kernel code in the function definition, DONT\
    \ remove any python imports or helper utils in the instruction/code provided,\
    \ DONT change/interfere with the provided function definition and parameter list.\n\
    \nThis kernel, `flip_kernel`,  Flips each row of a 2D tensor horizontally.\n\n\
    **Your objective is to optimize the body of `flip_kernel`.**\n\nYou must ensure\
    \ that:\n1.  All arguments received by `flip_kernel` are kept intact and not modified.\n\
    2. Provide you final code in ```python code block. \nExample:\n```python\n<YOUR-CODE-HERE>\n\
    ```\nThe full definition for `flip_kernel` and relevant helper utilities are provided\
    \ in the context below. You only need to optimize the code for `flip_kernel` whilst\
    \ keeping other things intact. DONT remove Imports and HELPER utils.\n\n########################################\
    \ Imports ########################################\nimport pytest\nimport torch\n\
    import triton\nimport triton.language as tl\nimport numpy as np\n########################################\
    \ Imports ########################################\n\n@triton.jit\ndef flip_kernel(X,\
    \ Z, N: tl.constexpr, M: tl.constexpr):\n    \"\"\"\n    Processes 2D blocks of\
    \ data, flipping each block horizontally.\n\n    This kernel loads a 2D block\
    \ of data of shape (N, M) from an input tensor `X`.\n    It then flips this block\
    \ along its second dimension (columns), meaning each\n    row within the block\
    \ is reversed. The resulting flipped block is then\n    stored into an output\
    \ tensor `Z` at the corresponding offset.\n\n    Parameters\n    ----------\n\
    \    X\n        Pointer to the input tensor. Each kernel instance will load an\
    \ (N, M) block from this tensor.\n    Z\n        Pointer to the output tensor.\
    \ Each kernel instance will store the flipped (N, M) block to this tensor.\n \
    \       It can be the same as X for an in-place operation if memory layout and\
    \ access patterns allow.\n    N : tl.constexpr\n        A compile-time constant\
    \ specifying the size of the first dimension\n        (e.g., number of rows) of\
    \ the 2D data block to be processed by each kernel instance.\n    M : tl.constexpr\n\
    \        A compile-time constant specifying the size of the second dimension\n\
    \        (e.g., number of columns) of the 2D data block to be processed by each\
    \ kernel instance.\n        The flip operation occurs along this dimension.\n\
    \    \"\"\"\n    # Your code here\n\n"
  source_code: null
source_file_path:
- test_triton_flip.py
target_kernel_functions:
- flip_kernel
