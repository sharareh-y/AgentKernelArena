source_file_path:
  - source/triton_silu_mul_fp8_quant_dg.py

target_kernel_functions:
  - _silu_mul_fp8_quant_deep_gemm

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton `_silu_mul_fp8_quant_deep_gemm` kernel for maximum GPU throughput.
    This kernel computes fused SiLU activation + elementwise multiply + FP8 quantization
    for DeepGEMM MoE. Input [E, T, 2*H] is split into gate and up projections,
    then y = silu(gate) * up is quantized to FP8 with per-group scales.

    Key optimization opportunities:
    - Pipeline stages for token loop
    - Warp count tuning
    - Memory access patterns for strided scales

    Constraints:
    - Must maintain the same function signature for `silu_mul_fp8_quant`
    - Dequantized output must match reference within atol=0.5, rtol=0.2
    - For AMD GPU use torch.float8_e4m3fnuz
  cheatsheet: null
