source_file_path:
  - source/_fwd_kernel_stage2.py

target_kernel_functions:
  - _fwd_kernel_stage2

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton decode attention stage2 kernel `_fwd_kernel_stage2` for
    maximum GPU throughput while maintaining numerical correctness.

    The kernel implements the second stage of split-KV decode attention. It
    reduces partial attention outputs from stage1 across KV splits:
    - Each program handles one (batch, head)
    - Iterates over all KV splits, loading partial output and logsumexp
    - Combines using the logsumexp trick:
        max_lse = max(lse across splits)
        o = sum(exp(lse_i - max_lse) * mid_o_i) / sum(exp(lse_i - max_lse))
    - Outputs final attention output and final logsumexp

    Key optimization opportunities:
    - Loop unrolling over NUM_KV_SPLITS
    - Memory access pattern optimization
    - Warp scheduling and occupancy tuning (num_warps, num_stages)
    - Vectorized loads/stores

    Constraints:
    - Must maintain the same function signature for `decode_softmax_reducev_fwd`
    - Output must match reference within atol=1e-2, rtol=1e-2 for float16
    - Must handle arbitrary head dimensions and number of KV splits
  cheatsheet: null
