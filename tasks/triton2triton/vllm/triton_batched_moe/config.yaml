source_file_path:
  - source/triton_batched_moe.py

target_kernel_functions:
  - batched_triton_kernel

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton `batched_triton_kernel` for maximum GPU throughput.
    This kernel performs batched MoE GEMM with all experts in a single launch
    using a 2D grid (expert_id, M_blocks*N_blocks). It skips experts with
    zero tokens and handles variable token counts per expert.

    Key optimization opportunities:
    - Block size tuning for the 2D grid
    - K-loop pipelining
    - Expert-level parallelism

    Constraints:
    - Must maintain the same function signature for `batched_moe_gemm`
    - Output must match reference within atol=5e-2, rtol=5e-2 for float16
  cheatsheet: null
