source_file_path:
  - source/triton_fused_moe_gptq_awq.py

target_kernel_functions:
  - fused_moe_kernel_gptq_awq

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton `fused_moe_kernel_gptq_awq` for maximum GPU throughput.
    This kernel performs fused MoE GEMM with GPTQ/AWQ quantized weights,
    supporting 4-bit and 8-bit weight-only quantization with scales and zero points.

    Key optimization opportunities:
    - Block size tuning
    - Efficient dequantization pipeline
    - Memory access patterns for packed weights

    Constraints:
    - Must maintain the same function signature for `fused_moe_gptq_awq`
    - Output must match reference within atol=1.0, rtol=0.5
  cheatsheet: null
