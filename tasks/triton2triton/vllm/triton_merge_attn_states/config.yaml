source_file_path:
  - source/triton_merge_attn_states.py

target_kernel_functions:
  - merge_attn_states_kernel

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton kernel `merge_attn_states_kernel` for maximum GPU throughput
    while maintaining numerical correctness.

    The kernel merges prefix and suffix partial attention outputs using logsumexp
    rescaling (section 2.2 of https://arxiv.org/pdf/2501.01005). It computes:
      out = (exp(lse_p - max_lse) * prefix_out + exp(lse_s - max_lse) * suffix_out)
            / (exp(lse_p - max_lse) + exp(lse_s - max_lse))

    Key optimization opportunities:
    - Vectorized loads/stores for the head dimension
    - Memory access coalescing
    - Warp scheduling and occupancy tuning

    Constraints:
    - Must maintain the same function signature for `merge_attn_states`
    - Output must match reference within atol=1e-2, rtol=1e-2 for float16
  cheatsheet: null
