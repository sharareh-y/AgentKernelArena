source_file_path:
  - source/_fwd_grouped_kernel_stage1.py

target_kernel_functions:
  - _fwd_grouped_kernel_stage1

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton decode attention grouped stage1 kernel
    `_fwd_grouped_kernel_stage1` for maximum GPU throughput while maintaining
    numerical correctness.

    The kernel implements the first stage of split-KV decode attention for
    grouped-query attention (GQA/MQA). Multiple Q heads that share the same KV
    head are processed together using BLOCK_H:
    - Uses a grid of (batch, ceil(num_heads / BLOCK_H), num_kv_splits)
    - Each program loads BLOCK_H Q vectors and computes attention against
      the shared KV head
    - Reads K/V from a paged KV cache via Req_to_tokens mapping
    - Uses tl.dot for batched Q @ K^T computation
    - Supports optional positional encoding (BLOCK_DPE) for architectures like
      DeepSeek MLA
    - Outputs partial attention output and logsumexp for later reduction

    Key optimization opportunities:
    - BLOCK_N and BLOCK_H tuning for the target GPU
    - Memory access coalescing for paged KV cache reads
    - Warp scheduling and occupancy tuning (num_warps, num_stages)
    - Efficient use of tl.dot for the grouped head computation
    - Reducing masking overhead

    Constraints:
    - Must maintain the same function signature for `decode_grouped_att_m_fwd`
    - Output must match reference within atol=1e-2, rtol=1e-2 for float16
    - Must handle arbitrary sequence lengths, head dimensions, page sizes,
      and group sizes
  cheatsheet: null
