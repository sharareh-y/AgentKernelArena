source_file_path:
  - source/triton_flash_prefill_attention.py

target_kernel_functions:
  - _fwd_kernel

compile_command:
  - python3 scripts/task_runner.py compile

correctness_command:
  - python3 scripts/task_runner.py correctness

performance_command:
  - python3 scripts/task_runner.py performance

task_type: triton2triton

task_result_template: null

prompt:
  source_code: null
  instructions: |
    Optimize the Triton flash attention prefill kernel `_fwd_kernel` for maximum
    GPU throughput while maintaining numerical correctness.

    The kernel implements memory-efficient flash attention for the prefill stage of
    LLM inference. It processes variable-length packed sequences with support for:
    - Grouped-query attention (GQA) and multi-query attention (MQA)
    - Causal masking
    - Bidirectional sliding window masking
    - Online softmax with exp2 for numerical stability

    Key optimization opportunities:
    - Tile size tuning (BLOCK_M, BLOCK_N) for the target GPU
    - Memory access pattern optimization (coalescing, prefetching)
    - Warp scheduling and occupancy tuning (num_warps, num_stages)
    - Reducing unnecessary masking overhead for common cases
    - Loop unrolling and instruction-level parallelism

    Constraints:
    - Must maintain the same function signature for `context_attention_fwd`
    - Output must match reference within atol=1e-2, rtol=1e-2 for float16
    - The kernel must handle arbitrary sequence lengths and head dimensions
  cheatsheet: null
