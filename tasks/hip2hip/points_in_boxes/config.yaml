source_file_path:
- src/points_in_boxes_cuda.hip
target_kernel_functions:
- points_in_boxes_part_kernel
- points_in_boxes_all_kernel
compile_command:
- python3 scripts/task_runner.py compile
correctness_command:
- python3 scripts/task_runner.py correctness
performance_command:
- python3 scripts/task_runner.py performance
task_type: hip2hip
task_result_template: task_result_template_four_output_perf.yaml
prompt:
  source_code: null
  instructions: null
  cheatsheet: 'Please optimize the a HIP code implementation (aimed for ROCM platform,
    MI300X GPU) for better performance. MI300X specs: 64KB LDS per Compute Unit (CU),
    304 CUs total. Follows are some guidelines for optimization: 1. Chunked processing:
    Divide large data into fixed-size chunks (e.g., threads x items/elements) to fit
    in registers/shared memory, enable streaming computation, and minimize global
    memory accesses. Process each chunk independently while carrying over state. \n2.
    Shared memory for state propagation: Use shared memory as a buffer to handle inter-chunk
    dependencies, avoiding redundant global memory reads. Store and shift data for
    efficient access by threads. \n3. Delayed operations: Postpone writes to shared
    memory until after dependent reads to prevent data races and overwrites, ensuring
    correct sequential dependencies. \n4. Vectorized I/O: Perform loads/stores in
    vector types (e.g., 4 or 8 elements for float/half) for coalesced memory access.
    Use direct mode for aligned data or warp-transpose for flexibility, reducing instruction
    count and boosting bandwidth. \n5. CUB primitives: Employ CUB library for parallel
    operations: BlockLoad/BlockStore for efficient, coalesced input/output with temporary
    shared memory; BlockScan for prefix computations where needed. \n6. Loop unrolling:
    Apply #pragma unroll to inner loops (e.g., over dimensions or elements) to reduce
    branching overhead and enable compiler optimizations like instruction scheduling.
    \n7. Bounded accesses: Implement conditional checks in loads/stores (e.g., if
    index < length) to safely handle variable data sizes and prevent out-of-bounds
    errors. \n8. Type and feature handling: Use templates for data types (e.g., float/half/bf16,
    optional complex); boolean switches for optional features like activations. \n9.
    Resource limiting for occupancy: Reduce shared memory (LDS) and register usage
    per workgroup to boost occupancy, allowing more concurrent workgroups per CU/SM
    for improved parallelism and latency hiding. \n10. Branch divergence minimization:
    Structure code to minimize divergent branches within warps, ensuring threads execute
    the same path where possible. \n11. Instruction-level parallelism: Maximize ILP
    by interleaving independent instructions to hide latencies. \n12. Performance-enhancing
    techniques specific to AMD GPUs: Apply AMD-specific optimizations like wavefront
    management or ROCm-tuned configurations. \n13. Kernel fusion or splitting opportunities:
    Fuse multiple kernels to reduce launches and global memory traffic, or split for
    better resource utilization. \n 14. Stream and asynchronous execution: Use ROCm
    streams for overlapping computation and data transfer asynchronously. \n15. Memory
    hierarchy utilization: Cache reusable data in shared memory (LDS on MI308X) to
    minimize global memory accesses and latency. \n16. Data packing and alignment:
    Restructure arrays (e.g., AoS to SoA or padded vectors) for coalesced, vectorized
    loads/stores. \n17. Loop unrolling and fusion: Unroll fixed-size loops; fuse operations
    (e.g., FMA) to boost ILP and reduce overhead. \n18. Branch minimization: Replace
    branches with arithmetic or bitwise masks; use constants for thresholds to enable
    compiler optimizations. \n19. Output streamlining: Accumulate and write results
    in a way that reduces strided accesses and leverages hardware intrinsics. \nYou
    can apply other aspects of optimization that fit the kernel. \nImportant requirements:\n1.
    MUST keep the exact same kernel function name \n2. MUST maintain the same kernel
    function signature and parameter types, unless signature change is essential for
    performance (e.g., data packing); if changed, MUST provide updated main function
    calls and document rationale.\n3. MUST keep the same kernel launch configuration
    structure\n4. MUST ensure the code is directly compilable and runnable\n5. MUST
    preserve the same algorithm logic and correctness\n6. MUST maintain the same comments
    and code formatting style\n7. If the parameter of the kernel is not used, you
    should remove it and not return it in the code\n8. MUST define shared_memory_size
    before kernel launch if using shared memory\n\nReturn the optimized implementation
    including:\n1. The optimized kernel function with the exact same name and signature\n2.
    Any modified kernel launch parameters (if needed)\n3. Any additional helper functions
    or kernels (if needed)\n4. Any changes to the launch configuration (if needed)\n\nThe
    code must be directly compilable and runnable with the same interface as the original
    implementation. Do not modify the input types and values used when calling the
    kernel in the main function.'
