compile_command:
- python apply_penalty.py
correctness_command:
- python apply_penalty_perf.py
performance_command:
- tb_eval -f apply_penalty.py -o apply_penalty_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton JIT kernel `_fwd_kernel_apply_penalty` is designed to adjust the\
    \ logits of a language model based on three types of penalties: presence, frequency,\
    \ and repetition. These adjustments help in controlling the model's output behavior\
    \ concerning repetitive and previously seen tokens. The kernel operates per batch\
    \ (`cur_batch`) and involves the following steps: it first loads the penalty coefficients\
    \ for the current batch. It then determines the range of token indices for the\
    \ batch using `p_cumsum_seq_len`. Using these indices, it loads token IDs (`batch_ids`)\
    \ and their corresponding counts (`batch_ids_count`) from `p_token_ids` and `p_token_counts`,\
    \ respectively. For each token, it adjusts the logit based on repetition, then\
    \ frequency, and finally presence penalties. The adjusted logits are stored back\
    \ into the `Logits` tensor. The function `apply_penalty` is a Python wrapper that\
    \ ensures `Logits` is contiguous and calculates the appropriate block size (`BLOCK`)\
    \ using Triton's `next_power_of_2` function, ensuring it adheres to a minimum\
    \ size for efficiency. It then invokes the kernel for each batch dimension (`Logits.shape[0]`),\
    \ passing necessary parameters like penalties, token IDs, counts, cumulative sequence\
    \ lengths, and strides of the logits tensor. The number of warps is fixed at 8\
    \ to balance performance and resource usage.\n    \nThe test code is:\n\n\nimport\
    \ torch\n\n# Define the test function\ndef test_apply_penalty():\n    # Define\
    \ the dimensions\n    results = {}\n    batch_size = 2\n    seq_len = 10\n   \
    \ vocab_size = 50\n\n    # Create random logits tensor\n    Logits = torch.randn((batch_size,\
    \ vocab_size), dtype=torch.float32, device='cuda').contiguous()\n\n    # Define\
    \ penalties\n    presence_penalty = torch.tensor([0.1, 0.2], dtype=torch.float32,\
    \ device='cuda')\n    freqency_penalty = torch.tensor([0.1, 0.2], dtype=torch.float32,\
    \ device='cuda')\n    repetition_penalty = torch.tensor([1.2, 1.3], dtype=torch.float32,\
    \ device='cuda')\n\n    # Define token ids and counts\n    p_token_ids = torch.randint(0,\
    \ vocab_size, (seq_len,), dtype=torch.int32, device='cuda')\n    p_token_counts\
    \ = torch.randint(1, 5, (seq_len,), dtype=torch.int32, device='cuda')\n\n    #\
    \ Define cumulative sequence lengths\n    p_cumsum_seq_len = torch.tensor([0,\
    \ seq_len], dtype=torch.int32, device='cuda')\n\n    # Maximum length in batch\n\
    \    p_max_len_in_batch = seq_len\n\n    # Call the apply_penalty function for\
    \ the first branch\n    apply_penalty(Logits, presence_penalty, freqency_penalty,\
    \ repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len, p_max_len_in_batch)\n\
    \    results['test_case_1'] = Logits.clone()\n\n    # Modify p_max_len_in_batch\
    \ to test another branch\n    p_max_len_in_batch = 600  # Testing BLOCK set to\
    \ 512\n    apply_penalty(Logits, presence_penalty, freqency_penalty, repetition_penalty,\
    \ p_token_ids, p_token_counts, p_cumsum_seq_len, p_max_len_in_batch)\n    results['test_case_2']\
    \ = Logits.clone()\n\n    # Modify p_max_len_in_batch to test another branch\n\
    \    p_max_len_in_batch = 800  # Testing BLOCK set to 1024\n    apply_penalty(Logits,\
    \ presence_penalty, freqency_penalty, repetition_penalty, p_token_ids, p_token_counts,\
    \ p_cumsum_seq_len, p_max_len_in_batch)\n    results['test_case_3'] = Logits.clone()\n\
    \n    return results\n\n# Run the test and capture results\nresult_gold = test_apply_penalty()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py apply_penalty.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- apply_penalty
