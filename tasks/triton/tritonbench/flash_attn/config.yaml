compile_command:
- python flash_attn.py
correctness_command:
- python flash_attn_perf.py
performance_command:
- tb_eval -f flash_attn.py -o flash_attn_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `flash_attn_triton` function is designed to compute the flash\
    \ attention mechanism efficiently using the Triton library. It accepts input tensors\
    \ `q`, `k`, and `v` representing the query, key, and value matrices with shape\
    \ `[batch_size, heads, sequence_length, dimensions]`. The function handles optional\
    \ causal masking and scales the dot products by `sm_scale`. It calculates the\
    \ output using the `_fwd_kernel`, a specialized Triton kernel.\n\n           \
    \ The Triton kernel `_fwd_kernel` performs operations in block-sized chunks defined\
    \ by `BLOCK_M` and `BLOCK_N`, iterating over the sequence length in these fixed-size\
    \ blocks. For each block, it loads segments of `Q`, `K`, and `V`, computes the\
    \ dot products `qk`, applies softmax scaling with corrections for numerical stability\
    \ using exp2 and logarithmic transformations. If `IS_CAUSAL` is enabled, it applies\
    \ causal masking ensuring future information is not used.\n\n            The maximum\
    \ values are tracked for each block to maintain precision during the softmax computation,\
    \ followed by accumulation of scaled values into an output buffer. Once all blocks\
    \ are processed, the output is normalized and stored in tensor `o`. The intermediate\
    \ maximum and denominator values are also stored for each query's position.\n\n\
    \            Stride parameters for each tensor define memory layout, facilitating\
    \ batched operations. `num_warps` controls the number of Triton warps used, determined\
    \ based on the `Lk` dimension size. The function is specialized for `Lk` sizes\
    \ of 16, 32, 64, or 128, ensuring optimal performance with these typical transformer\
    \ sizes. Each Triton kernel launch is controlled by a 3D grid where dimensions\
    \ correspond to blocks in the sequence length, the batch and head dimensions,\
    \ and a single block in depth, effectively parallelizing the attention computation\
    \ over the input dimensions.\n            \nThe test code is:\n\n\n# Test cases\
    \ for the flash_attn_triton function\ndef test_flash_attn_triton():\n    batch_size\
    \ = 2\n    num_heads = 2\n    seq_len = 128\n    dim = 64\n\n    # Create random\
    \ input tensors\n    q = torch.randn((batch_size, num_heads, seq_len, dim), dtype=torch.float16,\
    \ device='cuda')\n    k = torch.randn((batch_size, num_heads, seq_len, dim), dtype=torch.float16,\
    \ device='cuda')\n    v = torch.randn((batch_size, num_heads, seq_len, dim), dtype=torch.float16,\
    \ device='cuda')\n\n    # Test with causal=True\n    output_causal = flash_attn_triton(q,\
    \ k, v, causal=True, sm_scale=1.0)\n\n    # Test with causal=False\n    output_non_causal\
    \ = flash_attn_triton(q, k, v, causal=False, sm_scale=1.0)\n\n    results = {\n\
    \        \"test_case_1\": output_causal,\n        \"test_case_2\": output_non_causal\n\
    \    }\n\n    return results\n\n# Run the test\nresult_gold = test_flash_attn_triton()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py flash_attn.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- flash_attn
