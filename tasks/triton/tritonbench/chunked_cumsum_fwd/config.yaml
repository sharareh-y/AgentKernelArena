compile_command:
- python chunked_cumsum_fwd.py
correctness_command:
- python chunked_cumsum_fwd_perf.py
performance_command:
- tb_eval -f chunked_cumsum_fwd.py -o chunked_cumsum_fwd_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel '_chunk_cumsum_fwd_kernel' performs a chunked cumulative\
    \ sum on a 3D tensor 'dt' by iterating over batches, chunks, and heads. The data\
    \ pointers are adjusted based on thread identifiers to load appropriate sections\
    \ of the tensor. Optional bias and softplus transformations are applied to the\
    \ data. The cumulative sum is computed for each chunk using the scaling factors\
    \ 'A', and results are stored in 'dA_cumsum'. The output tensor 'dt_out' retains\
    \ the modified values of 'dt' after transformation and clamping. The kernel uses\
    \ block configurations to optimize performance for different problem sizes.\n\
    \        The '_chunk_cumsum_fwd' function initializes output tensors, computes\
    \ grid dimensions, and invokes the Triton kernel on the GPU. It checks shapes\
    \ of inputs and optionally includes biases in the computation. Key configurations\
    \ are determined by input dimensions, and blocks are selected for efficient parallel\
    \ computation.\n        Inputs:\n        - dt: 3D tensor (batch, seqlen, nheads),\
    \ source data.\n        - A: 1D tensor (nheads,), scaling factors for cumulative\
    \ sum.\n        - chunk_size: Integer, size of data chunks.\n        - dt_bias:\
    \ Optional 1D tensor (nheads,), biases for dt.\n        - dt_softplus: Boolean,\
    \ softplus transformation flag.\n        - dt_limit: Tuple, min and max clamping\
    \ values for dt.\n        Outputs:\n        - dA_cumsum: Tensor (batch, nheads,\
    \ nchunks, chunk_size), cumulative sum result.\n        - dt_out: Tensor (batch,\
    \ nheads, nchunks, chunk_size), transformed dt.\n    \nThe test code is:\n\n\n\
    import torch\n\ndef test_chunk_cumsum_fwd():\n    # Test case 1: Without dt_bias\
    \ and without dt_softplus\n    dt = torch.rand(2, 10, 4, device='cuda')  # (batch,\
    \ seqlen, nheads)\n    A = torch.rand(4, device='cuda')  # (nheads,)\n    chunk_size\
    \ = 5\n    dA_cumsum_1, dt_out_1 = _chunk_cumsum_fwd(dt, A, chunk_size)\n\n  \
    \  # Test case 2: With dt_bias and without dt_softplus\n    dt_bias = torch.rand(4,\
    \ device='cuda')  # (nheads,)\n    dA_cumsum_2, dt_out_2 = _chunk_cumsum_fwd(dt,\
    \ A, chunk_size, dt_bias=dt_bias)\n\n    # Test case 3: Without dt_bias and with\
    \ dt_softplus\n    dA_cumsum_3, dt_out_3 = _chunk_cumsum_fwd(dt, A, chunk_size,\
    \ dt_softplus=True)\n\n    # Test case 4: With dt_bias and with dt_softplus\n\
    \    dA_cumsum_4, dt_out_4 = _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=dt_bias,\
    \ dt_softplus=True)\n\n    return {\n        \"test_case_1\": (dA_cumsum_1, dt_out_1),\n\
    \        \"test_case_2\": (dA_cumsum_2, dt_out_2),\n        \"test_case_3\": (dA_cumsum_3,\
    \ dt_out_3),\n        \"test_case_4\": (dA_cumsum_4, dt_out_4),\n    }\n\nresult_gold\
    \ = test_chunk_cumsum_fwd()\n\n\nDon't append test code to the kernel code or\
    \ edit test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ chunked_cumsum_fwd.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunked_cumsum_fwd
