compile_command:
- python relu_triton_kernel.py
correctness_command:
- python relu_triton_kernel_perf.py
performance_command:
- tb_eval -f relu_triton_kernel.py -o relu_triton_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton kernel code defines a function, `relu_kernel`,\
    \ which performs the Rectified Linear Unit (ReLU) activation function on an input\
    \ tensor. The function is compiled using the `triton.jit` decorator, allowing\
    \ it to run on a GPU. The kernel processes the input tensor in parallel using\
    \ multiple threads, each identified by `pid`, the program ID. The block of data\
    \ each thread processes is determined by `block_start` and `offsets`, where `block_size`\
    \ specifies the number of elements each block handles. The ReLU operation is executed\
    \ by the `tl.where` function, setting negative values to zero and retaining positive\
    \ values. The result is conditionally stored back to global memory via `tl.store`,\
    \ only writing elements within the valid range of the input size `N`. The `relu`\
    \ function, which serves as a wrapper, prepares the output tensor, calculates\
    \ the required number of blocks, and launches the Triton kernel on a grid defined\
    \ by these blocks. It ultimately returns the ReLU-activated tensor.\n        \
    \    \nThe test code is:\n\n\nimport torch\n\ndef test_relu():\n    results =\
    \ {}\n    \n    # Test case 1: All negative values\n    input_tensor = torch.tensor([-3.0,\
    \ -1.0, -0.5, -2.0, -5.0], dtype=torch.float32, device='cuda')\n    output_tensor\
    \ = relu(input_tensor)\n    results['test_case_1'] = output_tensor\n\n    # Test\
    \ case 2: All positive values\n    input_tensor = torch.tensor([3.0, 1.0, 0.5,\
    \ 2.0, 5.0], dtype=torch.float32, device='cuda')\n    output_tensor = relu(input_tensor)\n\
    \    results['test_case_2'] = output_tensor\n\n    # Test case 3: Mixed values\n\
    \    input_tensor = torch.tensor([-3.0, -1.0, 0.0, 2.0, 5.0], dtype=torch.float32,\
    \ device='cuda')\n    output_tensor = relu(input_tensor)\n    results['test_case_3']\
    \ = output_tensor\n\n    # Test case 4: Zero values\n    input_tensor = torch.tensor([0.0,\
    \ 0.0, 0.0, 0.0, 0.0], dtype=torch.float32, device='cuda')\n    output_tensor\
    \ = relu(input_tensor)\n    results['test_case_4'] = output_tensor\n\n    return\
    \ results\n\nresult_gold = test_relu()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ relu_triton_kernel.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- relu_triton_kernel
