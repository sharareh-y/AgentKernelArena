compile_command:
- python index_select_bwd.py
correctness_command:
- python index_select_bwd_perf.py
performance_command:
- tb_eval -f index_select_bwd.py -o index_select_bwd_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided code implements a Triton kernel `index_select_cat_bwd_kernel`\
    \ and a Python wrapper function `index_select_cat_bwd` to handle the backward\
    \ operation of index selection followed by concatenation. The function `index_select_cat_bwd`\
    \ is used in a deep learning context where the forward operation involves selecting\
    \ certain rows from a source tensor (potentially using concatenation of results\
    \ from multiple selections) based on the indices provided. The backward operation,\
    \ therefore, needs to compute the gradient of the source tensor by redistributing\
    \ the gradient received from the output (`grad_output`) back into the positions\
    \ specified by `index` within `grad_source`. The Triton kernel processes the data\
    \ in a grid layout, with the axes of the grid defined by `BLOCK_SIZE_INDEX` and\
    \ `BLOCK_SIZE_COL`, ensuring parallel computation. The function also includes\
    \ extensive checks for tensor compatibility, ensuring that the inputs are 2D,\
    \ CUDA-based, and have appropriate matching strides. The grid configuration is\
    \ dynamically determined to cover all indices and columns of the inputs.\n   \
    \ \nThe test code is:\n\n\nimport torch\n\n# Test for index_select_cat_bwd\ndef\
    \ test_index_select_cat_bwd():\n    results = {}\n\n    # Test case 1: Basic test\n\
    \    grad_source = torch.zeros(10, 512, device='cuda')\n    index = torch.tensor([0,\
    \ 2, 4, 6, 8], device='cuda')\n    grad_output = torch.randn(len(index), grad_source.size(1),\
    \ device='cuda')\n    index_select_cat_bwd(grad_source, index, grad_output)\n\
    \    results['test_case_1'] = grad_source.clone()\n\n    # Test case 2: Different\
    \ indices\n    grad_source = torch.zeros(10, 512, device='cuda')\n    index =\
    \ torch.tensor([1, 3, 5, 7, 9], device='cuda')\n    grad_output = torch.randn(len(index),\
    \ grad_source.size(1), device='cuda')\n    index_select_cat_bwd(grad_source, index,\
    \ grad_output)\n    results['test_case_2'] = grad_source.clone()\n\n    # Test\
    \ case 3: All indices the same\n    grad_source = torch.zeros(10, 512, device='cuda')\n\
    \    index = torch.tensor([0, 0, 0, 0, 0], device='cuda')\n    grad_output = torch.randn(len(index),\
    \ grad_source.size(1), device='cuda')\n    index_select_cat_bwd(grad_source, index,\
    \ grad_output)\n    results['test_case_3'] = grad_source.clone()\n\n    # Test\
    \ case 4: Maximum index\n    grad_source = torch.zeros(10, 512, device='cuda')\n\
    \    index = torch.tensor([9, 9, 9, 9, 9], device='cuda')\n    grad_output = torch.randn(len(index),\
    \ grad_source.size(1), device='cuda')\n    index_select_cat_bwd(grad_source, index,\
    \ grad_output)\n    results['test_case_4'] = grad_source.clone()\n\n    return\
    \ results\n\n# Run the tests\nresult_gold = test_index_select_cat_bwd()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py index_select_bwd.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- index_select_bwd
