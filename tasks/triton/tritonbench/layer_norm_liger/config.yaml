compile_command:
- python layer_norm_liger.py
correctness_command:
- python layer_norm_liger_perf.py
performance_command:
- tb_eval -f layer_norm_liger.py -o layer_norm_liger_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The given code implements layer normalization using Triton's just-in-time\
    \ (JIT) compiler, allowing efficient GPU execution. The layer normalization operation\
    \ is split into two phases: forward and backward passes, each implemented with\
    \ its Triton kernel.\n\n    `_layer_norm_forward_kernel` calculates layer normalization\
    \ for each row of the input matrix `X`. It loads input, weight, and bias tensors,\
    \ computes the mean and variance, and stores the results in `Y` after applying\
    \ normalization using `W` and `B`. This kernel is called within `layer_norm_forward`,\
    \ which manages input reshaping, output initialization, and kernel configuration.\n\
    \n    `_layer_norm_backward_kernel` computes gradients with respect to inputs,\
    \ weights, and biases. It processes a block of rows per program instance, computes\
    \ gradients iteratively, and stores results in `DX`, `DW`, and `DB`. The kernel\
    \ considers the number of streaming multiprocessors available (`sm_count`) for\
    \ efficient distribution of workload.\n\n    `layer_norm_forward` initializes\
    \ the forward process by reshaping inputs and calling the forward kernel. It checks\
    \ input dimensions and creates output placeholders for `Y`, `Mean`, and `RSTD`,\
    \ setting up kernel launch parameters like `BLOCK_SIZE` and `num_warps`.\n\n \
    \   `layer_norm_backward` prepares and launches the backward kernel, consolidating\
    \ partial derivatives computed per GPU program into `DW` and `DB` to obtain gradients\
    \ w.r.t weights and biases.\n\n    `LigerLayerNormFunction` integrates these operations\
    \ within a PyTorch custom autograd function. It wraps `layer_norm_forward` and\
    \ `layer_norm_backward` to support end-to-end differentiation. The `forward` method\
    \ executes `layer_norm_forward`, saving inputs and results. The `backward` method\
    \ uses stored values to compute and return gradients via `layer_norm_backward`.\n\
    \n    Relevant details:\n    - `calculate_settings`: Determines appropriate `BLOCK_SIZE`\
    \ and warp count for kernel execution.\n    - `tl.constexpr` arguments: Used to\
    \ pass constants like block size to kernels.\n    - Kernel launch: Parameters\
    \ like strides and dimensions are crucial for indexing and data handling within\
    \ kernels.\n    \nThe test code is:\n\n\nimport torch\n\ndef test_layer_norm():\n\
    \    # Define input parameters\n    n_rows = 128\n    n_cols = 256\n    eps =\
    \ 1e-5\n\n    # Create random input tensors\n    X = torch.randn((n_rows, n_cols),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n    W = torch.randn((n_cols,),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n    B = torch.randn((n_cols,),\
    \ dtype=torch.float32, device='cuda', requires_grad=True)\n\n    # Test case 1:\
    \ Standard forward and backward pass\n    Y = LigerLayerNormFunction.apply(X,\
    \ W, B, eps)\n    dY = torch.randn_like(Y)\n    DX, DW, DB = torch.autograd.grad(Y,\
    \ (X, W, B), grad_outputs=dY)\n\n    result = {\n        \"test_case_1\": {\n\
    \            \"Y_shape\": Y.shape,\n            \"DX_shape\": DX.shape,\n    \
    \        \"DW_shape\": DW.shape,\n            \"DB_shape\": DB.shape,\n      \
    \  }\n    }\n\n    return result\n\nresult_gold = test_layer_norm()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py layer_norm_liger.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- layer_norm_liger
