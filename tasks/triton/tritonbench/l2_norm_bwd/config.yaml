compile_command:
- python l2_norm_bwd.py
correctness_command:
- python l2_norm_bwd_perf.py
performance_command:
- tb_eval -f l2_norm_bwd.py -o l2_norm_bwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel `_l2_norm_bwd_kernel` performs a backward pass operation\
    \ for L2 normalization on a per-row basis. It receives pointers to input `X`,\
    \ output gradient `DY`, and calculates the input gradient `DX`. Each row of the\
    \ input is accessed using the `stride_x_row`. `BLOCK_N` determines the number\
    \ of elements processed per block, set based on maximum allowable fused size and\
    \ next power of 2 of `N`. Within the kernel, it computes the variance of the input\
    \ slice, uses it to compute the reciprocal of the standard deviation (`rstd`),\
    \ and then calculates `dx` using the formula `dy * rstd - tl.sum(dy * x) * (1\
    \ / (var+eps)) * rstd * x`. The result is conditionally stored in `DX` using masks.\
    \ The `_l2_norm_bwd` function orchestrates this process, ensuring input tensors\
    \ `x` and `dy` are properly reshaped and their strides configured for contiguity\
    \ if necessary. If `N` exceeds `BLOCK_N`, an error is raised to prevent excessive\
    \ feature dimensions. Finally, the kernel is launched over `M` rows of the reshaped\
    \ tensors, and the output `dx` is reshaped back to the original input shape.\n\
    \    \nThe test code is:\n\n\nimport torch\n\n# Test the backward L2 normalization\n\
    def test_l2_norm_bwd():\n    results = {}\n    \n    # Test case 1: Default case\n\
    \    x = torch.randn(4, 8, device='cuda', dtype=torch.float32)\n    dy = torch.randn(4,\
    \ 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x, dy)\n    results['test_case_1']\
    \ = dx\n\n    # Test case 2: Different shape\n    x = torch.randn(2, 16, device='cuda',\
    \ dtype=torch.float32)\n    dy = torch.randn(2, 16, device='cuda', dtype=torch.float32)\n\
    \    dx = _l2_norm_bwd(x, dy)\n    results['test_case_2'] = dx\n\n    # Test case\
    \ 3: Larger tensor\n    x = torch.randn(8, 8, device='cuda', dtype=torch.float32)\n\
    \    dy = torch.randn(8, 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x,\
    \ dy)\n    results['test_case_3'] = dx\n\n    # Test case 4: Edge case with small\
    \ tensor\n    x = torch.randn(1, 8, device='cuda', dtype=torch.float32)\n    dy\
    \ = torch.randn(1, 8, device='cuda', dtype=torch.float32)\n    dx = _l2_norm_bwd(x,\
    \ dy)\n    results['test_case_4'] = dx\n\n    return results\n\n# Run the tests\n\
    result_gold = test_l2_norm_bwd()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ l2_norm_bwd.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- l2_norm_bwd
