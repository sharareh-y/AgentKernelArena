compile_command:
- python parallel_retention_attention.py
correctness_command:
- python parallel_retention_attention_perf.py
performance_command:
- tb_eval -f parallel_retention_attention.py -o parallel_retention_attention_output.json
  -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The code utilizes Triton to accelerate parallel attention mechanisms, commonly\
    \ used in transformer models, on GPUs. It introduces `parallel_retention_fwd_kernel`\
    \ and `parallel_retention_bwd_kernel` as the core computational kernels. These\
    \ kernels employ Triton block pointers for optimized data access patterns and\
    \ execute parallel computations over the sequence length and attention heads.\
    \ The forward kernel calculates the attention outputs by performing scaled dot-product\
    \ operations between the query (`q`), key (`k`), and value (`v`) tensors. It incorporates\
    \ decay factors computed from the head index to adjust the attention weights,\
    \ ensuring numerical stability and enhancing performance. The output (`o`) tensor\
    \ is computed through accumulation and rescaling steps involving iterative block-level\
    \ operations. The `ParallelRetentionFunction` class defines custom autograd functions\
    \ for the forward and backward passes using the kernels. The `forward` method\
    \ precomputes constants, including block sizes and scaling factors, and launches\
    \ the forward kernel, while preserving the inputs for gradient computation. In\
    \ contrast, the `backward` method leverages the stored inputs and the `do` (gradient\
    \ of the output) tensor to compute gradients of the inputs (`dq`, `dk`, `dv`).\
    \ It does so by invoking two sub-kernels: `_parallel_retention_bwd_dq` for the\
    \ query gradient and `_parallel_retention_bwd_dkv` for the key and value gradients.\
    \ The implementation carefully manages tensor dimensions and strides, ensuring\
    \ contiguous memory accesses for efficient GPU execution. Overall, the design\
    \ showcases advanced techniques in GPU programming, balancing performance with\
    \ clarity through modular function decomposition.\n    \nThe test code is:\n\n\
    \nimport torch\n\ndef test_parallel_retention():\n    # \u53C2\u6570\u8BBE\u7F6E\
    \n    batch_size = 2\n    n_heads = 4\n    seq_len = 128\n    d_head_qk = 64\n\
    \    d_head_v = 64\n\n    # \u521B\u5EFA\u8F93\u5165\u5F20\u91CF\uFF0C\u5E76\u8BBE\
    \u7F6Erequires_grad=True\u4EE5\u6D4B\u8BD5\u540E\u5411\u4F20\u64AD\n    q = torch.randn(batch_size,\
    \ n_heads, seq_len, d_head_qk, device='cuda', dtype=torch.float32, requires_grad=True)\n\
    \    k = torch.randn(batch_size, n_heads, seq_len, d_head_qk, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n    v = torch.randn(batch_size, n_heads, seq_len, d_head_v,\
    \ device='cuda', dtype=torch.float32, requires_grad=True)\n\n    # \u53C2\u6570\
    \u53D8\u5316: \u9700\u8981\u5206\u522B\u6D4B\u8BD5\u4E0D\u540C\u7684d_head_qk,\
    \ d_head_v\u7B49\n    # 1. Case 1: d_head_qk and d_head_v both set to 64\n   \
    \ output_1 = parallel_retention(q, k, v)  # Result for default 64,64 setting\n\
    \    result_gold_1 = output_1.sum().item()  # Placeholder for result validation\n\
    \n    # 2. Case 2: d_head_qk set to 32, d_head_v set to 128\n    q_2 = torch.randn(batch_size,\
    \ n_heads, seq_len, 32, device='cuda', dtype=torch.float32, requires_grad=True)\n\
    \    v_2 = torch.randn(batch_size, n_heads, seq_len, 128, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n    output_2 = parallel_retention(q_2, k, v_2)  # With\
    \ changed d_head_qk and d_head_v\n    result_gold_2 = output_2.sum().item()  #\
    \ Placeholder for result validation\n\n    # 3. Case 3: d_head_qk set to 128,\
    \ d_head_v set to 64\n    q_3 = torch.randn(batch_size, n_heads, seq_len, 128,\
    \ device='cuda', dtype=torch.float32, requires_grad=True)\n    output_3 = parallel_retention(q_3,\
    \ k, v)  # With d_head_qk = 128\n    result_gold_3 = output_3.sum().item()  #\
    \ Placeholder for result validation\n\n    # 4. Case 4: Test larger batch size\n\
    \    batch_size_2 = 4  # Increase batch size for larger computations\n    q_4\
    \ = torch.randn(batch_size_2, n_heads, seq_len, d_head_qk, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n    output_4 = parallel_retention(q_4, k, v)  # Larger\
    \ batch size\n    result_gold_4 = output_4.sum().item()  # Placeholder for result\
    \ validation\n\n    # Collecting results for all cases\n    test_results = {\n\
    \        \"test_case_1\": result_gold_1,\n        \"test_case_2\": result_gold_2,\n\
    \        \"test_case_3\": result_gold_3,\n        \"test_case_4\": result_gold_4,\n\
    \    }\n    \n    return test_results\n\n\n# \u6267\u884C\u6D4B\u8BD5\u51FD\u6570\
    \u5E76\u83B7\u53D6\u7ED3\u679C\nresult_gold = test_parallel_retention()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py parallel_retention_attention.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- parallel_retention_attention
