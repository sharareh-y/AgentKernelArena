compile_command:
- python sgmv_expand_slice.py
correctness_command:
- python sgmv_expand_slice_perf.py
performance_command:
- tb_eval -f sgmv_expand_slice.py -o sgmv_expand_slice_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided code consists of a Triton kernel, `_sgmv_expand_slice_kernel`,\
    \ and a wrapper function, `_sgmv_expand_slice`. \n            The kernel performs\
    \ a specialized form of matrix multiplication involving sparse generalized matrix-vector\
    \ multiplication (SGMV). \n            It operates on blocks of data defined by\
    \ the parameters BLOCK_M, BLOCK_N, and BLOCK_K. \n            The kernel checks\
    \ for boundary conditions and handles data casting between different precisions.\
    \ \n            It processes sequences of a batch in parallel and uses lora_indices\
    \ to access specific slices of LoRA weights for computations. \n            The\
    \ wrapper function sets up the input parameters, ensuring contiguity and dimension\
    \ correctness, and configures the Triton launch grid to execute the kernel. \n\
    \            It validates tensor shapes and data types, adjusts weights if needed,\
    \ and launches the kernel with a grid configured to cover the input data based\
    \ on the maximum sequence length and batch size.\n            \nThe test code\
    \ is:\n\n\nimport torch\n\n# Define the test function\ndef test_sgmv_expand_slice():\n\
    \    # Test parameters\n    batches = 2\n    max_seq_length = 64\n    token_nums\
    \ = 128\n    slice_size = 32\n    rank = 32\n\n    # Create input tensors\n  \
    \  inputs = torch.randn(token_nums, slice_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    lora_b_weights = torch.randn(1, rank, slice_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    output_tensor = torch.zeros(token_nums, slice_size, dtype=torch.float16,\
    \ device='cuda').contiguous()\n    b_seq_start_loc = torch.tensor([0, 64], dtype=torch.int32,\
    \ device='cuda')\n    seq_len_tensor = torch.tensor([64, 64], dtype=torch.int32,\
    \ device='cuda')\n    lora_indices_tensor = torch.tensor([0, 0], dtype=torch.int32,\
    \ device='cuda')\n\n    # Initialize a dictionary to store test results\n    results\
    \ = {}\n\n    # Test case 1: add_inputs is False\n    _sgmv_expand_slice(\n  \
    \      inputs,\n        lora_b_weights,\n        output_tensor,\n        b_seq_start_loc,\n\
    \        seq_len_tensor,\n        lora_indices_tensor,\n        batches,\n   \
    \     max_seq_length,\n        token_nums,\n        0,  # slice_offset\n     \
    \   slice_size,\n        False  # add_inputs\n    )\n    results[\"test_case_1\"\
    ] = output_tensor\n\n    # Test case 2: add_inputs is True\n    output_tensor\
    \ = torch.zeros(token_nums, slice_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    _sgmv_expand_slice(\n        inputs,\n        lora_b_weights,\n        output_tensor,\n\
    \        b_seq_start_loc,\n        seq_len_tensor,\n        lora_indices_tensor,\n\
    \        batches,\n        max_seq_length,\n        token_nums,\n        0,  #\
    \ slice_offset\n        slice_size,\n        True  # add_inputs\n    )\n    results[\"\
    test_case_2\"] = output_tensor\n\n    # Test case 3: Different slice_offset\n\
    \    output_tensor = torch.zeros(token_nums, slice_size, dtype=torch.float16,\
    \ device='cuda').contiguous()\n    _sgmv_expand_slice(\n        inputs,\n    \
    \    lora_b_weights,\n        output_tensor,\n        b_seq_start_loc,\n     \
    \   seq_len_tensor,\n        lora_indices_tensor,\n        batches,\n        max_seq_length,\n\
    \        token_nums,\n        16,  # slice_offset\n        slice_size,\n     \
    \   False  # add_inputs\n    )\n    results[\"test_case_3\"] = output_tensor\n\
    \n    # Test case 4: Different slice size\n    slice_size = 16\n    rank = 16\n\
    \    inputs = torch.randn(token_nums, slice_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    lora_b_weights = torch.randn(1, rank, slice_size, dtype=torch.float16, device='cuda').contiguous()\n\
    \    output_tensor = torch.zeros(token_nums, slice_size, dtype=torch.float16,\
    \ device='cuda').contiguous()\n    \n    _sgmv_expand_slice(\n        inputs,\n\
    \        lora_b_weights,\n        output_tensor,\n        b_seq_start_loc,\n \
    \       seq_len_tensor,\n        lora_indices_tensor,\n        batches,\n    \
    \    max_seq_length,\n        token_nums,\n        0,  # slice_offset\n      \
    \  slice_size,\n        False  # add_inputs\n    )\n    results[\"test_case_4\"\
    ] = output_tensor\n\n    return results\n\n# Run the test\nresult_gold = test_sgmv_expand_slice()\n\
    \nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py sgmv_expand_slice.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- sgmv_expand_slice
