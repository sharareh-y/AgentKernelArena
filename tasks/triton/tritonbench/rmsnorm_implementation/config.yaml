compile_command:
- python rmsnorm_implementation.py
correctness_command:
- python rmsnorm_implementation_perf.py
performance_command:
- tb_eval -f rmsnorm_implementation.py -o rmsnorm_implementation_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `rmsnorm_triton` is designed to perform RMS normalization\
    \ on a 3D tensor `x` with dimensions `[batch, M, K]`. The input pointers `x_ptr`,\
    \ `rms_w_ptr`, and `out_ptr` correspond to the input tensor, the RMS weights,\
    \ and the output tensor, respectively. The function is parallelized across the\
    \ `batch` and `M` dimensions using Triton's `program_id`. Inside the kernel, the\
    \ RMS value is computed by summing the squares of each element along the `K` dimension,\
    \ normalized by `N_SIZE` (size of `K`), and then adjusted with a small constant\
    \ `eps` to prevent division by zero. The computed RMS is then used to normalize\
    \ the elements, which are subsequently scaled by `rms_weights`. The `rmsnorm_wrapper`\
    \ function facilitates the execution of this kernel by organizing the input, output,\
    \ strides, and block sizes before invoking the kernel with specified grid dimensions\
    \ (`batch, M`). The kernel utilizes a `BLOCK_N_SIZE` to process chunks of the\
    \ `K` dimension at a time, enhancing parallel execution, and uses `num_warps`\
    \ to control the level of parallelism within a Triton block.\n            \nThe\
    \ test code is:\n\n\ndef test_rmsnorm():\n    # Define the input tensor x with\
    \ shape (batch, M, K)\n    batch = 2\n    M = 3\n    K = 4096\n    x = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device=\"cuda\")\n\n    # Define the rms_weights\
    \ tensor with shape (K,)\n    rms_weights = torch.randn((K,), dtype=torch.float16,\
    \ device=\"cuda\")\n\n    # Dictionary to store the results of different test\
    \ cases\n    results = {}\n\n    # Test case 1\n    out1 = rmsnorm_wrapper(x,\
    \ rms_weights)\n    results['test_case_1'] = out1.cpu()\n\n    # Additional test\
    \ cases for branch coverage\n\n    # Test case 2: Different batch size\n    batch\
    \ = 4\n    x = torch.randn((batch, M, K), dtype=torch.float16, device=\"cuda\"\
    )\n    out2 = rmsnorm_wrapper(x, rms_weights)\n    results['test_case_2'] = out2.cpu()\n\
    \n    # Test case 3: Different M size\n    M = 5\n    x = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device=\"cuda\")\n    out3 = rmsnorm_wrapper(x,\
    \ rms_weights)\n    results['test_case_3'] = out3.cpu()\n\n    # Test case 4:\
    \ Larger K size\n    K = 8192\n    rms_weights = torch.randn((K,), dtype=torch.float16,\
    \ device=\"cuda\")\n    x = torch.randn((batch, M, K), dtype=torch.float16, device=\"\
    cuda\")\n    out4 = rmsnorm_wrapper(x, rms_weights)\n    results['test_case_4']\
    \ = out4.cpu()\n\n    return results\n\n# Execute the test function\nresult_gold\
    \ = test_rmsnorm()\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rmsnorm_implementation.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rmsnorm_implementation
