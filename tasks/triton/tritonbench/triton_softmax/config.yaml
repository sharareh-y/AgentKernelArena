compile_command:
- python triton_softmax.py
correctness_command:
- python triton_softmax_perf.py
performance_command:
- tb_eval -f triton_softmax.py -o triton_softmax_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel function 'softmax_kernel' is designed to compute the\
    \ softmax operation for each row of a 2D input tensor, indicated by 'input_ptr'.\
    \ It writes the result to the location pointed by 'output_ptr'. This kernel takes\
    \ several parameters: 'input_row_stride' and 'output_row_stride' which denote\
    \ the memory strides for input and output matrices respectively, 'n_cols' which\
    \ denotes the number of columns in the row being processed, and a compile-time\
    \ constant 'BLOCK_SIZE' which determines the size of the block of memory being\
    \ processed in parallel.\n\n        For each row, identified by 'tl.program_id(axis=0)',\
    \ the kernel computes memory offsets for both input and output using the strides.\
    \ It loads a row of data from global memory into on-chip SRAM using 'tl.load'\
    \ with masking to handle cases where 'n_cols' is less than 'BLOCK_SIZE', filling\
    \ absent values with '-inf'.\n\n        The kernel computes the maximum value\
    \ of the loaded row using 'tl.max' for numerical stability. This maximum value\
    \ is subtracted from each element in the row, followed by exponentiation through\
    \ 'tl.exp' to form the numerator of the softmax calculation.\n\n        The sum\
    \ of the numerators is computed using 'tl.sum' to serve as the denominator for\
    \ normalization. Each numerator is then divided by this denominator to obtain\
    \ the softmax result for the row. The final normalized values are stored back\
    \ to the output tensor using 'tl.store', employing a mask to ensure that only\
    \ valid column entries (less than 'n_cols') are written.\n\n        The function\
    \ 'triton_softmax' acts as a wrapper to set up and execute the Triton kernel.\
    \ It extracts dimensions of the input tensor 'x' to determine 'n_rows' and 'n_cols'.\
    \ An output tensor of the same shape is initialized. The block size for parallel\
    \ processing is chosen as the next power of two greater than or equal to 'n_cols',\
    \ but capped at 1024, to optimize the hardware resources usage. It configures\
    \ the execution grid to cover all rows and launches the kernel to perform the\
    \ operation across the entire input tensor. This setup allows efficient parallel\
    \ execution of the softmax operation across multiple rows.\n        \nThe test\
    \ code is:\n\n\n# Test cases for the triton_softmax function\ndef test_triton_softmax():\n\
    \    results = {}\n    \n    # Test case 1: Simple 2x2 matrix\n    x1 = torch.tensor([[1.0,\
    \ 2.0], [3.0, 4.0]], dtype=torch.float32, device=\"cuda\")\n    output1 = triton_softmax(x1)\n\
    \    results['test_case_1'] = output1\n\n    # Test case 2: 3x3 matrix with negative\
    \ values\n    x2 = torch.tensor([[-1.0, -2.0, -3.0], [0.0, 0.0, 0.0], [1.0, 2.0,\
    \ 3.0]], dtype=torch.float32, device=\"cuda\")\n    output2 = triton_softmax(x2)\n\
    \    results['test_case_2'] = output2\n\n    # Test case 3: 4x4 matrix with larger\
    \ values\n    x3 = torch.tensor([[10.0, 20.0, 30.0, 40.0], [5.0, 15.0, 25.0, 35.0],\
    \ [0.0, 0.0, 0.0, 0.0], [-10.0, -20.0, -30.0, -40.0]], dtype=torch.float32, device=\"\
    cuda\")\n    output3 = triton_softmax(x3)\n    results['test_case_3'] = output3\n\
    \n    # Test case 4: 1x5 matrix (single row)\n    x4 = torch.tensor([[1.0, 2.0,\
    \ 3.0, 4.0, 5.0]], dtype=torch.float32, device=\"cuda\")\n    output4 = triton_softmax(x4)\n\
    \    results['test_case_4'] = output4\n\n    # Test case 5: 5x1 matrix (single\
    \ column)\n    x5 = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]], dtype=torch.float32,\
    \ device=\"cuda\")\n    output5 = triton_softmax(x5)\n    results['test_case_5']\
    \ = output5\n\n    return results\n\nresult_gold = test_triton_softmax()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py triton_softmax.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- triton_softmax
