compile_command:
- python triton_linear_activation.py
correctness_command:
- python triton_linear_activation_perf.py
performance_command:
- tb_eval -f triton_linear_activation.py -o triton_linear_activation_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This code defines a Triton-based implementation of a linear layer\
    \ with optional activation functions. The implementation starts with several auxiliary\
    \ functions for different activations, such as tanh, ReLU, GELU, and fast GELU.\
    \ The core computation is done in the kernel_fma function, which is a Triton kernel.\
    \ This function takes in matrix pointers A, B, and C, along with optional bias\
    \ and activation inputs, and computes the matrix multiplication with optional\
    \ additions and activations. Key parameters like matrix dimensions, strides, and\
    \ meta-parameters are passed to control the operation. This kernel is designed\
    \ to work efficiently on GPUs, employing Triton's autotuning and heuristics to\
    \ choose optimal configurations based on the inputs' dimensions and characteristics.\
    \ The LinearLayer class is a custom PyTorch Function that sets up and calls the\
    \ kernel_fma, managing inputs and outputs between PyTorch tensors and Triton.\
    \ It ensures the input dimensions match and applies the chosen activation function.\
    \ The linear_layer function is a user-facing wrapper for applying the LinearLayer\
    \ function. It handles activation options and the optional saving of pre-activation\
    \ inputs for backpropagation.\n            \nThe test code is:\n\n\ndef test_linear_layer():\n\
    \    # Test case 1: No bias, no activation, no act_inputs\n    x = torch.randn(64,\
    \ 128, device='cuda', dtype=torch.float16)\n    weight = torch.randn(128, 128,\
    \ device='cuda', dtype=torch.float16)\n    output1 = linear_layer(x, weight, None)\n\
    \n    # Test case 2: With bias, no activation, no act_inputs\n    bias = torch.randn(128,\
    \ device='cuda', dtype=torch.float16)\n    output2 = linear_layer(x, weight, bias)\n\
    \n    # Test case 3: With bias, with activation (ReLU), no act_inputs\n    output3\
    \ = linear_layer(x, weight, bias, activation=\"relu\")\n\n    # Test case 4: With\
    \ bias, with activation (GELU), with act_inputs\n    act_inputs = torch.empty_like(output3)\n\
    \    output4 = linear_layer(x, weight, bias, activation=\"gelu\", act_inputs=act_inputs)\n\
    \n    # Test case 5: With bias, with activation (tanh), with act_inputs\n    output5\
    \ = linear_layer(x, weight, bias, activation=\"tanh\", act_inputs=act_inputs)\n\
    \n    return {\n        \"test_case_1\": output1,\n        \"test_case_2\": output2,\n\
    \        \"test_case_3\": output3,\n        \"test_case_4\": output4,\n      \
    \  \"test_case_5\": output5,\n    }\n\n# Run the test cases\nresult_gold = test_linear_layer()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py triton_linear_activation.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- triton_linear_activation
