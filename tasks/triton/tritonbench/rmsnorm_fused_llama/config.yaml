compile_command:
- python rmsnorm_fused_llama.py
correctness_command:
- python rmsnorm_fused_llama_perf.py
performance_command:
- tb_eval -f rmsnorm_fused_llama.py -o rmsnorm_fused_llama_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The 'rmsnorm_forward' function performs Root Mean Square (RMS) normalization\
    \ on an input tensor using a custom Triton kernel, '_rms_norm_fwd_fused'. The\
    \ inputs are 'x', a PyTorch tensor with arbitrary dimensions, and 'weight', a\
    \ tensor of the same size as the last dimension of 'x' for element-wise multiplication\
    \ after normalization. It outputs 'y', which is a tensor of the same shape as\
    \ 'x'. Inside the Triton kernel, '_rms_norm_fwd_fused', the function is mapped\
    \ to each row of the input 'X'. It calculates the variance by loading input elements\
    \ in blocks of size 'BLOCK_SIZE', summing their squares, and averaging over the\
    \ number of columns 'N'. The reciprocal standard deviation (rstd) is computed\
    \ to normalize each element of the input tensor. The normalized values are then\
    \ multiplied by the corresponding elements of 'weight' and stored in the output\
    \ tensor 'Y'. The operation is conditioned to handle edge cases when the number\
    \ of columns is not a perfect multiple of 'BLOCK_SIZE' using masking. Additionally,\
    \ the 'rmsnorm_forward' function includes heuristics for deciding the number of\
    \ Triton warps, managing feature size constraints, and arranging the data for\
    \ efficient kernel execution.\n    \nThe test code is:\n\n\nimport torch\n\n#\
    \ Test function for rmsnorm_forward\ndef test_rmsnorm_forward():\n    results\
    \ = {}\n    \n    # Test case 1: Small input tensor\n    x1 = torch.randn(2, 64,\
    \ dtype=torch.float16).cuda()\n    weight1 = torch.randn(64, dtype=torch.float16).cuda()\n\
    \    eps1 = 1e-5\n    y1 = rmsnorm_forward(x1, weight1, eps1)\n    results['test_case_1']\
    \ = y1\n    \n    # Test case 2: Larger input tensor\n    x2 = torch.randn(4,\
    \ 128, dtype=torch.float16).cuda()\n    weight2 = torch.randn(128, dtype=torch.float16).cuda()\n\
    \    eps2 = 1e-5\n    y2 = rmsnorm_forward(x2, weight2, eps2)\n    results['test_case_2']\
    \ = y2\n    \n    # Test case 3: Edge case with maximum supported feature dimension\n\
    \    x3 = torch.randn(1, 8192, dtype=torch.float16).cuda()  # 8192 * 2 bytes =\
    \ 16384 bytes < 64KB\n    weight3 = torch.randn(8192, dtype=torch.float16).cuda()\n\
    \    eps3 = 1e-5\n    y3 = rmsnorm_forward(x3, weight3, eps3)\n    results['test_case_3']\
    \ = y3\n\n    # Test case 4: Edge case with minimum supported feature dimension\n\
    \    x4 = torch.randn(1, 1, dtype=torch.float16).cuda()\n    weight4 = torch.randn(1,\
    \ dtype=torch.float16).cuda()\n    eps4 = 1e-5\n    y4 = rmsnorm_forward(x4, weight4,\
    \ eps4)\n    results['test_case_4'] = y4\n\n    return results\n\n# Run the test\
    \ function\nresult_gold = test_rmsnorm_forward()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rmsnorm_fused_llama.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rmsnorm_fused_llama
