compile_command:
- python rotary_transform.py
correctness_command:
- python rotary_transform_perf.py
performance_command:
- tb_eval -f rotary_transform.py -o rotary_transform_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The `rotary_kernel` function is a Triton kernel that performs rotary\
    \ position encoding on a tensor `X` using precomputed cosine (`COS`) and sine\
    \ (`SIN`) matrices. It modifies or populates the output tensor `OUT` with the\
    \ transformed data. The kernel accommodates both fixed and variable sequence lengths,\
    \ controlled by the presence of `CU_SEQLENS`. The kernel handles interleaved and\
    \ non-interleaved formats and allows for in-place transformations and conjugate\
    \ computations if specified.\n\n        The kernel operates in a three-dimensional\
    \ grid, processing batches (`pid_batch`), heads (`pid_head`), and sequences (`pid_m`).\
    \ It calculates transformations by loading blocks of data and applying rotary\
    \ transformations based on cosine and sine values. The key operations are tailored\
    \ based on whether the data is interleaved or not, with conditional handling for\
    \ conjugation using `CONJUGATE`.\n\n        The `apply_rotary` function acts as\
    \ a high-level interface to the Triton kernel. It accepts the input tensor `x`,\
    \ cosine and sine matrices, sequence length offsets, and optional cumulative sequence\
    \ lengths (`cu_seqlens`). The function determines the execution grid and block\
    \ sizes, aligning them with the input data shape and configuration. It initializes\
    \ an output tensor, copying non-rotary parts of `x` if required. The function\
    \ ensures that the kernel is called with appropriate arguments, matching the shape\
    \ and type expectations set within the kernel logic. This design allows for efficient\
    \ rotary transformations in transformer architectures.\n    \nThe test code is:\n\
    \n\nimport torch\n\ndef test_apply_rotary():\n    results = {}\n    \n    # Test\
    \ case 1: Basic test with fixed sequence length and no interleaving\n    batch,\
    \ seqlen, nheads, headdim = 2, 128, 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch,\
    \ seqlen, nheads, headdim, device='cuda')\n    cos = torch.randn(seqlen, rotary_dim\
    \ // 2, device='cuda')\n    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n\
    \    output = apply_rotary(x, cos, sin)\n    results['test_case_1'] = output.shape\n\
    \n    # Test case 2: Variable length sequences with interleaving\n    total_seqlen,\
    \ nheads, headdim = 256, 4, 64\n    batch = 3\n    cu_seqlens = torch.tensor([0,\
    \ 100, 200, 256], device='cuda')\n    max_seqlen = 128\n    rotary_dim = 32\n\
    \    x = torch.randn(total_seqlen, nheads, headdim, device='cuda')\n    cos =\
    \ torch.randn(max_seqlen, rotary_dim // 2, device='cuda')\n    sin = torch.randn(max_seqlen,\
    \ rotary_dim // 2, device='cuda')\n    output = apply_rotary(x, cos, sin, cu_seqlens=cu_seqlens,\
    \ max_seqlen=max_seqlen, interleaved=True)\n    results['test_case_2'] = output.shape\n\
    \n    # Test case 3: Conjugate flag enabled\n    batch, seqlen, nheads, headdim\
    \ = 2, 128, 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch, seqlen, nheads,\
    \ headdim, device='cuda')\n    cos = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n\
    \    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x,\
    \ cos, sin, conjugate=True)\n    results['test_case_3'] = output.shape\n\n   \
    \ # Test case 4: Inplace operation\n    batch, seqlen, nheads, headdim = 2, 128,\
    \ 4, 64\n    rotary_dim = 32\n    x = torch.randn(batch, seqlen, nheads, headdim,\
    \ device='cuda')\n    cos = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n\
    \    sin = torch.randn(seqlen, rotary_dim // 2, device='cuda')\n    output = apply_rotary(x,\
    \ cos, sin, inplace=True)\n    results['test_case_4'] = output.shape\n\n    return\
    \ results\n\nresult_gold = test_apply_rotary()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rotary_transform.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rotary_transform
