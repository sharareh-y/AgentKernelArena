compile_command:
- python rowwise_quantization_triton.py
correctness_command:
- python rowwise_quantization_triton_perf.py
performance_command:
- tb_eval -f rowwise_quantization_triton.py -o rowwise_quantization_triton_output.json
  -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The _quantize_rowwise kernel in Triton is designed to perform row-wise\
    \ quantization on a 2D input tensor, which is provided as a CUDA tensor. The kernel\
    \ is executed per row, with each row processed independently by leveraging Triton's\
    \ parallel computing capabilities. It takes several parameters: x_ptr (pointer\
    \ to the input tensor), output_ptr (pointer to the output tensor where quantized\
    \ results are stored), output_maxs (pointer to an array where max values per row\
    \ are stored), and n_elements, which is the total number of elements in the output\
    \ tensor. Two compile-time constants, BLOCK_SIZE and P2, define the block size\
    \ for processing and the power of 2 ceiling of the row size, respectively. The\
    \ kernel computes the absolute values of elements within a row, finds the maximum\
    \ value among them, and uses this max value to scale each element such that it\
    \ fits into the int8 range via the function tl.extra.cuda.libdevice.llrint. The\
    \ quantized values and max values are then stored in their respective output locations.\
    \ The quantize_rowwise function acts as a wrapper to set up the necessary data\
    \ structures, ensure the input is a CUDA tensor, and launch the kernel with appropriate\
    \ grid size, which corresponds to the number of rows. It returns the quantized\
    \ output tensor and a tensor containing the max values for each row.\n    \nThe\
    \ test code is:\n\n\ndef test_quantize_rowwise():\n    results = {}\n\n    # Test\
    \ case 1: Small 2D tensor\n    x1 = torch.tensor([[1.0, 2.0, 3.0], [-1.0, -2.0,\
    \ -3.0]], device='cuda')\n    output1, output_maxs1 = quantize_rowwise(x1)\n \
    \   results['test_case_1'] = (output1, output_maxs1)\n\n    # # Test case 2: Larger\
    \ 2D tensor\n    # x2 = torch.randn(4, 8, device='cuda')\n    # output2, output_maxs2\
    \ = quantize_rowwise(x2)\n    # results['test_case_2'] = (output2, output_maxs2)\n\
    \n    # Test case 3: Tensor with zeros\n    x3 = torch.zeros(2, 5, device='cuda')\n\
    \    output3, output_maxs3 = quantize_rowwise(x3)\n    results['test_case_3']\
    \ = (output3, output_maxs3)\n\n    return results\n\n# Run the test function\n\
    result_gold = test_quantize_rowwise()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rowwise_quantization_triton.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rowwise_quantization_triton
