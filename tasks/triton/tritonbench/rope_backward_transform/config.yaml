compile_command:
- python rope_backward_transform.py
correctness_command:
- python rope_backward_transform_perf.py
performance_command:
- tb_eval -f rope_backward_transform.py -o rope_backward_transform_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_triton_rope` kernel is a Triton-annotated function (`@triton.jit`)\
    \ designed to apply rotary position embeddings to query (`q_ptr`) and key (`k_ptr`)\
    \ matrices by using precomputed cosine and sine arrays (`cos`, `sin`). The kernel\
    \ uses a unique identifier `pid` for each program instance, indexing into matrices\
    \ for batch processing. The embeddings split the head dimensions into two halves,\
    \ applying a rotation formula to each. The `cos` and `sin` arrays determine the\
    \ phase shift for the embedding. Two configurations are supported: forward and\
    \ backward, determined by `BACKWARD_PASS`. During the forward pass, the rotation\
    \ uses standard trigonometric relationships, while the backward pass applies the\
    \ inverse rotation. The `rope_backward` function is a wrapper that ensures gradients\
    \ (`dq`, `dk`) are processed correctly by transposing and configuring the kernel\
    \ call. Inputs are padded to power-of-two dimensions to align with efficient Triton\
    \ memory access patterns. This function primarily sets up the operation and handles\
    \ the memory alignment, then triggers the kernel for execution.\n            \n\
    The test code is:\n\n\nimport torch\n\ndef test_rope_backward():\n    # Define\
    \ the test parameters\n    batch_size = 2\n    seq_len = 4\n    n_q_head = 8\n\
    \    n_kv_head = 8\n    head_dim = 16\n\n    # Create random gradient tensors\
    \ for backward test\n    dq = torch.randn(batch_size, n_q_head, seq_len, head_dim,\
    \ dtype=torch.float32, device='cuda')\n    dk = torch.randn(batch_size, n_kv_head,\
    \ seq_len, head_dim, dtype=torch.float32, device='cuda')\n    cos = torch.randn(seq_len,\
    \ head_dim // 2, dtype=torch.float32, device='cuda')\n    sin = torch.randn(seq_len,\
    \ head_dim // 2, dtype=torch.float32, device='cuda')\n\n    # Test the backward\
    \ function for the first branch\n    dq_out_1, dk_out_1 = rope_backward(dq, dk,\
    \ cos, sin)\n\n    # Test the backward function for the second branch\n    dq_out_2,\
    \ dk_out_2 = rope_backward(dq, dk, cos, sin)\n\n    # Test the backward function\
    \ for the third branch\n    dq_out_3, dk_out_3 = rope_backward(dq, dk, cos, sin)\n\
    \n    # Test the backward function for the fourth branch\n    dq_out_4, dk_out_4\
    \ = rope_backward(dq, dk, cos, sin)\n\n    results = {\n        \"test_case_1\"\
    : (dq_out_1, dk_out_1),\n        \"test_case_2\": (dq_out_2, dk_out_2),\n    \
    \    \"test_case_3\": (dq_out_3, dk_out_3),\n        \"test_case_4\": (dq_out_4,\
    \ dk_out_4),\n    }\n    return results\n\nresult_gold = test_rope_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py rope_backward_transform.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rope_backward_transform
