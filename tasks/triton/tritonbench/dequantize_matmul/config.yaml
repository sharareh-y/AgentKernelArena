compile_command:
- python dequantize_matmul.py
correctness_command:
- python dequantize_matmul_perf.py
performance_command:
- tb_eval -f dequantize_matmul.py -o dequantize_matmul_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The code includes a Triton kernel called 'dequantize_kernel', which takes\
    \ input int8 matrix 'b', scale factors 'b_scale', and outputs a float matrix 'fpb'.\
    \ It uses block indices to load segments of 'b' and 'b_scale', multiplies these\
    \ to dequantize, and stores the result in 'fpb'. Parameters 'K' and 'N' define\
    \ matrix dimensions, while strides manage memory layout. The function 'matmul_dequantize_int8'\
    \ ensures matrix dimension compatibility and performs the dequantization using\
    \ 'dequantize_kernel', followed by a matrix multiplication with input matrix 'a'\
    \ using PyTorch's 'torch.mm'. The result is stored in matrix 'c'. The kernel utilizes\
    \ Triton's '@autotune' to optimize performance with different configurations specified\
    \ by 'BLOCK_SIZE_N' and 'BLOCK_SIZE_K'.\n    \nThe test code is:\n\n\nimport torch\n\
    \n# Define the test function\ndef test_matmul_dequantize_int8():\n    # Define\
    \ the dimensions\n    M, K, N = 64, 128, 256  # Example dimensions\n\n    # Create\
    \ input tensors\n    a = torch.randn((M, K), dtype=torch.float32, device='cuda')\
    \  # Matrix A\n    b = torch.randint(-128, 127, (K, N), dtype=torch.int8, device='cuda')\
    \  # Matrix B (int8)\n    b_scale = torch.rand((N,), dtype=torch.float32, device='cuda')\
    \  # Scale factors for B\n\n    # Create different configurations to test all\
    \ branches\n    test_cases = {}\n\n    for config in [\n        {'BLOCK_SIZE_N':\
    \ 128, 'BLOCK_SIZE_K': 128, 'num_stages': 3, 'num_warps': 4},\n        {'BLOCK_SIZE_N':\
    \ 64, 'BLOCK_SIZE_K': 256, 'num_stages': 3, 'num_warps': 8},\n        {'BLOCK_SIZE_N':\
    \ 32, 'BLOCK_SIZE_K': 256, 'num_stages': 4, 'num_warps': 4},\n        {'BLOCK_SIZE_N':\
    \ 256, 'BLOCK_SIZE_K': 64, 'num_stages': 3, 'num_warps': 8},\n    ]:\n       \
    \ # Override the config\n        grid = lambda META: (\n            triton.cdiv(K,\
    \ config['BLOCK_SIZE_K']), triton.cdiv(N, config['BLOCK_SIZE_N']),\n        )\n\
    \n        # Run the kernel with the current configuration\n        fp_b = torch.empty((K,\
    \ N), device=a.device, dtype=a.dtype)\n        dequantize_kernel[grid](\n    \
    \        b, b_scale, fp_b,\n            K, N,\n            b.stride(0), b.stride(1),\n\
    \            fp_b.stride(0), fp_b.stride(1)\n        )\n        result = torch.mm(a,\
    \ fp_b)\n\n        # Store the result in the test_cases dictionary\n        test_cases[f'test_case_{config[\"\
    BLOCK_SIZE_N\"]}_{config[\"BLOCK_SIZE_K\"]}'] = result\n\n    return test_cases\n\
    \n# Execute the test and store the results\nresult_gold = test_matmul_dequantize_int8()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py dequantize_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- dequantize_matmul
