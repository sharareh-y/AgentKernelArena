compile_command:
- python fused_rotary_embedding.py
correctness_command:
- python fused_rotary_embedding_perf.py
performance_command:
- tb_eval -f fused_rotary_embedding.py -o fused_rotary_embedding_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel function `decoding_fused_rotary_embedding_kernel`\
    \ applies rotary embedding transformation to the input tensors `q`, `k`, and `v`.\
    \ This is achieved by computing sine and cosine rotations separately for halves\
    \ of the head dimension. The result is stored back into the original query tensor\
    \ `q`. Additionally, it selectively updates `k_cache` and `v_cache` tensors, which\
    \ store past key/value embeddings, based on the `KV_GROUP_NUM` and layout configurations.\n\
    \n            The kernel uses tensor slicing based on current program IDs to target\
    \ specific heads and tokens within the tensors. The core of the computation involves\
    \ two halves of the head dimension where sine and cosine transformations are applied\
    \ to achieve rotary embeddings, which rotates the input embedding vectors to incorporate\
    \ positional information.\n\n            The function `decoding_fused_rotary_embedding`\
    \ is responsible for setting up kernel execution, including determining the grid\
    \ size which depends on the number of heads and tokens. It takes in additional\
    \ parameters `block_tables` and `kv_lengths` for managing cache layouts. It supports\
    \ flexible cache layouts controlled by `use_new_kcache_layout` and dynamically\
    \ computes strides for tensor access. The function chooses the number of warps\
    \ based on head dimension to balance performance and resource usage.\n\n     \
    \       Parameters for the kernel include various strides and offsets, indicating\
    \ how to traverse and manipulate the memory layout efficiently. The grid defined\
    \ by `(q_head_num, q_total_tokens)` enables parallel processing of each head and\
    \ token.\n\n            Together, these components facilitate efficient decoding\
    \ in transformer models, enhancing operations like attention by embedding positional\
    \ information directly in the input embeddings.\n            \nThe test code is:\n\
    \n\ndef test_decoding_fused_rotary_embedding():\n    # \u5B9A\u4E49\u6D4B\u8BD5\
    \u53C2\u6570\n    total_tokens = 16       # \u603B token \u6570\n    q_head_num\
    \ = 8          # Query \u7684\u5934\u6570\u91CF\n    kv_head_num = 4         #\
    \ Key/Value \u7684\u5934\u6570\u91CF\n    head_dim = 64           # \u6BCF\u4E2A\
    \u5934\u7684\u7EF4\u5EA6\n    max_position_len = 128  # \u6700\u5927\u4F4D\u7F6E\
    \u957F\u5EA6\n    block_size = 4          # \u5757\u5927\u5C0F\n    num_blocks\
    \ = 4          # Key/Value cache \u5757\u6570\u91CF\n    batch_size = 2      \
    \    # \u6279\u5927\u5C0F\n\n    # \u521D\u59CB\u5316\u8F93\u5165\u5F20\u91CF\n\
    \    q = torch.randn((total_tokens, q_head_num, head_dim), dtype=torch.float32,\
    \ device='cuda')  # Query\n    k = torch.randn((total_tokens, kv_head_num, head_dim),\
    \ dtype=torch.float32, device='cuda')  # Key\n    v = torch.randn((total_tokens,\
    \ kv_head_num, head_dim), dtype=torch.float32, device='cuda')  # Value\n    cos\
    \ = torch.randn((max_position_len, head_dim), dtype=torch.float32, device='cuda')\
    \  # Cosine\n    sin = torch.randn((max_position_len, head_dim), dtype=torch.float32,\
    \ device='cuda')  # Sine\n\n    # \u521D\u59CB\u5316 Key/Value \u7F13\u5B58\u548C\
    \u8F85\u52A9\u5F20\u91CF\n    k_cache = torch.zeros((num_blocks, kv_head_num,\
    \ block_size, head_dim), dtype=torch.float32, device='cuda')\n    v_cache = torch.zeros((num_blocks,\
    \ kv_head_num, block_size, head_dim), dtype=torch.float32, device='cuda')\n  \
    \  block_tables = torch.randint(0, num_blocks, (batch_size, num_blocks), dtype=torch.int32,\
    \ device='cuda')\n    kv_lengths = torch.randint(1, total_tokens, (batch_size,),\
    \ dtype=torch.int32, device='cuda')\n\n    results = {}\n\n    # \u6D4B\u8BD5\u9ED8\
    \u8BA4 k_cache \u5E03\u5C40\n    decoding_fused_rotary_embedding(\n        q=q,\n\
    \        k=k,\n        v=v,\n        cos=cos,\n        sin=sin,\n        k_cache=k_cache,\n\
    \        v_cache=v_cache,\n        block_tables=block_tables,\n        kv_lengths=kv_lengths,\n\
    \        use_new_kcache_layout=False,\n    )\n    results['test_case_1'] = {\n\
    \        'q_shape': q.shape,\n        'k_cache_shape': k_cache.shape,\n      \
    \  'v_cache_shape': v_cache.shape\n    }\n\n    # \u6D4B\u8BD5\u65B0\u7684 k_cache\
    \ \u5E03\u5C40\n    x = 16  # \u5206\u5272\u56E0\u5B50\n    k_cache = torch.zeros((num_blocks,\
    \ kv_head_num, head_dim // x, block_size, x), dtype=torch.float32, device='cuda')\n\
    \    v_cache = torch.zeros((num_blocks, kv_head_num, block_size, head_dim), dtype=torch.float32,\
    \ device='cuda')\n\n    # \u6D4B\u8BD5\u65B0\u7684 k_cache \u5E03\u5C40\n    decoding_fused_rotary_embedding(\n\
    \        q=q,\n        k=k,\n        v=v,\n        cos=cos,\n        sin=sin,\n\
    \        k_cache=k_cache,\n        v_cache=v_cache,\n        block_tables=block_tables,\n\
    \        kv_lengths=kv_lengths,\n        use_new_kcache_layout=True,\n    )\n\
    \    results['test_case_2'] = {\n        'q_shape': q.shape,\n        'k_cache_shape':\
    \ k_cache.shape,\n        'v_cache_shape': v_cache.shape\n    }\n\n    return\
    \ results\n\nresult_gold = test_decoding_fused_rotary_embedding()\n\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fused_rotary_embedding.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_rotary_embedding
