compile_command:
- python chunk_gla_simple.py
correctness_command:
- python chunk_gla_simple_perf.py
performance_command:
- tb_eval -f chunk_gla_simple.py -o chunk_gla_simple_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `chunk_simple_gla_fwd_kernel_o` function, decorated with `@triton.autotune`,\
    \ implements a Triton kernel for efficient computation of a forward pass in a\
    \ chunked matrix-multiplication setting. It receives tensors `q`, `k`, `v`, `h`,\
    \ and `g`, alongside tensor strides (`s_k_h`, `s_k_t`, etc.), scaling factor `scale`,\
    \ and chunk dimensions (`BT`, `BK`, `BV`). The kernel operates using Triton's\
    \ parallel execution, where indices are computed using `tl.program_id` for efficient\
    \ matrix partitioning. Block pointers (e.g., `p_q`, `p_k`) load sub-blocks of\
    \ `q`, `k`, `h` into registers. It computes partial outputs `b_o` and `b_s` using\
    \ dot products, adjusts them with exponentials, and conditions based on the mask\
    \ `m_s`. The result is stored in the output tensor `o` using `tl.store`. The `chunk_fwd_o_fn`\
    \ function handles higher-level orchestration, preparing grid dimensions, calculating\
    \ chunk sizes (`BK`, `BV`), and calling the kernel with the pre-computed grid\
    \ and problem parameters.\n            \nThe test code is:\n\n\nimport torch\n\
    \n# Define the test function for the forward kernel\ndef test_chunk_fwd_o_fn():\n\
    \    B, H, T, K, V = 2, 4, 128, 64, 64  # Example dimensions\n    BT = 32  # Block\
    \ size for T\n    scale = 0.1  # Example scale factor\n\n    # Create random input\
    \ tensors\n    q = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n\
    \    k = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n    v =\
    \ torch.randn(B, H, T, V, dtype=torch.float32, device='cuda')\n    h = torch.randn(B,\
    \ H, K, V, dtype=torch.float32, device='cuda')\n    g = torch.randn(B, H, T, dtype=torch.float32,\
    \ device='cuda')\n\n    # Dictionary to store results\n    results = {}\n\n  \
    \  # Test case 1\n    o1 = chunk_fwd_o_fn(h, q, k, v, g, BT, scale)\n    results['test_case_1']\
    \ = o1\n\n    # Test case 2: Different BT\n    BT = 64\n    o2 = chunk_fwd_o_fn(h,\
    \ q, k, v, g, BT, scale)\n    results['test_case_2'] = o2\n\n    # Test case 3:\
    \ Different scale\n    scale = 0.2\n    o3 = chunk_fwd_o_fn(h, q, k, v, g, BT,\
    \ scale)\n    results['test_case_3'] = o3\n\n    # Test case 4: Different dimensions\n\
    \    B, H, T, K, V = 1, 2, 64, 32, 32\n    q = torch.randn(B, H, T, K, dtype=torch.float32,\
    \ device='cuda')\n    k = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n\
    \    v = torch.randn(B, H, T, V, dtype=torch.float32, device='cuda')\n    h =\
    \ torch.randn(B, H, K, V, dtype=torch.float32, device='cuda')\n    g = torch.randn(B,\
    \ H, T, dtype=torch.float32, device='cuda')\n    o4 = chunk_fwd_o_fn(h, q, k,\
    \ v, g, BT, scale)\n    results['test_case_4'] = o4\n\n    return results\n\n\
    # Execute the test function\nresult_gold = test_chunk_fwd_o_fn()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py chunk_gla_simple.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_gla_simple
