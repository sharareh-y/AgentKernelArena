compile_command:
- python rope_embedding.py
correctness_command:
- python rope_embedding_perf.py
performance_command:
- tb_eval -f rope_embedding.py -o rope_embedding_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    This Triton code provides an implementation of the RoPE (Rotary Position Embeddings)\
    \ using a combination of Python and a Triton JIT-compiled kernel. It is intended\
    \ to perform this computation efficiently on a GPU. The key components include:\n\
    \n1. `calculate_settings(n)` function:\n   - Purpose: Computes optimal block size\
    \ and number of warps for executing the Triton kernel based on the input dimension\
    \ `n`.\n   - It calculates the next power of two for `n` to determine `BLOCK_SIZE`\
    \ and decides the appropriate number of `num_warps` based on this block size.\
    \ It raises a runtime error if the block size exceeds the maximum allowed size\
    \ `MAX_FUSED_SIZE`.\n\n2. `_rope_embedding` Triton kernel:\n   - Signature: `_rope_embedding(Q,\
    \ Q_row_stride, cos, cos_row_stride, sin, sin_row_stride, seqlen, head_dim, n_heads,\
    \ BACKWARD_PASS, BLOCK_SIZE, ROPE_GROUP_SIZE)`\n   - Role: Executes the computation\
    \ of the RoPE embeddings. This involves multiplying the input `Q` with `cos` and\
    \ `sin` matrices, applying a rotation to half of `Q`, and adjusting based on the\
    \ `BACKWARD_PASS` flag.\n   - The kernel leverages the Triton language to parallelize\
    \ computations across rows and groups of heads. It uses `tl.load` and `tl.store`\
    \ for memory operations with specified masks to ensure only valid operations are\
    \ performed.\n\n3. `_rope_embedding_forward_impl(Q, cos, sin)`:\n   - Functionality:\
    \ Handles the forward pass for RoPE embedding. This involves preparing data, calculating\
    \ necessary settings using `calculate_settings`, and launching the Triton kernel\
    \ `_rope_embedding`.\n   - Prepares the data by reshaping and transposing `Q`\
    \ for compatibility with the kernel's expectations. Determines the number of groups\
    \ (`n_groups`) by dividing the number of heads by `ROPE_GROUP_SIZE`.\n\n4. `_rope_embedding_backward_impl(dY,\
    \ cos, sin, n_groups, BLOCK_SIZE, num_warps)`:\n   - Purpose: Facilitates the\
    \ backward pass (gradient computation) for the RoPE operation, useful in training\
    \ scenarios.\n   - Similarly reshapes and transposes `dY` to call the Triton kernel\
    \ with appropriate parameters, setting `BACKWARD_PASS` to `True` to reverse the\
    \ transformation logic.\n\nCommon variables:\n- `Q`: The input tensor for the\
    \ forward pass.\n- `cos`, `sin`: Tensors holding the cosine and sine values used\
    \ in the computation.\n- `seqlen`, `head_dim`, `n_heads`: Dimensions describing\
    \ sequence length, head dimension, and number of attention heads.\n- `BLOCK_SIZE`,\
    \ `num_warps`: Parameters determined through heuristics to optimize GPU execution.\n\
    \nThe test code is:\n\n\ndef test_rope_embedding_forward():\n    # \u6D4B\u8BD5\
    _rope_embedding_forward_impl\u7684\u524D\u5411\u4F20\u64AD\n    batch, seq_len,\
    \ n_heads, head_dim = 2, 16, 8, 64\n    Q = torch.randn(batch, seq_len, n_heads,\
    \ head_dim, device='cuda')\n    cos = torch.randn(seq_len, head_dim//2, device='cuda')\n\
    \    sin = torch.randn(seq_len, head_dim//2, device='cuda')\n\n    # \u6B63\u5411\
    \u4F20\u64AD\n    Q_out, cos_out, sin_out, n_groups, BLOCK_SIZE, num_warps = _rope_embedding_forward_impl(Q,\
    \ cos, sin)\n\n    # \u53CD\u5411\u4F20\u64AD\n    dY = torch.randn(batch, seq_len,\
    \ n_heads, head_dim, device='cuda')\n    dY_out = _rope_embedding_backward_impl(dY,\
    \ cos, sin, n_groups, BLOCK_SIZE, num_warps)\n\n    # Additional test cases to\
    \ cover all branches\n    results = {}\n\n    # Test case 1\n    batch, seq_len,\
    \ n_heads, head_dim = 1, 8, 4, 32\n    Q = torch.randn(batch, seq_len, n_heads,\
    \ head_dim, device='cuda')\n    cos = torch.randn(seq_len, head_dim//2, device='cuda')\n\
    \    sin = torch.randn(seq_len, head_dim//2, device='cuda')\n    Q_out, cos_out,\
    \ sin_out, n_groups, BLOCK_SIZE, num_warps = _rope_embedding_forward_impl(Q, cos,\
    \ sin)\n    dY = torch.randn(batch, seq_len, n_heads, head_dim, device='cuda')\n\
    \    dY_out = _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE,\
    \ num_warps)\n    results['test_case_1'] = (Q_out, dY_out)\n\n    # Test case\
    \ 2\n    batch, seq_len, n_heads, head_dim = 4, 32, 16, 128\n    Q = torch.randn(batch,\
    \ seq_len, n_heads, head_dim, device='cuda')\n    cos = torch.randn(seq_len, head_dim//2,\
    \ device='cuda')\n    sin = torch.randn(seq_len, head_dim//2, device='cuda')\n\
    \    Q_out, cos_out, sin_out, n_groups, BLOCK_SIZE, num_warps = _rope_embedding_forward_impl(Q,\
    \ cos, sin)\n    dY = torch.randn(batch, seq_len, n_heads, head_dim, device='cuda')\n\
    \    dY_out = _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE,\
    \ num_warps)\n    results['test_case_2'] = (Q_out, dY_out)\n\n    # Test case\
    \ 3\n    batch, seq_len, n_heads, head_dim = 8, 64, 32, 256\n    Q = torch.randn(batch,\
    \ seq_len, n_heads, head_dim, device='cuda')\n    cos = torch.randn(seq_len, head_dim//2,\
    \ device='cuda')\n    sin = torch.randn(seq_len, head_dim//2, device='cuda')\n\
    \    Q_out, cos_out, sin_out, n_groups, BLOCK_SIZE, num_warps = _rope_embedding_forward_impl(Q,\
    \ cos, sin)\n    dY = torch.randn(batch, seq_len, n_heads, head_dim, device='cuda')\n\
    \    dY_out = _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE,\
    \ num_warps)\n    results['test_case_3'] = (Q_out, dY_out)\n\n    # Test case\
    \ 4\n    batch, seq_len, n_heads, head_dim = 16, 128, 64, 512\n    Q = torch.randn(batch,\
    \ seq_len, n_heads, head_dim, device='cuda')\n    cos = torch.randn(seq_len, head_dim//2,\
    \ device='cuda')\n    sin = torch.randn(seq_len, head_dim//2, device='cuda')\n\
    \    Q_out, cos_out, sin_out, n_groups, BLOCK_SIZE, num_warps = _rope_embedding_forward_impl(Q,\
    \ cos, sin)\n    dY = torch.randn(batch, seq_len, n_heads, head_dim, device='cuda')\n\
    \    dY_out = _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE,\
    \ num_warps)\n    results['test_case_4'] = (Q_out, dY_out)\n\n    return results\n\
    \nresult_gold = test_rope_embedding_forward()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rope_embedding.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rope_embedding
