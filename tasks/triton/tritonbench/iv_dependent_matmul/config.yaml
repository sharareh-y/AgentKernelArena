compile_command:
- python iv_dependent_matmul.py
correctness_command:
- python iv_dependent_matmul_perf.py
performance_command:
- tb_eval -f iv_dependent_matmul.py -o iv_dependent_matmul_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided Triton operator defines a matrix multiplication function\
    \ `iv_dependent_matmul_kernel`, which is executed on a GPU using the Triton framework.\
    \ The kernel function takes pointers to matrices `a`, `b`, and `c`, along with\
    \ their dimensions `M`, `N`, `K`, and strides for memory layout. It also accepts\
    \ block sizes `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, `BLOCK_SIZE_K` and a string `type`\
    \ to define different loading strategies. The kernel uses a double loop strategy\
    \ to load and compute block matrices from `a` and `b`, storing results in `c`,\
    \ based on specified offsets calculated from program ID `pid`. The function supports\
    \ conditional logic for various data preloading strategies to optimize memory\
    \ access patterns. The wrapper function `iv_dependent_matmul_wrapper` handles\
    \ setup, including device selection, random matrix generation, and grid configuration,\
    \ to invoke the kernel with specific parameters. It uses the Triton grid configuration\
    \ to determine execution dimensions and sets the number of pipeline stages depending\
    \ on the scheduling type. The result is stored in `triton_output`.\n    \nThe\
    \ test code is:\n\n\nimport torch\n\n# \u5C01\u88C5 IV Dependent MatMul \u6D4B\
    \u8BD5\u7684\u51FD\u6570\ndef test_iv_dependent_matmul_kernel():\n    # \u5B9A\
    \u4E49\u77E9\u9635\u7EF4\u5EA6\u548C\u5757\u5927\u5C0F\n    M = 256\n    K = 256\n\
    \    N = 256\n    BLOCK_SIZE_M = 32\n    BLOCK_SIZE_N = 32\n    BLOCK_SIZE_K =\
    \ 32\n\n    # \u521B\u5EFA CUDA \u8BBE\u5907\n    device = torch.device('cuda')\n\
    \n    # \u5B9A\u4E49\u6240\u6709\u7C7B\u578B\u7684\u5185\u6838\u914D\u7F6E\n \
    \   types = [\n        \"pre_load\",\n        \"post_load\",\n        \"post_pre_mixed\"\
    ,\n        \"post_load_two_iters\",\n        \"post_load_three_iters\"\n    ]\n\
    \n    # \u5B57\u5178\u7528\u4E8E\u5B58\u50A8\u6BCF\u4E2A\u6D4B\u8BD5\u7528\u4F8B\
    \u7684\u7ED3\u679C\n    results = {}\n\n    # \u904D\u5386\u6BCF\u79CD\u5185\u6838\
    \u7C7B\u578B\uFF0C\u8FDB\u884C\u6D4B\u8BD5\n    for i, type in enumerate(types):\n\
    \        # \u8C03\u7528\u5C01\u88C5\u51FD\u6570\u8FD0\u884C Triton \u6838\u5FC3\
    \n        triton_output = iv_dependent_matmul_wrapper(\n            M=M,\n   \
    \         K=K,\n            N=N,\n            BLOCK_SIZE_M=BLOCK_SIZE_M,\n   \
    \         BLOCK_SIZE_N=BLOCK_SIZE_N,\n            BLOCK_SIZE_K=BLOCK_SIZE_K,\n\
    \            type=type,\n            device=device\n        )\n\n        # \u786E\
    \u4FDD\u8F93\u51FA\u7684\u5927\u5C0F\u6B63\u786E\n        assert triton_output.shape\
    \ == (M, N), f\"Expected output shape {(M, N)} but got {triton_output.shape} for\
    \ type {type}\"\n\n        # \u4FDD\u5B58\u7ED3\u679C\u5230\u5B57\u5178\n    \
    \    results[f\"test_case_{i+1}\"] = triton_output\n\n    return results\n\n#\
    \ \u6267\u884C\u6D4B\u8BD5\u51FD\u6570\u8FDB\u884C\u6240\u6709\u7C7B\u578B\u7684\
    \u9A8C\u8BC1\nresult_gold = test_iv_dependent_matmul_kernel()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py iv_dependent_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- iv_dependent_matmul
