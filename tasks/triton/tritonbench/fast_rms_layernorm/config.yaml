compile_command:
- python fast_rms_layernorm.py
correctness_command:
- python fast_rms_layernorm_perf.py
performance_command:
- tb_eval -f fast_rms_layernorm.py -o fast_rms_layernorm_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The code defines a fast RMS Layernorm operation using the Triton language\
    \ for GPU acceleration. It contains three main kernels: `_rms_layernorm_forward`,\
    \ `_rms_layernorm_backward`, and `_gemma_rms_layernorm_forward`. The `_rms_layernorm_forward`\
    \ kernel computes the row-wise variance, calculates the inverse square root of\
    \ the variance, and normalizes the input data using this inverse variance. The\
    \ result is scaled by a weight vector to produce the output. The `_rms_layernorm_backward`\
    \ kernel computes gradients with respect to the input and the weight based on\
    \ the chain rule. The `_gemma_rms_layernorm_forward` kernel is similar to `_rms_layernorm_forward`\
    \ but includes an additional constant `1.0` added to the weight during the scaling\
    \ operation. These kernels are used within the `Fast_RMS_Layernorm` PyTorch autograd\
    \ function. The forward method calculates the output of the layer normalization\
    \ and saves the relevant variables for the backward pass, while the backward method\
    \ computes the gradients with respect to the inputs. Common variables include\
    \ `BLOCK_SIZE`, which determines the computational block size, and `num_warps`,\
    \ indicating the number of GPU warps used for parallel computation. The function\
    \ `calculate_settings` decides these values based on input dimensions. The `fast_rms_layernorm`\
    \ function is a helper that applies the `Fast_RMS_Layernorm` on the given input\
    \ and layernorm configuration. This setup allows for efficient layer normalization\
    \ computation on GPUs using custom Triton kernels, aimed at providing significant\
    \ speedup over standard implementations.\n    \nThe test code is:\n\n\n# Test\
    \ function for the fast_rms_layernorm with backward pass\ndef test_fast_rms_layernorm_with_backward():\n\
    \    # Create a random input tensor with gradient tracking\n    X = torch.randn(2,\
    \ 4, 8, device='cuda', dtype=torch.float32, requires_grad=True)\n\n    # Create\
    \ a layernorm instance\n    layernorm = SimpleLayerNorm(normalized_shape=X.shape[-1])\n\
    \n    results = {}\n\n    # Test without GEMMA\n    output = fast_rms_layernorm(layernorm,\
    \ X, gemma=False)\n    loss = output.mean()  # Compute a dummy loss\n\n    # Perform\
    \ backward propagation\n    loss.backward()\n    results['test_case_1'] = X.grad.clone()\n\
    \n    # Reset gradients for the next test\n    X.grad.zero_()\n\n    # Test with\
    \ GEMMA\n    output_gemma = fast_rms_layernorm(layernorm, X, gemma=True)\n   \
    \ loss_gemma = output_gemma.mean()  # Compute a dummy loss\n\n    # Perform backward\
    \ propagation\n    loss_gemma.backward()\n    results['test_case_2'] = X.grad.clone()\n\
    \n    return results\n\n# Run the test\nresult_gold = test_fast_rms_layernorm_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fast_rms_layernorm.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fast_rms_layernorm
