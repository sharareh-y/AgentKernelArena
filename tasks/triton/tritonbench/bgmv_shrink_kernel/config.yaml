compile_command:
- python bgmv_shrink_kernel.py
correctness_command:
- python bgmv_shrink_kernel_perf.py
performance_command:
- tb_eval -f bgmv_shrink_kernel.py -o bgmv_shrink_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `_bgmv_shrink_kernel` is designed for executing\
    \ a batched generalized matrix-vector multiplication (BGMV) using low-rank adaptation\
    \ (LORA). It is optimized for GPU execution using Triton's just-in-time (JIT)\
    \ compilation.\n\n            The kernel operates as follows:\n            - It\
    \ processes multiple batches of input data, with each batch associated with a\
    \ possibly different LORA matrix indicated by `lora_indices`. If a batch's index\
    \ in `lora_indices` is -1, it is skipped.\n            - The computation involves\
    \ splitting the `K` dimension into manageable blocks of size `BLOCK_K`. `SPLIT_K`\
    \ determines how many such blocks are processed by each kernel instance.\n   \
    \         - `input_ptr` and `lora_ptr` provide the starting addresses of input\
    \ data and LORA matrices, respectively. Strides define how elements in these tensors\
    \ are laid out in memory, facilitating efficient data loading.\n            -\
    \ The kernel initializes a zeroed accumulator for each block and iteratively loads\
    \ blocks of data from the input tensor and LORA matrix. It performs element-wise\
    \ multiplication and reduction, summing the results along the `K` dimension.\n\
    \            - The `scaling` factor is applied to the accumulated result before\
    \ storing it back in the `out_ptr` location, with atomic addition used if reductions\
    \ across multiple kernel instances are required (`SPLIT_K` > 1).\n\n         \
    \   The wrapper function `_bgmv_shrink` is responsible for configuring and launching\
    \ the kernel:\n            - It ensures the input, LORA weight matrix, and output\
    \ tensor are contiguous in memory to enhance access speed and reduces risk of\
    \ memory-bound operations.\n            - It extracts the batch count and the\
    \ dimensions `N` and `K` from the LORA weight tensor.\n            - `BLOCK_N`\
    \ is computed as the next power of two greater than or equal to `N` to ensure\
    \ optimal performance on the hardware.\n            - A grid of threads is configured\
    \ to handle the number of batches and the split of the `K` dimension, ensuring\
    \ efficient parallel execution.\n            - Finally, the kernel is invoked\
    \ with the prepared arguments and configuration.\n            \nThe test code\
    \ is:\n\n\nimport torch\n\n# Test function for _bgmv_shrink\ndef test_bgmv_shrink():\n\
    \    # Define input parameters\n    batch_size = 2\n    N = 16\n    K = 32\n \
    \   scaling = 1.0\n\n    # Create input tensors\n    inputs = torch.randn((batch_size,\
    \ K), dtype=torch.float16, device='cuda').contiguous()\n    lora_a_weights = torch.randn((batch_size,\
    \ 1, N, K), dtype=torch.float16, device='cuda').contiguous()\n    output_tensor\
    \ = torch.zeros((batch_size, N), dtype=torch.float16, device='cuda').contiguous()\n\
    \    lora_indices_tensor = torch.tensor([0, 1], dtype=torch.int32, device='cuda')\n\
    \n    # Call the _bgmv_shrink function\n    _bgmv_shrink(\n        inputs=inputs,\n\
    \        lora_a_weights=lora_a_weights,\n        output_tensor=output_tensor,\n\
    \        lora_indices_tensor=lora_indices_tensor,\n        scaling=scaling\n \
    \   )\n\n    # Store the result in a dictionary\n    results = {\n        \"test_case_1\"\
    : output_tensor.clone()\n    }\n\n    # Additional test cases to cover more branches\n\
    \    lora_indices_tensor = torch.tensor([-1, 1], dtype=torch.int32, device='cuda')\n\
    \    _bgmv_shrink(\n        inputs=inputs,\n        lora_a_weights=lora_a_weights,\n\
    \        output_tensor=output_tensor,\n        lora_indices_tensor=lora_indices_tensor,\n\
    \        scaling=scaling\n    )\n    results[\"test_case_2\"] = output_tensor.clone()\n\
    \n    lora_indices_tensor = torch.tensor([0, -1], dtype=torch.int32, device='cuda')\n\
    \    _bgmv_shrink(\n        inputs=inputs,\n        lora_a_weights=lora_a_weights,\n\
    \        output_tensor=output_tensor,\n        lora_indices_tensor=lora_indices_tensor,\n\
    \        scaling=scaling\n    )\n    results[\"test_case_3\"] = output_tensor.clone()\n\
    \n    lora_indices_tensor = torch.tensor([-1, -1], dtype=torch.int32, device='cuda')\n\
    \    _bgmv_shrink(\n        inputs=inputs,\n        lora_a_weights=lora_a_weights,\n\
    \        output_tensor=output_tensor,\n        lora_indices_tensor=lora_indices_tensor,\n\
    \        scaling=scaling\n    )\n    results[\"test_case_4\"] = output_tensor.clone()\n\
    \n    return results\n\n# Run the test\nresult_gold = test_bgmv_shrink()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py bgmv_shrink_kernel.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- bgmv_shrink_kernel
