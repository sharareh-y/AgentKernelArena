compile_command:
- python softmax_optimize.py
correctness_command:
- python softmax_optimize_perf.py
performance_command:
- tb_eval -f softmax_optimize.py -o softmax_optimize_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The `softmax_kernel_online_v2` function is a Triton-optimized kernel\
    \ designed to compute the softmax function over a 2D input tensor. It uses tiling\
    \ to manage computation over potentially large matrices by splitting the data\
    \ into manageable chunks, or tiles, of size `TILE_N`. This function accepts pointers\
    \ to the input and output data, dimensions of the input matrix (M by N), and the\
    \ tile size `TILE_N`. The computation occurs in two main phases: a reduction phase\
    \ to compute necessary statistics (maximum value and sum of exponentials for stable\
    \ softmax) and a final output phase where the input data is exponentiated, normalized\
    \ by the sum, and stored in the output location. The kernel efficiently handles\
    \ matrix boundaries by leveraging the `prev_multiple_of` helper to compute the\
    \ largest possible tile-aligned boundary within matrix dimensions. This boundary\
    \ is used to handle potential edge cases in matrix dimensions that are not perfectly\
    \ divisible by the tile size. The function's design ensures numerical stability\
    \ and efficiency by computing softmax in a numerically stable manner using a two-pass\
    \ algorithm. The wrapper `softmax` function prepares the necessary parameters,\
    \ invokes this kernel, and returns the softmax-normalized output.\n    \nThe test\
    \ code is:\n\n\n# Comparison Test\ndef test_softmax():\n\n    torch.manual_seed(0)\n\
    \    \n    result = {}\n    \n    # Case 1: M = 128, N = 512\n    x1 = torch.randn(128,\
    \ 512, device='cuda', dtype=torch.float32)\n    result['test_case_1'] = softmax(x1)\n\
    \n    # Case 2: M = 64, N = 1024\n    x2 = torch.randn(64, 1024, device='cuda',\
    \ dtype=torch.float32)\n    result['test_case_2'] = softmax(x2)\n\n    # Case\
    \ 3: M = 256, N = 128\n    x3 = torch.randn(256, 128, device='cuda', dtype=torch.float32)\n\
    \    result['test_case_3'] = softmax(x3)\n    \n    return result\n\n# Execute\
    \ test function\nresult_gold = test_softmax()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ softmax_optimize.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- softmax_optimize
