compile_command:
- python context_attn_fwd.py
correctness_command:
- python context_attn_fwd_perf.py
performance_command:
- tb_eval -f context_attn_fwd.py -o context_attn_fwd_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton kernel function `_fwd_kernel_int8kv` is designed to perform a\
    \ scaled dot-product attention operation on inputs Q, K, and V, where Q represents\
    \ the query tensor, K represents the key tensor, and V represents the value tensor.\
    \ The function computes attention scores by taking the dot product of Q and K,\
    \ scales them using a softmax scaling factor (derived from the inverse square\
    \ root of the model dimension and a logarithmic factor), and applies a causal\
    \ mask to enforce autoregressive behavior. The resulting probabilities are then\
    \ used to weigh the values in V, producing the final attention output stored in\
    \ Out. The function uses constant expression parameters (`H`, `BLOCK_DMODEL`,\
    \ `BLOCK_M`, `BLOCK_N`) to define block sizes and tensor dimensions for efficient\
    \ parallel execution. It accommodates prompt caching and supports multiple head\
    \ and batch sizes. The wrapper function `context_attention_fwd_ppl_int8kv` configures\
    \ the execution grid, sets parameters based on input tensor dimensions, and invokes\
    \ the kernel. The wrapper ensures the block size is adjusted based on the GPU\
    \ architecture (e.g., Tesla GPUs). This implementation is tailored for high-performance\
    \ operations on compatible hardware, particularly optimized for workloads requiring\
    \ efficient memory and computational throughput.\n    \nThe test code is:\n\n\n\
    import torch\n\ndef test_context_attention_fwd_ppl_int8kv():\n    Z, H, N_CTX,\
    \ D_HEAD = 16, 16, 2048, 128\n    dtype = torch.float16\n    prompt_cache_len\
    \ = 0\n    q = torch.empty((Z * (N_CTX - prompt_cache_len), H, D_HEAD), dtype=dtype,\
    \ device=\"cuda\").normal_(mean=0.1, std=0.2)\n    kv = torch.empty((Z, 2 * H,\
    \ N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2)\n \
    \   k = kv[:, :H]\n    v = kv[:, H:]\n    o = torch.empty((Z * (N_CTX - prompt_cache_len),\
    \ H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n    max_input_len\
    \ = N_CTX\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32, device=\"cuda\"\
    )\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"cuda\")\n    b_prompt_cache_len\
    \ = torch.zeros(Z, dtype=torch.int32, device=\"cuda\")\n\n    results = {}\n\n\
    \    # Test case 1\n    context_attention_fwd_ppl_int8kv(q, k, v, o, b_start_loc,\
    \ b_seq_len, max_input_len, b_prompt_cache_len)\n    results['test_case_1'] =\
    \ o.clone()\n\n    # Test case 2: Different prompt_cache_len\n    prompt_cache_len\
    \ = 10\n    q = torch.empty((Z * (N_CTX - prompt_cache_len), H, D_HEAD), dtype=dtype,\
    \ device=\"cuda\").normal_(mean=0.1, std=0.2)\n    b_prompt_cache_len = torch.full((Z,),\
    \ prompt_cache_len, dtype=torch.int32, device=\"cuda\")\n    context_attention_fwd_ppl_int8kv(q,\
    \ k, v, o, b_start_loc, b_seq_len, max_input_len, b_prompt_cache_len)\n    results['test_case_2']\
    \ = o.clone()\n\n    # Test case 3: Different max_input_len\n    max_input_len\
    \ = N_CTX // 2\n    context_attention_fwd_ppl_int8kv(q, k, v, o, b_start_loc,\
    \ b_seq_len, max_input_len, b_prompt_cache_len)\n    results['test_case_3'] =\
    \ o.clone()\n\n    # Test case 4: Different batch size\n    Z = 8\n    q = torch.empty((Z\
    \ * (N_CTX - prompt_cache_len), H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1,\
    \ std=0.2)\n    kv = torch.empty((Z, 2 * H, N_CTX, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.4, std=0.2)\n    k = kv[:, :H]\n    v = kv[:, H:]\n   \
    \ o = torch.empty((Z * (N_CTX - prompt_cache_len), H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.3, std=0.2)\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32,\
    \ device=\"cuda\")\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"\
    cuda\")\n    b_prompt_cache_len = torch.zeros(Z, dtype=torch.int32, device=\"\
    cuda\")\n    context_attention_fwd_ppl_int8kv(q, k, v, o, b_start_loc, b_seq_len,\
    \ max_input_len, b_prompt_cache_len)\n    results['test_case_4'] = o.clone()\n\
    \n    return results\n\nresult_gold = test_context_attention_fwd_ppl_int8kv()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py context_attn_fwd.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- context_attn_fwd
