compile_command:
- python kldiv_compute.py
correctness_command:
- python kldiv_compute_perf.py
performance_command:
- tb_eval -f kldiv_compute.py -o kldiv_compute_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel function `kldivergence_kernel` performs element-wise\
    \ computation of Kullback-Leibler (KL) divergence between two input vectors, `x_ptr`\
    \ and `y_ptr`, storing results in `output_ptr`. It utilizes a block-wise processing\
    \ approach, specified by `BLOCK_SIZE`, where each program (or kernel instance)\
    \ computes over a distinct block of elements. The kernel starts by obtaining a\
    \ unique program identifier (`pid`) for each execution using `tl.program_id(axis=0)`,\
    \ which allows computation over a 1D grid. For each block, indices are determined\
    \ by calculating `block_start = pid * BLOCK_SIZE` and creating an `offsets` tensor\
    \ as `block_start + tl.arange(0, BLOCK_SIZE)`. A `mask` is used to ensure memory\
    \ accesses are within bounds, protecting against out-of-bounds errors when loading\
    \ from `x_ptr` and `y_ptr`. Elements of `x` and `y` within the current block are\
    \ loaded using `tl.load()`, and KL divergence is computed with `output = x * tl.log(x\
    \ / y)`. This result is conditionally stored in `output_ptr` using `tl.store(output_ptr\
    \ + offsets, output, mask=mask)`. The function `kldivergence` serves as the Python\
    \ API, enforcing GPU presence of input tensors and setting up the grid for kernel\
    \ execution. It defines a grid function, `grid = lambda meta: (triton.cdiv(n_elements,\
    \ meta['BLOCK_SIZE']), )`, which calculates the number of blocks needed to cover\
    \ all elements. The kernel is then launched with specified block size, processing\
    \ inputs `x` and `y` and returning the resulting `output`.\n            \nThe\
    \ test code is:\n\n\nimport torch\n\ndef test_kldivergence():\n    size = 98432\n\
    \    x = torch.rand(size, device='cuda')\n    y = torch.rand(size, device='cuda')\n\
    \n    # \u4F7F\u7528 Triton \u8BA1\u7B97 KL \u6563\u5EA6\n    output_triton =\
    \ kldivergence(x, y)\n\n    # \u5206\u652F\u8986\u76D6\u7387\u30101/4\u3011\n\n\
    \    # \u8865\u5168\u6240\u6709\u5206\u652F\u8C03\u7528\n    results = {}\n  \
    \  \n    # Test case 1\n    x1 = torch.rand(1024, device='cuda')\n    y1 = torch.rand(1024,\
    \ device='cuda')\n    results['test_case_1'] = kldivergence(x1, y1)\n\n    # Test\
    \ case 2\n    x2 = torch.rand(2048, device='cuda')\n    y2 = torch.rand(2048,\
    \ device='cuda')\n    results['test_case_2'] = kldivergence(x2, y2)\n\n    # Test\
    \ case 3\n    x3 = torch.rand(4096, device='cuda')\n    y3 = torch.rand(4096,\
    \ device='cuda')\n    results['test_case_3'] = kldivergence(x3, y3)\n\n    # Test\
    \ case 4\n    x4 = torch.rand(8192, device='cuda')\n    y4 = torch.rand(8192,\
    \ device='cuda')\n    results['test_case_4'] = kldivergence(x4, y4)\n\n    return\
    \ results\n\nresult_gold = test_kldivergence()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ kldiv_compute.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- kldiv_compute
