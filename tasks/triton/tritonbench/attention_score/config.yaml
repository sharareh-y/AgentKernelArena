compile_command:
- python attention_score.py
correctness_command:
- python attention_score_perf.py
performance_command:
- tb_eval -f attention_score.py -o attention_score_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_score_kernel` is a Triton-based GPU kernel designed to compute\
    \ attention scores in transformer models. It uses a block-wise approach to leverage\
    \ GPU parallelism and optimize memory access. The main inputs are the matrices\
    \ `Q` (query), `K` (key), and `M` (mask), with the result being stored in `Out`.\
    \ The kernel iterates over blocks defined by `BLOCK_M` and `BLOCK_N`, performing\
    \ operations in these fixed-size submatrices.\n\n            Key computations\
    \ involve loading query blocks `Q` and key blocks `K`, calculating the dot product\
    \ to get the score matrix `qk`, and scaling this matrix by a factor `sm_scale`\
    \ which accounts for the softmax operation. It includes conditions for sliding\
    \ window attention, checking bounds to handle non-even dimensions, and reducing\
    \ results into the output vector `o`.\n\n            The function `get_score`\
    \ is a Python wrapper function that prepares inputs and executes the kernel. It\
    \ determines grid size based on the dimensions of K and Q. If there is a resource\
    \ constraint error due to large block sizes, it reduces `BLOCK_M` and `BLOCK_N`\
    \ by half and retries execution. This function also calculates the scale factor\
    \ for attention (`sm_scale`) and manages additional parameters like sliding window\
    \ configurations.\n            \nThe test code is:\n\n\nimport torch\n\n# Define\
    \ the test function for get_score\ndef test_get_score():\n    # Define input dimensions\n\
    \    batch_size = 2\n    num_heads = 4\n    seq_len = 128\n    d_model = 64\n\n\
    \    # Create random input tensors\n    q = torch.randn((batch_size, num_heads,\
    \ seq_len, d_model), device='cuda', dtype=torch.float16)\n    k = torch.randn((batch_size,\
    \ num_heads, seq_len, d_model), device='cuda', dtype=torch.float16)\n    m = torch.zeros((batch_size,\
    \ num_heads, seq_len), device='cuda', dtype=torch.float32)\n\n    # Define sliding\
    \ window parameters\n    sliding_window = (0, 64)\n    complement_sliding_window\
    \ = False\n\n    # Call the get_score function\n    ret1 = get_score(q, k, m,\
    \ sliding_window, complement_sliding_window)\n\n    # Test with complement_sliding_window\
    \ = True\n    complement_sliding_window = True\n    ret2 = get_score(q, k, m,\
    \ sliding_window, complement_sliding_window)\n\n    # Test without sliding window\n\
    \    sliding_window = None\n    complement_sliding_window = False\n    ret3 =\
    \ get_score(q, k, m, sliding_window, complement_sliding_window)\n\n    # Test\
    \ with different sliding window size\n    sliding_window = (0, 32)\n    ret4 =\
    \ get_score(q, k, m, sliding_window, complement_sliding_window)\n\n    results\
    \ = {\n        \"test_case_1\": ret1,\n        \"test_case_2\": ret2,\n      \
    \  \"test_case_3\": ret3,\n        \"test_case_4\": ret4\n    }\n    return results\n\
    \n# Run the tests\nresult_gold = test_get_score()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ attention_score.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attention_score
