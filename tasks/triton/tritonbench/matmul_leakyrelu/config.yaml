compile_command:
- python matmul_leakyrelu.py
correctness_command:
- python matmul_leakyrelu_perf.py
performance_command:
- tb_eval -f matmul_leakyrelu.py -o matmul_leakyrelu_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The code implements a Triton kernel `matmul_kernel` to perform matrix\
    \ multiplication C = A x B, where A has shape (M, K) and B has shape (K, N), resulting\
    \ in matrix C with shape (M, N). The kernel processes blocks of A and B defined\
    \ by `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`, with parallel execution\
    \ managed using a grid of program IDs. The kernel computes a block of C by iteratively\
    \ loading blocks of A and B, performing dot products, and accumulating the results\
    \ in the `accumulator`. Optionally, a leaky ReLU activation is applied to the\
    \ `accumulator`. The final block result is stored in C using Triton's memory operations,\
    \ with handling for matrix boundaries via masks. The `matmul` function serves\
    \ as a high-level interface, ensuring input validity, preparing execution parameters,\
    \ and invoking the kernel. It accepts optional activation, setting `ACTIVATION`\
    \ based on user input.\n    \nThe test code is:\n\n\ndef test_matmul():\n    #\
    \ Set random seed for reproducibility\n    torch.manual_seed(0)\n\n    # Define\
    \ matrix dimensions\n    M, K, N = 64, 128, 64\n\n    # Create random matrices\
    \ A and B\n    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n \
    \   b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    # Compute\
    \ matrix multiplication using Triton with leaky_relu activation\n    c_triton_leaky_relu\
    \ = matmul(a, b, activation=\"leaky_relu\")\n\n    # Compute matrix multiplication\
    \ using Triton without activation\n    c_triton_no_activation = matmul(a, b, activation=\"\
    \")\n\n    # Store results in a dictionary\n    results = {\n        \"test_case_1\"\
    : c_triton_leaky_relu,\n        \"test_case_2\": c_triton_no_activation\n    }\n\
    \    \n    return results\n\n# Run the test\nresult_gold = test_matmul()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matmul_leakyrelu.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_leakyrelu
