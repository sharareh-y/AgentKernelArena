compile_command:
- python attention_llama.py
correctness_command:
- python attention_llama_perf.py
performance_command:
- tb_eval -f attention_llama.py -o attention_llama_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton JIT kernel `_fwd_kernel` is designed to compute the forward\
    \ pass of a self-attention mechanism with support for both regular and causal\
    \ attention (controlled by the `IS_CAUSAL` parameter). It takes in matrices Q,\
    \ K, and V, which represent the query, key, and value tensors of the attention\
    \ mechanism. The kernel implements a block-wise approach to handle large input\
    \ sizes efficiently, processing data in chunks defined by constants `BLOCK_M`,\
    \ `BLOCK_N`, and `BLOCK_DMODEL`.\n\n            The kernel computes a scaled dot-product\
    \ between the query and key matrices, applying a softmax operation to generate\
    \ attention scores. It then applies these scores to the value matrix to produce\
    \ the output. The `sm_scale` parameter is used to scale the logits before applying\
    \ the softmax, which is a common practice to stabilize gradients. \n\n       \
    \     If `IS_CAUSAL` is true, the kernel masks future positions to maintain causality\
    \ in autoregressive models, ensuring that each position only attends to previous\
    \ positions.\n\n            The kernel supports mixed-precision computations,\
    \ leveraging 8-bit floats (FP8) when keys and values are stored as `torch.int8`,\
    \ controlled by `USE_FP8`. It rematerializes offsets to manage Triton's register\
    \ pressure effectively.\n\n            The wrapper function `triton_fa` is responsible\
    \ for invoking `_fwd_kernel`, ensuring the correct configurations and input data\
    \ formats. It checks data type consistency and adjusts grid and block sizes based\
    \ on the input dimensions, utilizing a dynamic grid size to cover the full input.\
    \ The grid is determined by `triton.cdiv(m_size, BLOCK)` for the rows, and `head_size\
    \ * batch` for the second dimension, to handle all heads in parallel. The kernel\
    \ uses either 4 or 8 warps based on the block size (`Lk`), and stages are managed\
    \ with `num_stages=2` to balance register usage and computational latency.\n \
    \           \nThe test code is:\n\n\nimport torch\nimport math\n\ndef test_triton_fa():\n\
    \    # Test case 1: Non-causal, float16\n    xq = torch.randn([1, 16, 32, 128],\
    \ dtype=torch.float16, device=\"cuda\")\n    keys = torch.randn([1, 16, 32, 128],\
    \ dtype=torch.float16, device=\"cuda\")\n    values = torch.randn([1, 16, 32,\
    \ 128], dtype=torch.float16, device=\"cuda\")\n\n    xq = xq.transpose(1, 2)\n\
    \    keys = keys.transpose(1, 2)\n    values = values.transpose(1, 2)\n\n    scale\
    \ = 1 / math.sqrt(128)\n    output_t1 = triton_fa(xq, keys, values, scale, False,\
    \ 0)\n\n    # Test case 2: Causal, float16\n    output_t2 = triton_fa(xq, keys,\
    \ values, scale, True, 0)\n\n    # Test case 3: Non-causal, int8\n    keys_int8\
    \ = keys.to(torch.int8)\n    values_int8 = values.to(torch.int8)\n    output_t3\
    \ = triton_fa(xq, keys_int8, values_int8, scale, False, 0)\n\n    # Test case\
    \ 4: Causal, int8\n    output_t4 = triton_fa(xq, keys_int8, values_int8, scale,\
    \ True, 0)\n\n    return {\n        \"test_case_1\": output_t1,\n        \"test_case_2\"\
    : output_t2,\n        \"test_case_3\": output_t3,\n        \"test_case_4\": output_t4\n\
    \    }\n\nresult_gold = test_triton_fa()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ attention_llama.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- attention_llama
