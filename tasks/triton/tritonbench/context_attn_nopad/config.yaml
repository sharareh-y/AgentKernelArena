compile_command:
- python context_attn_nopad.py
correctness_command:
- python context_attn_nopad_perf.py
performance_command:
- tb_eval -f context_attn_nopad.py -o context_attn_nopad_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The kernel `_fwd_kernel` is part of a Triton operator for computing scaled\
    \ dot-product attention. It accepts tensors Q (queries), K (keys), V (values),\
    \ and additional parameters like `sm_scale`, `B_Start_Loc`, and `B_Seqlen` to\
    \ handle batches of varying sequence lengths. The `context_attention_fwd` function\
    \ initializes and dispatches this kernel, with arguments including strides for\
    \ Q, K, V, and Out, which help index these multidimensional tensors correctly\
    \ within the kernel. Inside `_fwd_kernel`, a blocking strategy defined by constants\
    \ BLOCK_M, BLOCK_DMODEL, and BLOCK_N is used to partition the computation, enhancing\
    \ parallelism and memory efficiency. The kernel calculates attention scores by\
    \ dot-product of queries and keys, scales them, and applies a softmax operation.\
    \ It accumulates results to form the output by weighting value vectors V accordingly.\
    \ The `context_attention_fwd` computes the softmax scaling factor `sm_scale` as\
    \ the inverse square root of query depth, ensuring numeric stability. The grid\
    \ for launching the kernel is determined by the batch size, number of heads, and\
    \ input length, and it adapts the number of warps based on key length Lk.\n  \
    \  \nThe test code is:\n\n\ndef test_context_attention_fwd():\n    Z, H, N_CTX,\
    \ D_HEAD = 4, 6, 1024, 128\n    dtype = torch.float16\n    Z = 3\n    q = torch.empty((Z\
    \ * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n\
    \    k = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4,\
    \ std=0.2)\n    v = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.3, std=0.2)\n    o = torch.empty((Z * N_CTX, H, D_HEAD),\
    \ dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n\n    max_input_len\
    \ = N_CTX\n    Z = 4\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32, device=\"\
    cuda\")\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"cuda\")\n\
    \n    b_seq_len[0] = 512\n    b_seq_len[1] = 1024\n    b_seq_len[2] = 512\n  \
    \  b_seq_len[3] = 1024\n\n    for i in range(1, Z):\n        b_start_loc[i] =\
    \ b_start_loc[i - 1] + b_seq_len[i - 1]\n\n    # case 1: Normal call with the\
    \ given setup (should run without issue)\n    result_case_1 = {}\n    context_attention_fwd(q,\
    \ k, v, o, b_start_loc, b_seq_len, max_input_len)\n    result_case_1['normal']\
    \ = o.clone()\n\n    # case 2: Alter max_input_len, making it smaller or larger\
    \ to check boundary conditions\n    max_input_len_case_2 = 512\n    result_case_2\
    \ = {}\n    context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len_case_2)\n\
    \    result_case_2['max_input_len_512'] = o.clone()\n\n    # case 3: Modify batch\
    \ size Z to test larger batch processing\n    Z_case_3 = 8  # larger batch size\n\
    \    b_start_loc_case_3 = torch.zeros((Z_case_3,), dtype=torch.int32, device=\"\
    cuda\")\n    b_seq_len_case_3 = torch.ones((Z_case_3,), dtype=torch.int32, device=\"\
    cuda\")\n    b_seq_len_case_3[0] = 512\n    b_seq_len_case_3[1] = 1024\n    for\
    \ i in range(1, Z_case_3):\n        b_start_loc_case_3[i] = b_start_loc_case_3[i\
    \ - 1] + b_seq_len_case_3[i - 1]\n\n    result_case_3 = {}\n    context_attention_fwd(q,\
    \ k, v, o, b_start_loc_case_3, b_seq_len_case_3, max_input_len)\n    result_case_3['batch_size_8']\
    \ = o.clone()\n\n    # case 4: Test with different sequence lengths to check handling\
    \ of varying sequence lengths\n    b_seq_len_case_4 = torch.tensor([512, 256,\
    \ 1024, 512], dtype=torch.int32, device=\"cuda\")\n    b_start_loc_case_4 = torch.zeros((4,),\
    \ dtype=torch.int32, device=\"cuda\")\n    for i in range(1, 4):\n        b_start_loc_case_4[i]\
    \ = b_start_loc_case_4[i - 1] + b_seq_len_case_4[i - 1]\n\n    result_case_4 =\
    \ {}\n    context_attention_fwd(q, k, v, o, b_start_loc_case_4, b_seq_len_case_4,\
    \ max_input_len)\n    result_case_4['varying_seq_len'] = o.clone()\n\n    # Return\
    \ all results in a dictionary\n    return {\n        'result_case_1': result_case_1,\n\
    \        'result_case_2': result_case_2,\n        'result_case_3': result_case_3,\n\
    \        'result_case_4': result_case_4\n    }\n\n# Execute and save results\n\
    result_gold = test_context_attention_fwd()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ context_attn_nopad.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- context_attn_nopad
