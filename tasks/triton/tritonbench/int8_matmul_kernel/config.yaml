compile_command:
- python int8_matmul_kernel.py
correctness_command:
- python int8_matmul_kernel_perf.py
performance_command:
- tb_eval -f int8_matmul_kernel.py -o int8_matmul_kernel_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The provided code is a Triton-based implementation for efficient matrix multiplication\
    \ of two matrices `a` and `b`. It utilizes `triton.jit` to compile the `matmul_kernel`,\
    \ which computes the product using block-wise operations for parallel execution\
    \ on GPUs. The function `matmul` serves as a wrapper around this kernel to facilitate\
    \ its usage.\n\n**matmul_kernel Function:**\n- **Inputs:** \n  - Pointers to matrices\
    \ `a_ptr`, `b_ptr`, and `c_ptr`.\n  - Dimensions `M`, `N`, and `K`.\n  - Strides\
    \ for each matrix to navigate through memory.\n  - Constants for blocking: `BLOCK_SIZE_M`,\
    \ `BLOCK_SIZE_N`, `BLOCK_SIZE_K`, `GROUP_SIZE_M`.\n- **Outputs:** \n  - Writes\
    \ the result to the matrix pointed by `c_ptr`.\n- **Logic:**\n  - Calculates thread\
    \ and block IDs for distribution of work among threads.\n  - Computes the offsets\
    \ for accessing blocks of `a` and `b`.\n  - Accumulates dot products of int8 elements\
    \ using four stages of inner loop unrolling, ensuring correctness by shifting\
    \ and masking operations.\n  - The final result is stored in `c`.\n\n**matmul\
    \ Function:**\n- **Inputs:** \n  - `a`: A 2D tensor with shape `(M, 4*K)` in `int8`\
    \ type.\n  - `b`: A 2D tensor with shape `(K, N)` in `uint8` type, packed for\
    \ efficiency.\n- **Outputs:** \n  - Returns the resulting matrix `c` of shape\
    \ `(M, N)` with `int32` type.\n- **Functionality:**\n  - Checks for dimensional\
    \ compatibility and matrix continuity.\n  - Initializes an empty tensor `c` for\
    \ output.\n  - Defines a lambda `grid` for determining the execution grid size\
    \ based on block configurations.\n  - Launches the `matmul_kernel` with the calculated\
    \ grid size and required metadata.\n  \nThe test code is:\n\n\nimport torch\n\n\
    def test_matmul():\n    # Define test matrices for different configurations\n\
    \    test_results = {}\n\n    # Test case\n    M1, K1, N1 = 256, 256, 128\n  \
    \  a1 = torch.randint(0, 256, (M1, K1), dtype=torch.int32, device='cuda')\n  \
    \  b1 = torch.randint(0, 4, (K1 // 4, N1), dtype=torch.uint8, device='cuda')\n\
    \    c1 = matmul(a1, b1)\n    test_results[\"test_case\"] = c1\n\n    return test_results\n\
    \nresult_gold = test_matmul()\n# print(result_gold)\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ int8_matmul_kernel.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int8_matmul_kernel
