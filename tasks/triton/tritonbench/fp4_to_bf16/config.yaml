compile_command:
- python fp4_to_bf16.py
correctness_command:
- python fp4_to_bf16_perf.py
performance_command:
- tb_eval -f fp4_to_bf16.py -o fp4_to_bf16_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The given Triton code provides an implementation to convert tensors containing\
    \ packed FP4 (4-bit floating point) values into BF16 (bfloat16) values on a CUDA-capable\
    \ GPU using Triton. The implementation is composed of two main functions: `_fp4_packed_to_bf16`\
    \ and `triton_f4_to_bf16_kernel`.\n\n        1. Function: `_fp4_packed_to_bf16`\n\
    \        This function accepts packed FP4 values and produces BF16 values. The\
    \ packed FP4 values consist of low and high bits, which are separated using bitwise\
    \ operations and then interleaved. Each FP4 value consists of sign, exponent,\
    \ and mantissa components that are extracted using provided bit masks and constants.\n\
    \        \n        Key operations include:\n        - Separating sign, exponent,\
    \ and mantissa.\n        - Handling special cases, such as zero and denormal (0.5)\
    \ values.\n        - Adjusting exponents from FP4 to BF16, accounting for differences\
    \ in exponent bias.\n        - Reconstructing the value in FP32 format first,\
    \ then converting to BF16.\n        \n        2. Function: `triton_f4_to_bf16_kernel`\n\
    \        This Triton kernel handles parallel processing of the input tensor. It\
    \ processes data in blocks and employs `_fp4_packed_to_bf16` for conversion. Input\
    \ includes pointers to packed FP4 data and the intended BF16 output location.\
    \ The kernel uses a block size to iterate over chunks of data, efficiently converting\
    \ each.\n\n        Essential constants and parameters involved in conversion include:\n\
    \        - `SIGN_MASK_F4`, `MANTISSA_MASK_F4`: Used for extracting parts of FP4\
    \ numbers.\n        - `ZERO_BITS_F32`, `ZERO_POINT_FIVE_BITS_F32`: Bit patterns\
    \ for special FP32 values (zero and 0.5).\n        - `EBITS_F4_E2M1`, `MBITS_F4_E2M1`,\
    \ `EBITS_F32`, `MBITS_F32`: Exponent and mantissa specifications for FP4 and FP32.\n\
    \        - Bias constants to correct exponent value differences between formats.\n\
    \n        3. Wrapper: `triton_f4_to_bf16`\n        This Python function wraps\
    \ around the kernel call, ensuring the input tensor is contiguous and CUDA-allocated.\
    \ It calculates the grid dimensions for the kernel launch, corresponding to the\
    \ number of elements and block size. The output tensor is allocated with the appropriate\
    \ shape and type (bfloat16), and the kernel is executed.\n\n        In conclusion,\
    \ this code demonstrates how to effectively convert FP4 encoded data into BF16\
    \ using Triton's CUDA parallelism to achieve high performance, with attention\
    \ to numerical precision and edge-case handling.\n    \nThe test code is:\n\n\n\
    import torch\n\ndef test_triton_f4_to_bf16():\n    results = {}\n    \n    # Test\
    \ case 1\n    n_elements_in = 1024\n    x = torch.randint(0, 256, (n_elements_in,),\
    \ dtype=torch.uint8, device='cuda')\n    output = triton_f4_to_bf16(x)\n    results[\"\
    test_case_1\"] = output\n\n    # Test case 2\n    n_elements_in = 2048\n    x\
    \ = torch.randint(0, 256, (n_elements_in,), dtype=torch.uint8, device='cuda')\n\
    \    output = triton_f4_to_bf16(x)\n    results[\"test_case_2\"] = output\n\n\
    \    # Test case 3\n    n_elements_in = 512\n    x = torch.randint(0, 256, (n_elements_in,),\
    \ dtype=torch.uint8, device='cuda')\n    output = triton_f4_to_bf16(x)\n    results[\"\
    test_case_3\"] = output\n\n    # Test case 4\n    n_elements_in = 256\n    x =\
    \ torch.randint(0, 256, (n_elements_in,), dtype=torch.uint8, device='cuda')\n\
    \    output = triton_f4_to_bf16(x)\n    results[\"test_case_4\"] = output\n\n\
    \    return results\n\nresult_gold = test_triton_f4_to_bf16()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py fp4_to_bf16.py {kernel_path}` to check\
    \ the correctness and performance.The kernel_path is where you stored the generated\
    \ code.\nCall Status means whether the code can be executed, Exec Status means\
    \ whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fp4_to_bf16
