compile_command:
- python rmsnorm_fused.py
correctness_command:
- python rmsnorm_fused_perf.py
performance_command:
- tb_eval -f rmsnorm_fused.py -o rmsnorm_fused_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This code implements a custom RMS normalization using Triton to accelerate\
    \ the computation process. The kernel function `rms_norm_fwd_fused` is defined\
    \ with the `@triton.jit` decorator, which makes it suitable for just-in-time compilation\
    \ and execution on GPUs. The function is responsible for calculating the root\
    \ mean square (RMS) normalization of each row in the input matrix `X`. It does\
    \ so by computing the variance of elements in each row, calculating the inverse\
    \ of the standard deviation (rstd), and applying a learned weight matrix `W` to\
    \ the normalized output.\n\n            The function takes in pointers to input\
    \ tensor `X`, output tensor `Y`, and weights `W`, along with some metadata such\
    \ as `stride` for row access and `N` for the number of columns. It utilizes a\
    \ block-based approach to iterate over input data, computing partial results in\
    \ blocks of `BLOCK_SIZE` to efficiently leverage GPU capabilities.\n\n       \
    \     The `TritonLlamaRMSNorm` class is a PyTorch `nn.Module` that integrates\
    \ this kernel into a model. The constructor receives a `weight` tensor and an\
    \ `eps` value for numerical stability. The `forward` method reshapes the input\
    \ tensor into a 2D format, calculates the optimal block size, and enqueues the\
    \ Triton kernel for execution. The kernel computes the normalized output stored\
    \ in tensor `y`, ensuring high performance for inputs where the feature dimension\
    \ is less than 64KB.\n            \nThe test code is:\n\n\n# Define the test function\n\
    def test_triton_llama_rms_norm():\n    results = {}\n    \n    # Test case 1:\
    \ Small input size\n    x1 = torch.randn(2, 16, dtype=torch.float32, device=\"\
    cuda\")\n    weight1 = torch.ones(16, dtype=torch.float32, device=\"cuda\")\n\
    \    norm1 = TritonLlamaRMSNorm(weight1)\n    y1 = norm1(x1)\n    results['test_case_1']\
    \ = y1\n\n    # Test case 2: Larger input size within 64KB limit\n    x2 = torch.randn(4,\
    \ 256, dtype=torch.float32, device=\"cuda\")\n    weight2 = torch.ones(256, dtype=torch.float32,\
    \ device=\"cuda\")\n    norm2 = TritonLlamaRMSNorm(weight2)\n    y2 = norm2(x2)\n\
    \    results['test_case_2'] = y2\n\n    # Test case 3: Input size at the edge\
    \ of 64KB limit\n    x3 = torch.randn(1, 65536 // 4, dtype=torch.float32, device=\"\
    cuda\")  # 65536 bytes / 4 bytes per float\n    weight3 = torch.ones(65536 //\
    \ 4, dtype=torch.float32, device=\"cuda\")\n    norm3 = TritonLlamaRMSNorm(weight3)\n\
    \    y3 = norm3(x3)\n    results['test_case_3'] = y3\n\n    # Test case 4: Input\
    \ size exceeding 64KB limit (should raise an error)\n    try:\n        x4 = torch.randn(1,\
    \ 65536 // 4 + 1, dtype=torch.float32, device=\"cuda\")\n        weight4 = torch.ones(65536\
    \ // 4 + 1, dtype=torch.float32, device=\"cuda\")\n        norm4 = TritonLlamaRMSNorm(weight4)\n\
    \        y4 = norm4(x4)\n    except RuntimeError as e:\n        results['test_case_4']\
    \ = str(e)\n\n    return results\n\n# Run the test function\nresult_gold = test_triton_llama_rms_norm()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py rmsnorm_fused.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rmsnorm_fused
