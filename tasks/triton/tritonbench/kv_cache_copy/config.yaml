compile_command:
- python kv_cache_copy.py
correctness_command:
- python kv_cache_copy_perf.py
performance_command:
- tb_eval -f kv_cache_copy.py -o kv_cache_copy_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `_copy_to_kvcache_seqlen1_kernel` is defined to\
    \ efficiently copy elements from the input tensors `K` (keys) and `V` (values)\
    \ to the respective cache tensors `KCache` and `VCache`. It supports two types\
    \ of cache layouts: one with a four-dimensional layout `[num_blocks, num_kv_heads,\
    \ block_size, head_dim]`, and another with a five-dimensional layout `[num_blocks,\
    \ num_kv_heads, head_dim // x, block_size, x]`.\n            The kernel is parameterized\
    \ by the block size, head dimensions, and stride lengths for efficient memory\
    \ indexing. It uses Triton's `program_id` to obtain the current sequence and head\
    \ index, determining where to store the data in the cache. The `block_tables`\
    \ tensor stores mapping information of blocks for each sequence, and `context_lengths`\
    \ provides the lengths of past sequences.\n            The wrapper function `copy_kv_to_blocked_cache`\
    \ orchestrates the setup for kernel invocation, asserting the compatibility of\
    \ input shapes and deciding on the cache layout. It calculates strides and offsets\
    \ based on whether the new cache layout is used, then launches the kernel with\
    \ a calculated grid size based on batch size and number of heads.\n          \
    \  This setup ensures that during model decoding, only necessary data is efficiently\
    \ copied to cache for further operations, thereby optimizing the memory bandwidth\
    \ and computation required in subsequent steps.\n            \nThe test code is:\n\
    \n\n# Test for copy_kv_to_blocked_cache\ndef test_copy_kv_to_blocked_cache():\n\
    \    # Parameters\n    bsz = 2\n    num_kv_heads = 4\n    head_dim = 64\n    block_size\
    \ = 16\n    max_blocks_per_sequence = 10\n\n    # Inputs\n    k = torch.randn(bsz,\
    \ 1, num_kv_heads, head_dim, dtype=torch.float32, device=\"cuda\")\n    v = torch.randn(bsz,\
    \ 1, num_kv_heads, head_dim, dtype=torch.float32, device=\"cuda\")\n    k_cache\
    \ = torch.zeros(max_blocks_per_sequence, num_kv_heads, block_size, head_dim, dtype=torch.float32,\
    \ device=\"cuda\")\n    v_cache = torch.zeros(max_blocks_per_sequence, num_kv_heads,\
    \ block_size, head_dim, dtype=torch.float32, device=\"cuda\")\n    kv_lengths\
    \ = torch.tensor([5, 10], dtype=torch.int32, device=\"cuda\")\n    block_tables\
    \ = torch.randint(0, max_blocks_per_sequence, (bsz, max_blocks_per_sequence),\
    \ dtype=torch.int32, device=\"cuda\")\n\n    # Test with old kcache layout\n \
    \   copy_kv_to_blocked_cache(k, v, k_cache, v_cache, kv_lengths, block_tables,\
    \ use_new_kcache_layout=False)\n\n    # Test with new kcache layout\n    k_cache_new_layout\
    \ = torch.zeros(max_blocks_per_sequence, num_kv_heads, head_dim // 8, block_size,\
    \ 8, dtype=torch.float32, device=\"cuda\")\n    v_cache_new_layout = torch.zeros(max_blocks_per_sequence,\
    \ num_kv_heads, block_size, head_dim, dtype=torch.float32, device=\"cuda\")\n\
    \    copy_kv_to_blocked_cache(k, v, k_cache_new_layout, v_cache_new_layout, kv_lengths,\
    \ block_tables, use_new_kcache_layout=True)\n\n    # Collect results\n    results\
    \ = {\n        \"test_case_1\": (k_cache.clone(), v_cache.clone()),\n        \"\
    test_case_2\": (k_cache_new_layout.clone(), v_cache_new_layout.clone())\n    }\n\
    \    return results\n\n# Execute the test function\nresult_gold = test_copy_kv_to_blocked_cache()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py kv_cache_copy.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- kv_cache_copy
