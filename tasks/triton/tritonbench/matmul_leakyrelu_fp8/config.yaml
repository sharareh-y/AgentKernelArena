compile_command:
- python matmul_leakyrelu_fp8.py
correctness_command:
- python matmul_leakyrelu_fp8_perf.py
performance_command:
- tb_eval -f matmul_leakyrelu_fp8.py -o matmul_leakyrelu_fp8_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton operator is composed of a kernel function `matmul_kernel`\
    \ and a wrapper function `matmul`. The kernel performs matrix multiplication C\
    \ = A x B, where A, B, and C have dimensions (M, K), (K, N), and (M, N) respectively.\
    \ The kernel is optimized using Triton\u2019s block mapping, with configurable\
    \ block sizes: BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K. The operation supports\
    \ activation functions, such as leaky ReLU, defined in `leaky_relu(x)`. The wrapper\
    \ function `matmul` initializes the output matrix C, configures the execution\
    \ grid based on input dimensions, and invokes the kernel. Key parameters include\
    \ matrix dimensions and strides, alongside the activation function name, which\
    \ determines if and how the activation is applied. The process involves splitting\
    \ the computation into blocks and iterating over the K dimension to accumulate\
    \ results. Finally, results are stored back into the appropriate locations in\
    \ C using masked memory operations.\n    \nThe test code is:\n\n\ndef test_matmul():\n\
    \    results = {}\n\n    # Test case 1: Basic matrix multiplication without activation\n\
    \    a = torch.randn((256, 64), device='cuda', dtype=torch.float16)\n    b = torch.randn((64,\
    \ 256), device='cuda', dtype=torch.float16)\n    c = matmul(a, b)\n    results[\"\
    test_case_1\"] = c\n\n    # Test case 2: Matrix multiplication with leaky ReLU\
    \ activation\n    c_with_activation = matmul(a, b, activation=\"leaky_relu\")\n\
    \    results[\"test_case_2\"] = c_with_activation\n\n    # Test case 3: Matrix\
    \ multiplication with larger dimensions\n    a_large = torch.randn((512, 128),\
    \ device='cuda', dtype=torch.float16)\n    b_large = torch.randn((128, 512), device='cuda',\
    \ dtype=torch.float16)\n    c_large = matmul(a_large, b_large)\n    results[\"\
    test_case_3\"] = c_large\n\n    # Test case 4: Matrix multiplication with larger\
    \ dimensions and leaky ReLU activation\n    c_large_with_activation = matmul(a_large,\
    \ b_large, activation=\"leaky_relu\")\n    results[\"test_case_4\"] = c_large_with_activation\n\
    \n    return results\n\nresult_gold = test_matmul()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ matmul_leakyrelu_fp8.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_leakyrelu_fp8
