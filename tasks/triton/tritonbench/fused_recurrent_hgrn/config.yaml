compile_command:
- python fused_recurrent_hgrn.py
correctness_command:
- python fused_recurrent_hgrn_perf.py
performance_command:
- tb_eval -f fused_recurrent_hgrn.py -o fused_recurrent_hgrn_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The code implements a fused recurrent hierarchical gated recurrent network\
    \ (HGRN) using Triton, which provides efficient GPU kernels. It includes two main\
    \ kernels: `fused_recurrent_hgrn_fwd_kernel` for the forward pass and `fused_recurrent_hgrn_bwd_kernel`\
    \ for the backward pass.\n\n    The `fused_recurrent_hgrn_fwd_kernel` takes inputs\
    \ `x` (input sequence), `g` (gates), `o` (output storage), `h0` (optional initial\
    \ hidden state), `ht` (final state storage), and configuration constants `T`,\
    \ `D`, `BD`, `USE_INITIAL_STATE`, `STORE_FINAL_STATE`. The kernel computes the\
    \ output for each time step by iterating over the time dimension `T`, using the\
    \ formula `b_h = b_g * b_h + b_x`, where `b_x` and `b_g` are loaded from `x` and\
    \ `g`, respectively. The result is stored in `o`.\n\n    The `fused_recurrent_hgrn_bwd_kernel`\
    \ computes gradients `dx` and `dg` for the inputs `x` and `g`, given the gradient\
    \ of the output `do`. It backpropagates through time from `T-1` to `0`, updating\
    \ the gradients using `b_dh = b_dh + b_do`, `b_dx = b_dh`, `b_dg = b_dh * b_o`,\
    \ and `b_dh = b_dh * b_g`.\n\n    The `FusedRecurrentHGRNFunction` wraps these\
    \ kernels in a PyTorch `autograd.Function` to allow for forward and backward passes\
    \ during training. Its `forward` method sets up the necessary grid and calls the\
    \ forward kernel. The `backward` method retrieves saved tensors and calls the\
    \ backward kernel to compute gradients.\n\n    Finally, `fused_recurrent_hgrn`\
    \ is a user-friendly function that initializes the operation, handling input detachment\
    \ and configuring the kernels based on whether an initial state is provided and\
    \ whether the final state should be output. It returns both the computed sequence\
    \ and optionally the final state of the RNN.\n    \nThe test code is:\n\n\nimport\
    \ torch\n\ndef test_fused_recurrent_hgrn_with_backward():\n    # Define the input\
    \ dimensions\n    B, H, T, D = 1, 2, 2, 2  # Batch size, number of heads, sequence\
    \ length, feature dimension\n\n    # Create random input tensors with gradients\
    \ enabled\n    x = torch.randn(B, H, T, D, dtype=torch.float32, requires_grad=True,\
    \ device='cuda')\n    g = torch.randn(B, H, T, D, dtype=torch.float32, requires_grad=True,\
    \ device='cuda')\n\n    results = {}\n\n    # Test case 1: Without initial state,\
    \ without final state output\n    o, final_state = fused_recurrent_hgrn(x, g)\n\
    \    results['test_case_1'] = (o, final_state)\n\n    # Backward pass\n    loss\
    \ = o.sum()\n    loss.backward()\n    results['test_case_1_grad'] = (x.grad.clone(),\
    \ g.grad.clone())\n\n    # Reset gradients for next test\n    x.grad.zero_()\n\
    \    g.grad.zero_()\n\n    # Test case 2: With initial state, without final state\
    \ output\n    initial_state = torch.randn(B, H, D, dtype=torch.float32, requires_grad=False,\
    \ device='cuda')\n    o, final_state = fused_recurrent_hgrn(x, g, initial_state)\n\
    \    results['test_case_2'] = (o, final_state)\n\n    # Backward pass\n    loss\
    \ = o.sum()\n    loss.backward()\n    results['test_case_2_grad'] = (x.grad.clone(),\
    \ g.grad.clone())\n\n    # Reset gradients for next test\n    x.grad.zero_()\n\
    \    g.grad.zero_()\n\n    # Test case 3: Without initial state, with final state\
    \ output\n    o, final_state = fused_recurrent_hgrn(x, g, output_final_state=True)\n\
    \    results['test_case_3'] = (o, final_state)\n\n    # Backward pass\n    loss\
    \ = o.sum() + final_state.sum()\n    loss.backward()\n    results['test_case_3_grad']\
    \ = (x.grad.clone(), g.grad.clone())\n\n    # Reset gradients for next test\n\
    \    x.grad.zero_()\n    g.grad.zero_()\n\n    # Test case 4: With initial state,\
    \ with final state output\n    o, final_state = fused_recurrent_hgrn(x, g, initial_state,\
    \ output_final_state=True)\n    results['test_case_4'] = (o, final_state)\n\n\
    \    # Backward pass\n    loss = o.sum() + final_state.sum()\n    loss.backward()\n\
    \    results['test_case_4_grad'] = (x.grad.clone(), g.grad.clone())\n\n    return\
    \ results\n\n# Run the test\nresult_gold = test_fused_recurrent_hgrn_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fused_recurrent_hgrn.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_recurrent_hgrn
