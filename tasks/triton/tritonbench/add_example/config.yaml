compile_command:
- python add_example.py
correctness_command:
- python add_example_perf.py
performance_command:
- tb_eval -f add_example.py -o add_example_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton code provides an element-wise addition kernel named `add_kernel`\
    \ and a corresponding wrapper function `add_wrapper`. The `add_kernel` is a decorated\
    \ function with `@triton.jit`, enabling Just-In-Time compilation for optimized\
    \ GPU execution. It receives pointers to two input tensors (`in_ptr0`, `in_ptr1`),\
    \ an output tensor (`out_ptr`), the number of elements to process (`n_elements`),\
    \ and a `BLOCK_SIZE`. Within the kernel, `pid` identifies the program's ID for\
    \ the block dimension. The computation starts by calculating `block_start` as\
    \ the base index for each block using `pid * BLOCK_SIZE`, and `offsets` as a range\
    \ from `block_start` up to `BLOCK_SIZE`. A mask is applied using `offsets < n_elements`\
    \ to prevent out-of-bound memory access, loading tensor values with `tl.load`\
    \ for both input pointers. The values are summed and stored back in `out_ptr`\
    \ using `tl.store`, still respecting the mask.\n\n            The `add_wrapper`\
    \ function orchestrates the setup and execution of the kernel. It initializes\
    \ the output tensor `out` using `torch.zeros_like(x)` for similar shape and dtype.\
    \ The function determines the total number of elements using `x.numel()` and computes\
    \ the number of blocks needed with `num_blocks = (n_elements + BLOCK_SIZE - 1)\
    \ // BLOCK_SIZE`. Finally, `add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)`\
    \ launches the kernel, passing the input and output pointers, number of elements,\
    \ and block size. The final result is returned in `out`.\n            \nThe test\
    \ code is:\n\n\n# Test the kernel with appropriate inputs\ndef test_add_kernel():\n\
    \    results = {}\n    \n    # Test case 1\n    x1 = torch.randn(16, device='cuda')\n\
    \    y1 = torch.randn(16, device='cuda')\n    out1 = add_wrapper(x1, y1)\n   \
    \ results['test_case_1'] = out1\n\n    # Test case 2: Different size\n    x2 =\
    \ torch.randn(8, device='cuda')\n    y2 = torch.randn(8, device='cuda')\n    out2\
    \ = add_wrapper(x2, y2)\n    results['test_case_2'] = out2\n\n    # Test case\
    \ 3: Larger size\n    x3 = torch.randn(32, device='cuda')\n    y3 = torch.randn(32,\
    \ device='cuda')\n    out3 = add_wrapper(x3, y3)\n    results['test_case_3'] =\
    \ out3\n\n    # Test case 4: Edge case with zero elements\n    x4 = torch.randn(0,\
    \ device='cuda')\n    y4 = torch.randn(0, device='cuda')\n    out4 = add_wrapper(x4,\
    \ y4)\n    results['test_case_4'] = out4\n\n    return results\n\n# Run the test\n\
    result_gold = test_add_kernel()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ add_example.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- add_example
