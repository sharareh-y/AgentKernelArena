compile_command:
- python parallel_attention.py
correctness_command:
- python parallel_attention_perf.py
performance_command:
- tb_eval -f parallel_attention.py -o parallel_attention_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton-based custom operation consists of several key components\
    \ designed to efficiently perform parallel computations on batch tensor data,\
    \ often seen in transformer models. The main kernel functions are `parallel_rebased_fwd_kernel`\
    \ and `parallel_rebased_bwd_kernel`. The forward kernel (`parallel_rebased_fwd_kernel`)\
    \ calculates attention scores by performing matrix multiplications between query\
    \ (`q`), key (`k`), and value (`v`) tensors, with scaling and optional normalization,\
    \ and outputs the result into tensor `o`. An additional tensor `z` holds a normalization\
    \ factor computed from the scores. This kernel leverages Triton's block-wise parallelism\
    \ by dividing the computations into blocks of size defined by `BTL`, `BTS`, `BK`,\
    \ and `BV`.\n\n        The backward pass is handled by `parallel_rebased_bwd_kernel`,\
    \ which calculates gradients for input tensors using previously computed scores\
    \ and the derivative outputs (`do` and `dz`). This is further divided into two\
    \ helper functions `_parallel_rebased_bwd_dq` and `_parallel_rebased_bwd_dkv`\
    \ to compute gradients for queries (`dq`) and key-value pairs (`dk`, `dv`) respectively.\
    \ These functions utilize the block pointer API to efficiently handle memory access\
    \ patterns for large tensor dimensions.\n\n        The Triton kernel operations\
    \ are encapsulated within the `ParallelBasedFunction` class that inherits from\
    \ `torch.autograd.Function`, allowing for automatic differentiation. The `forward`\
    \ method initializes the output tensors and launches the forward kernel. The `backward`\
    \ method prepares the gradient tensors and invokes the backward kernel, returning\
    \ gradients with respect to the inputs.\n\n        The `parallel_rebased` function\
    \ is a user-facing API that sets up and applies the custom operation. It supports\
    \ optional scaling (`use_scale`) and normalization (`use_normalize`) of outputs,\
    \ with configurable options such as `return_both` to return both output and normalization\
    \ factors. It asserts that the feature dimension does not exceed 128, ensuring\
    \ compatibility with the defined Triton grid and block sizes.\n    \nThe test\
    \ code is:\n\n\nimport torch\n\ndef test_parallel_rebased_with_backward():\n \
    \   # Define the input dimensions\n    B, H, T, D_head_K, D_head_V = 2, 4, 128,\
    \ 64, 64\n\n    # Create random input tensors with requires_grad=True for backward\
    \ testing\n    q = torch.randn(B, H, T, D_head_K, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n    k = torch.randn(B, H, T, D_head_K, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n    v = torch.randn(B, H, T, D_head_V, device='cuda', dtype=torch.float32,\
    \ requires_grad=True)\n\n    results = {}\n\n    # Test case 1: Use scale and\
    \ normalize, return only output\n    output = parallel_rebased(q, k, v, eps=1e-5,\
    \ use_scale=True, use_normalize=True, return_both=False)\n    loss = output.sum()\
    \  # Define a simple loss function\n    loss.backward()  # Perform backward pass\n\
    \    results['test_case_1'] = {\n        \"output_shape\": output.shape,\n   \
    \     \"loss\": loss.item(),\n        \"grad_q\": q.grad.norm().item(),\n    \
    \    \"grad_k\": k.grad.norm().item(),\n        \"grad_v\": v.grad.norm().item()\n\
    \    }\n\n    # Reset gradients for the next test\n    q.grad.zero_()\n    k.grad.zero_()\n\
    \    v.grad.zero_()\n\n    # Test case 2: Use scale, do not normalize, return\
    \ only output\n    output = parallel_rebased(q, k, v, eps=1e-5, use_scale=True,\
    \ use_normalize=False, return_both=False)\n    loss = output.sum()\n    loss.backward()\n\
    \    results['test_case_2'] = {\n        \"output_shape\": output.shape,\n   \
    \     \"loss\": loss.item(),\n        \"grad_q\": q.grad.norm().item(),\n    \
    \    \"grad_k\": k.grad.norm().item(),\n        \"grad_v\": v.grad.norm().item()\n\
    \    }\n\n    # Reset gradients for the next test\n    q.grad.zero_()\n    k.grad.zero_()\n\
    \    v.grad.zero_()\n\n    # Test case 3: Use scale and normalize, return both\
    \ output and normalizer\n    output, normalizer = parallel_rebased(q, k, v, eps=1e-5,\
    \ use_scale=True, use_normalize=True, return_both=True)\n    loss = output.sum()\
    \ + normalizer.sum()\n    loss.backward()\n    results['test_case_3'] = {\n  \
    \      \"output_shape\": output.shape,\n        \"normalizer_shape\": normalizer.shape,\n\
    \        \"loss\": loss.item(),\n        \"grad_q\": q.grad.norm().item(),\n \
    \       \"grad_k\": k.grad.norm().item(),\n        \"grad_v\": v.grad.norm().item()\n\
    \    }\n\n    return results\n\n# Run the test cases with backward\nresult_gold\
    \ = test_parallel_rebased_with_backward()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ parallel_attention.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- parallel_attention
