compile_command:
- python masked_select.py
correctness_command:
- python masked_select_perf.py
performance_command:
- tb_eval -f masked_select.py -o masked_select_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This code snippet implements a masked select operation using Triton to\
    \ enable high-performance parallel computations on GPUs. The core logic is encapsulated\
    \ in a Triton JIT-compiled kernel `masked_select_kernel`, which processes data\
    \ in parallel across many threads organized in blocks. The kernel takes pointers\
    \ to input data (`inp_ptr`), a selection mask (`select_mask_ptr`), a prefix sum\
    \ array (`prefix_sum_ptr`), and an output buffer (`out_ptr`). Each thread computes\
    \ its global ID using `tl.program_id` and `tl.arange`, checking if it's within\
    \ the bounds of `n_elements`. It loads the input and mask values, converts the\
    \ mask to a Boolean type `tl.int1`, and calculates output positions using a prefix\
    \ sum. The selected input values are stored in the output buffer where both the\
    \ mask and bounds conditions are met. The `masked_select` function wraps the kernel\
    \ invocation, performing necessary pre-processing steps such as broadcasting tensors\
    \ to compatible shapes, flattening the mask, and computing prefix sums for determining\
    \ output positions. The kernel's grid size is dynamically computed based on the\
    \ number of elements and the block size (`BLOCK_SIZE`). The `cfggen` function\
    \ generates various Triton configurations to optimize execution by testing different\
    \ block sizes and warp numbers, enhancing performance through autotuning. This\
    \ ensures efficient parallel computation by exploring multiple execution configurations.\
    \ The `broadcastable` function provides a utility to ensure that two tensor shapes\
    \ can be broadcasted, adhering to broadcasting rules essential for element-wise\
    \ operations in multi-dimensional arrays.\n        \nThe test code is:\n\n\ndef\
    \ test_masked_select():\n    # Initialize a dictionary to store results\n    results\
    \ = {}\n\n    # Test case 9: Random mask for 2D tensor, float32\n    x_random\
    \ = torch.rand((4, 4), device='cuda', dtype=torch.float32)\n    mask_random =\
    \ torch.randint(0, 2, (4, 4), dtype=torch.bool, device='cuda')\n    result_random\
    \ = masked_select(x_random, mask_random)\n    results['test_case_0'] = result_random\n\
    \n    # Test case 3: 3D tensor, float64, mask with all True\n    x_3d = torch.rand((2,\
    \ 3, 4), dtype=torch.float64, device='cuda')\n    mask_3d = torch.ones((2, 3,\
    \ 4), dtype=torch.bool, device='cuda')\n    result_3d = masked_select(x_3d, mask_3d)\n\
    \    results['test_case_1'] = result_3d\n\n    # Test case 4: 4D tensor, int64,\
    \ mask with all False\n    x_4d = torch.randint(0, 100, (2, 2, 2, 2), dtype=torch.int64,\
    \ device='cuda')\n    mask_4d = torch.zeros((2, 2, 2, 2), dtype=torch.bool, device='cuda')\n\
    \    result_4d = masked_select(x_4d, mask_4d)\n    results['test_case_2'] = result_4d\n\
    \n\n    # Test case 13: Large tensor, float32, random mask\n    x_large = torch.rand((512,\
    \ 1024), device='cuda', dtype=torch.float32)\n    mask_large = torch.randint(0,\
    \ 2, (512, 1024), dtype=torch.bool, device='cuda')\n    result_large = masked_select(x_large,\
    \ mask_large)\n    results['test_case_3'] = result_large\n\n    return results\n\
    \nresult_gold = test_masked_select()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ masked_select.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- masked_select
