compile_command:
- python rotary_emb.py
correctness_command:
- python rotary_emb_perf.py
performance_command:
- tb_eval -f rotary_emb.py -o rotary_emb_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `_rotary_kernel` is a Triton JIT-compiled kernel aimed at applying\
    \ a rotary positional encoding to tensors `Q` and `K`. This is achieved through\
    \ the use of precomputed cosine and sine values contained in tensors `Cos` and\
    \ `Sin`. The kernel operates on blocks of heads and sequences defined by compile-time\
    \ constants `BLOCK_HEAD`, `BLOCK_SEQ`, and `BLOCK_DMODEL`.\n\n            The\
    \ kernel leverages Triton's parallel processing model by executing on a 2D grid.\
    \ Each block is tasked with processing specific portions of the input based on\
    \ the current head and sequence indices, which are derived from the program's\
    \ IDs (`tl.program_id(0)` and `tl.program_id(1)`). Strides and offsets are computed\
    \ to efficiently access and manipulate the relevant sections of `Q`, `K`, `Cos`,\
    \ and `Sin`.\n\n            The main computational steps involve:\n          \
    \  - Calculating offsets for `Q` and `K` based on their strides and the current\
    \ block indices.\n            - Loading segments of `Q` and `K` using these offsets\
    \ and applying boundary masks to avoid out-of-bounds accesses.\n            -\
    \ Applying the rotary transformation by combining the loaded values with `Cos`\
    \ and `Sin` using the rotation formulas: `out0 = q0 * cos0 - q1 * sin0` and `out1\
    \ = q0 * sin1 + q1 * cos1`.\n            - Storing the transformed segments back\
    \ to the original locations in `Q` and `K`.\n\n            The `rotary_emb_fwd`\
    \ function acts as a high-level interface for invoking the kernel. It validates\
    \ input shapes, calculates execution grid dimensions (`grid`), and determines\
    \ the number of warps to use based on the head dimension size. This function encapsulates\
    \ all preparation logic required for the kernel execution, ensuring that `Q`,\
    \ `K`, `Cos`, and `Sin` are correctly aligned with the expected input format of\
    \ `_rotary_kernel`.\n            \nThe test code is:\n\n\nimport torch\n\n# Define\
    \ the test function\ndef test_rotary_emb_fwd():\n    # Define the dimensions\n\
    \    total_len = 32  # Sequence length\n    head_num_q = 8  # Number of heads\
    \ for Q\n    head_num_k = 8  # Number of heads for K\n    head_dim = 64   # Dimension\
    \ of each head\n\n    # Create random input tensors\n    q = torch.randn((total_len,\
    \ head_num_q, head_dim), dtype=torch.float32, device='cuda')\n    k = torch.randn((total_len,\
    \ head_num_k, head_dim), dtype=torch.float32, device='cuda')\n    cos = torch.randn((total_len,\
    \ head_dim), dtype=torch.float32, device='cuda')\n    sin = torch.randn((total_len,\
    \ head_dim), dtype=torch.float32, device='cuda')\n\n    # Call the rotary embedding\
    \ function\n    rotary_emb_fwd(q, k, cos, sin)\n\n    # Store the results in a\
    \ dictionary\n    results = {}\n    results['test_case_1'] = (q.clone(), k.clone())\n\
    \n    # Additional test cases to cover more branches\n    # Test case 2: Different\
    \ head dimension\n    head_dim = 128\n    q = torch.randn((total_len, head_num_q,\
    \ head_dim), dtype=torch.float32, device='cuda')\n    k = torch.randn((total_len,\
    \ head_num_k, head_dim), dtype=torch.float32, device='cuda')\n    cos = torch.randn((total_len,\
    \ head_dim), dtype=torch.float32, device='cuda')\n    sin = torch.randn((total_len,\
    \ head_dim), dtype=torch.float32, device='cuda')\n    rotary_emb_fwd(q, k, cos,\
    \ sin)\n    results['test_case_2'] = (q.clone(), k.clone())\n\n    # Test case\
    \ 3: Partial rotary factor\n    head_dim = 64\n    partial_rotary_factor = 0.5\n\
    \    q = torch.randn((total_len, head_num_q, head_dim), dtype=torch.float32, device='cuda')\n\
    \    k = torch.randn((total_len, head_num_k, head_dim), dtype=torch.float32, device='cuda')\n\
    \    cos = torch.randn((total_len, head_dim), dtype=torch.float32, device='cuda')\n\
    \    sin = torch.randn((total_len, head_dim), dtype=torch.float32, device='cuda')\n\
    \    rotary_emb_fwd(q, k, cos, sin, partial_rotary_factor)\n    results['test_case_3']\
    \ = (q.clone(), k.clone())\n\n    # Test case 4: Different sequence length\n \
    \   total_len = 64\n    q = torch.randn((total_len, head_num_q, head_dim), dtype=torch.float32,\
    \ device='cuda')\n    k = torch.randn((total_len, head_num_k, head_dim), dtype=torch.float32,\
    \ device='cuda')\n    cos = torch.randn((total_len, head_dim), dtype=torch.float32,\
    \ device='cuda')\n    sin = torch.randn((total_len, head_dim), dtype=torch.float32,\
    \ device='cuda')\n    rotary_emb_fwd(q, k, cos, sin)\n    results['test_case_4']\
    \ = (q.clone(), k.clone())\n\n    return results\n\n# Run the test\nresult_gold\
    \ = test_rotary_emb_fwd()\n\n\nDon't append test code to the kernel code or edit\
    \ test function.\n\nThe generated code should be written into a python file.\n\
    If you have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rotary_emb.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rotary_emb
