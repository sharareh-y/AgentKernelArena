compile_command:
- python int8_matmul_quantization.py
correctness_command:
- python int8_matmul_quantization_perf.py
performance_command:
- tb_eval -f int8_matmul_quantization.py -o int8_matmul_quantization_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This Triton implementation provides a specialized routine for performing\
    \ matrix multiplication with quantized int8 matrices to leverage efficient GPU\
    \ computation. \n\n            The `quantize_int8_perrow_kernel` is a Triton JIT-compiled\
    \ kernel, decorated with multiple `triton.autotune` configurations to optimize\
    \ execution parameters such as `num_stages` and `num_warps`. It is designed to\
    \ quantize a floating-point matrix to int8 per row, by first calculating the maximum\
    \ absolute value per row and using it to scale the values to fit within the int8\
    \ range [-127, 127]. The kernel operates on a matrix `fpa` of dimensions MxK,\
    \ loading blocks of data to compute maximums and perform quantization, and stores\
    \ the resulting int8 values in matrix `a` while outputting the scale per row to\
    \ `as`.\n\n            The `quantize_int8_perrow` function serves as a Python\
    \ wrapper for this kernel. It allocates output tensors for the quantized result\
    \ and the scale factors, determines grid size based on input matrix dimensions,\
    \ and launches the Triton kernel.\n\n            The `matmul_kernel` is another\
    \ Triton JIT kernel that handles matrix multiplication C = A x B where A and B\
    \ are quantized matrices, accompanied by scaling factors `as_ptr` and `bs_ptr`.\
    \ The kernel iteratively loads sub-blocks of A and B, accumulates the results\
    \ in `accumulator`, and applies scaling factors before storing the final result\
    \ in matrix C. The kernel supports various configurations, including SPLIT_K for\
    \ advanced memory management and optimization.\n\n            The `matmul_quantize_int8`\
    \ function first applies row-wise quantization on a floating-point matrix `fpa`\
    \ and then performs multiplication using `matmul_int8`.\n\n            The `matmul_int8`\
    \ method calls `matmul_kernel` with appropriate parameters, allocating and preparing\
    \ output storage if not provided. It calculates grid size using configurable meta-parameters\
    \ `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `SPLIT_K`.\n\n            Finally, the `quantize_int8`\
    \ function handles the quantization of matrices, computing scale factors based\
    \ on maximum absolute values along the specified axis, and transforming the weight\
    \ matrix to int8, preparing it for efficient matrix operations. Outputs from these\
    \ functions include the quantized matrix and corresponding scaling factors, essential\
    \ for accurate result reconstruction after computation.\nThe test code is:\n\n\
    \n# Test case for quantize_int8 and matmul_quantize_int8\ndef test_quantize_and_matmul():\n\
    \    # Initialize test results dictionary\n    test_results = {}\n\n    # Generate\
    \ random float matrix A (fpa) and B\n    M, K, N = 128, 64, 128  # Example sizes\
    \ for A, B, and C\n    device = 'cuda'  # Ensure this runs on GPU\n\n    # Test\
    \ case 1\n    fpa = torch.randn((M, K), device=device, dtype=torch.float32)\n\
    \    b = torch.randn((K, N), device=device, dtype=torch.float32)\n    b, b_scale,\
    \ _ = quantize_int8(b)\n    c_quantized = matmul_quantize_int8(fpa, b, b_scale)\n\
    \    test_results['test_case_1'] = c_quantized\n\n    # Test case 2\n    M, K,\
    \ N = 32, 16, 32\n    fpa = torch.randn((M, K), device=device, dtype=torch.float32)\n\
    \    b = torch.randn((K, N), device=device, dtype=torch.float32)\n    b, b_scale,\
    \ _ = quantize_int8(b)\n    c_quantized = matmul_quantize_int8(fpa, b, b_scale)\n\
    \    test_results['test_case_2'] = c_quantized\n\n    return test_results\n\n\
    # Run the test case\nresult_gold = test_quantize_and_matmul()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py int8_matmul_quantization.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int8_matmul_quantization
