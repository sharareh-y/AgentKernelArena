compile_command:
- python uniform_sampling.py
correctness_command:
- python uniform_sampling_perf.py
performance_command:
- tb_eval -f uniform_sampling.py -o uniform_sampling_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `uniform_kernel` function in Triton is designed to generate uniform\
    \ random numbers on the GPU within a specified range `[from_, to)`. It accepts\
    \ several parameters: `out_ptr` is a pointer to where the result is stored, `N`\
    \ is the total number of random numbers to generate, `philox_seed` and `philox_offset`\
    \ are used for initializing the Philox random number generator, and `from_` and\
    \ `to` define the range of random numbers. The block size `BLOCK` determines the\
    \ number of threads per block.\n\n            A key part of the kernel is utilizing\
    \ Triton's Philox function, which takes the seed and offset to generate four random\
    \ numbers (`r0`, `r1`, `r2`, `r3`). These random uints are then converted to floats\
    \ in the `[0, 1)` range using `uint_to_uniform_float`, and scaled to the specified\
    \ range `[from_, to)`.\n\n            To efficiently manage memory, the kernel\
    \ uses a loop to calculate offsets (`off_0`, `off_1`, `off_2`, `off_3`) for storing\
    \ the four random numbers generated per loop iteration. Each number is conditionally\
    \ stored based on its offset relative to `N`, using Triton's `tl.store` function\
    \ with an eviction policy set to \"evict_first\". The kernel also includes heuristics\
    \ to optimize `BLOCK` size and `num_warps` based on `N`.\n\n            The `uniform_`\
    \ function is a high-level wrapper that initializes the Philox generator state\
    \ using the `philox_cuda_seed_offset` function, calculates the grid size, and\
    \ invokes the kernel with the appropriate parameters. It ensures the kernel is\
    \ executed within the context of the current CUDA device using `torch.cuda.device`.\n\
    \            \nThe test code is:\n\n\ndef test_uniform_():\n    results = {}\n\
    \n    # Test case 1: 1D tensor, float32, default from=0, to=1\n    x_1d = torch.empty((10,),\
    \ device='cuda', dtype=torch.float32)\n    uniform_(x_1d)\n    results['test_case_1']\
    \ = x_1d\n\n    # Test case 2: 2D tensor, float32, from=2, to=5\n    x_2d = torch.empty((4,\
    \ 4), device='cuda', dtype=torch.float32)\n    uniform_(x_2d, from_=2.0, to=5.0)\n\
    \    results['test_case_2'] = x_2d\n\n    # Test case 3: 3D tensor, float64, from=-1,\
    \ to=1\n    x_3d = torch.empty((2, 3, 4), device='cuda', dtype=torch.float64)\n\
    \    uniform_(x_3d, from_=-1.0, to=1.0)\n    results['test_case_3'] = x_3d\n\n\
    \    # Test case 4: Empty tensor, float32, from=0, to=1\n    x_empty = torch.empty((0,),\
    \ device='cuda', dtype=torch.float32)\n    uniform_(x_empty)\n    results['test_case_4']\
    \ = x_empty\n\n    # Test case 5: Single-element tensor, float32, from=5, to=10\n\
    \    x_single = torch.empty((1,), device='cuda', dtype=torch.float32)\n    uniform_(x_single,\
    \ from_=5.0, to=10.0)\n    results['test_case_5'] = x_single\n\n    # Test case\
    \ 6: Large tensor, float32, from=-10, to=10\n    x_large = torch.empty((1024,\
    \ 1024), device='cuda', dtype=torch.float32)\n    uniform_(x_large, from_=-10.0,\
    \ to=10.0)\n    results['test_case_6'] = x_large\n\n    # Test case 7: 1D tensor,\
    \ float16, default from=0, to=1\n    x_float16 = torch.empty((10,), device='cuda',\
    \ dtype=torch.float16)\n    uniform_(x_float16)\n    results['test_case_7'] =\
    \ x_float16\n\n    # Test case 8: Tensor with specific shapes (non-square), float32,\
    \ from=-5, to=5\n    x_shape1 = torch.empty((3, 7), device='cuda', dtype=torch.float32)\n\
    \    x_shape2 = torch.empty((5, 3, 2), device='cuda', dtype=torch.float32)\n \
    \   uniform_(x_shape1, from_=-5.0, to=5.0)\n    uniform_(x_shape2, from_=-5.0,\
    \ to=5.0)\n    results['test_case_8_1'] = x_shape1\n    results['test_case_8_2']\
    \ = x_shape2\n\n    # Test case 9: Tensor with 4D shape, float32, from=0.5, to=2.5\n\
    \    x_4d = torch.empty((2, 2, 3, 4), device='cuda', dtype=torch.float32)\n  \
    \  uniform_(x_4d, from_=0.5, to=2.5)\n    results['test_case_9'] = x_4d\n\n  \
    \  # Test case 10: Tensor with nan values, float32, checking if the range is filled\
    \ correctly\n    x_nan = torch.full((10,), float('nan'), device='cuda', dtype=torch.float32)\n\
    \    uniform_(x_nan, from_=1.0, to=2.0)\n    results['test_case_10'] = x_nan\n\
    \n    # Test case 11: Tensor with negative shape values (expecting an exception)\n\
    \    try:\n        x_negative = torch.empty((-1,), device='cuda', dtype=torch.float32)\n\
    \        uniform_(x_negative)\n    except Exception as e:\n        results['test_case_11']\
    \ = str(e)\n\n    # Test case 12: Tensor with mixed positive and negative shape\
    \ values (expecting an exception)\n    try:\n        x_invalid = torch.empty((3,\
    \ -4), device='cuda', dtype=torch.float32)\n        uniform_(x_invalid)\n    except\
    \ Exception as e:\n        results['test_case_12'] = str(e)\n\n    # Test case\
    \ 13: Very large tensor, checking if it handles allocation and computation\n \
    \   x_very_large = torch.empty((4096, 4096), device='cuda', dtype=torch.float32)\n\
    \    uniform_(x_very_large, from_=-10.0, to=10.0)\n    results['test_case_13']\
    \ = x_very_large\n\n    # Test case 14: Tensor with extreme value ranges\n   \
    \ x_extreme = torch.empty((10,), device='cuda', dtype=torch.float32)\n    uniform_(x_extreme,\
    \ from_=-1e5, to=1e5)\n    results['test_case_14'] = x_extreme\n\n    # Test case\
    \ 15: Edge case where from_ == to\n    x_equal = torch.empty((10,), device='cuda',\
    \ dtype=torch.float32)\n    uniform_(x_equal, from_=5.0, to=5.0)\n    results['test_case_15']\
    \ = x_equal\n\n    # Test case 16: Tensor with NaN, Inf values for from_ and to\
    \ (expecting exception)\n    try:\n        x_nan_inf = torch.empty((10,), device='cuda',\
    \ dtype=torch.float32)\n        uniform_(x_nan_inf, from_=float('nan'), to=float('inf'))\n\
    \    except Exception as e:\n        results['test_case_16'] = str(e)\n\n    return\
    \ results\n\nresult_gold = test_uniform_()\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ uniform_sampling.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- uniform_sampling
