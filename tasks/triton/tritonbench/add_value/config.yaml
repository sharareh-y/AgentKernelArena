compile_command:
- python add_value.py
correctness_command:
- python add_value_perf.py
performance_command:
- tb_eval -f add_value.py -o add_value_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton code defines a kernel named 'puzzle1_kernel'.\
    \ This kernel performs an element-wise addition of a constant value to each element\
    \ in the input tensor 'x'. The kernel expects the following parameters: 'x_ptr'\
    \ (a pointer to the input tensor), 'output_ptr' (a pointer to the output tensor),\
    \ 'N' (the total number of elements), 'BLOCK_SIZE' (a compile-time constant defining\
    \ the block size), and 'value' (the constant to add). Within the kernel, 'pid'\
    \ identifies the program's unique ID along the specified axis (axis=0). Each block\
    \ computes a starting index 'block_start', and 'offsets' determines the positions\
    \ within this block. A 'mask' ensures that computations only occur within valid\
    \ indices (offsets < N). The kernel loads data from 'x_ptr', adds 'value', and\
    \ stores the result in 'output_ptr'. The wrapper function 'puzzle1' initializes\
    \ an output tensor with the same shape and type as 'x'. It verifies CUDA compatibility\
    \ of input and output tensors, calculates the total number of elements 'N', defines\
    \ the grid size using the function 'grid', and invokes 'puzzle1_kernel' with specified\
    \ block size and constant value (10). It returns the output tensor that contains\
    \ the modified data.\n            \nThe test code is:\n\n\nimport torch\n\ndef\
    \ test_puzzle():\n    results = {}\n    \n    # Test case 1\n    a1 = torch.Tensor([4,\
    \ 5, 3, 2]).to(device=torch.device('cuda'))\n    triton_output1 = puzzle1(a1)\n\
    \    results['test_case_1'] = triton_output1\n    \n    # Test case 2\n    a2\
    \ = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8]).to(device=torch.device('cuda'))\n \
    \   triton_output2 = puzzle1(a2)\n    results['test_case_2'] = triton_output2\n\
    \    \n    # Test case 3\n    a3 = torch.Tensor([10, 20, 30]).to(device=torch.device('cuda'))\n\
    \    triton_output3 = puzzle1(a3)\n    results['test_case_3'] = triton_output3\n\
    \    \n    # Test case 4\n    a4 = torch.Tensor([0, -1, -2, -3]).to(device=torch.device('cuda'))\n\
    \    triton_output4 = puzzle1(a4)\n    results['test_case_4'] = triton_output4\n\
    \    \n    return results\n\nresult_gold = test_puzzle()\n\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py add_value.py {kernel_path}` to check\
    \ the correctness and performance.The kernel_path is where you stored the generated\
    \ code.\nCall Status means whether the code can be executed, Exec Status means\
    \ whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- add_value
