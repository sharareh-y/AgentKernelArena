compile_command:
- python adam_update_triton.py
correctness_command:
- python adam_update_triton_perf.py
performance_command:
- tb_eval -f adam_update_triton.py -o adam_update_triton_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The given Triton kernel named `update_fn_kernel` is designed to execute\
    \ a parameter update using gradient and momentum for optimization purposes in\
    \ a GPU environment. It operates over a 1D grid where each block handles a set\
    \ of elements defined by `BLOCK_SIZE`. The kernel begins by computing the starting\
    \ offset for each block using the program ID `pid` and `BLOCK_SIZE`. It masks\
    \ the operation to ensure that only valid elements (within `n_elements`) are processed.\n\
    \n            The kernel reads values from memory for parameters (`p_ptr`), gradients\
    \ (`grad_ptr`), and the exponential moving average of past gradients (`exp_avg_ptr`).\
    \ For each parameter, it applies step weight decay to scale down the parameter\
    \ by `(1 - lr * wd)`. It calculates the difference between the exponential average\
    \ and the current gradient, and uses this to update the parameter with a momentum\
    \ term determined by `beta1`. A sign-based adjustment is applied to the parameter\
    \ only if there's a change, driven by a conditional mask, emulating a `torch.sign`\
    \ operation.\n\n            After the parameter update, the exponential moving\
    \ average is decayed using `beta2` and the new gradient. The results for the updated\
    \ parameter and exponential average are stored back to global memory. The wrapper\
    \ function `update_fn` ensures all tensors involved are CUDA tensors and calculates\
    \ the execution grid using the total number of elements divided by `BLOCK_SIZE`.\
    \ This wrapper also sets up the execution environment, invoking the kernel with\
    \ these tensors and the relevant hyperparameters like learning rate (`lr`), weight\
    \ decay (`wd`), and momentum terms (`beta1`, `beta2`).\n            \nThe test\
    \ code is:\n\n\nimport torch\n\ndef test_update_fn():\n    # Initialize input\
    \ tensors\n    n_elements = 128\n    p1 = torch.randn(n_elements, device='cuda',\
    \ dtype=torch.float32)\n    grad1 = torch.randn(n_elements, device='cuda', dtype=torch.float32)\n\
    \    exp_avg1 = torch.zeros(n_elements, device='cuda', dtype=torch.float32)\n\n\
    \    n_elements = 1024\n    p2 = torch.randn(n_elements, device='cuda', dtype=torch.float32)\n\
    \    grad2 = torch.randn(n_elements, device='cuda', dtype=torch.float32)\n   \
    \ exp_avg2 = torch.zeros(n_elements, device='cuda', dtype=torch.float32)\n\n \
    \   # Hyperparameters\n    lr = 0.01\n    wd = 0.01\n    beta1 = 0.9\n    beta2\
    \ = 0.999\n\n    # Call the update function for different configurations\n   \
    \ update_fn(p1, grad1, exp_avg1, lr, wd, beta1, beta2)\n    update_fn(p2, grad2,\
    \ exp_avg2, lr, wd, beta1, beta2)\n\n    # Store results in a dictionary\n   \
    \ results = {\n        \"test_case_1\": (p1.clone(), exp_avg1.clone()),\n    \
    \    \"test_case_2\": (p2.clone(), exp_avg2.clone())\n    }\n\n    return results\n\
    \nresult_gold = test_update_fn()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ adam_update_triton.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- adam_update_triton
