compile_command:
- python mixed_sparse_attention.py
correctness_command:
- python mixed_sparse_attention_perf.py
performance_command:
- tb_eval -f mixed_sparse_attention.py -o mixed_sparse_attention_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This Triton operator implements a mixed sparse attention mechanism. The\
    \ primary kernel function, `_triton_mixed_sparse_attn_fwd_kernel`, processes query\
    \ (Q), key (K), and value (V) tensors, generating an output tensor (Out) by applying\
    \ block-wise sparse attention computations. It uses block indices and counts specified\
    \ by `block_count`, `block_offset`, `column_count`, and `column_index` tensors\
    \ to selectively attend to certain regions of the inputs. The inputs also include\
    \ `seqlens` for sequence lengths and `sm_scale` for scaling dot products. Within\
    \ the kernel, the algorithm computes scaled dot-products, applies a causal mask\
    \ to prevent attending to future tokens, and accumulates weighted sums to produce\
    \ the output. Blocks of the input tensors are processed iteratively, using a maximum\
    \ block and column count approach to manage computations. The `qk_scale` factor\
    \ is applied to scale the query tensor before computing attention scores. The\
    \ wrapper function `_triton_mixed_sparse_attention` prepares the input tensors\
    \ and dimensions, manages the execution grid configuration, and invokes the Triton\
    \ kernel.\n        \nThe test code is:\n\n\nimport torch\n\n# Define the test\
    \ function\ndef test_triton_mixed_sparse_attention():\n    # Parameters\n    batch_size\
    \ = 2\n    num_heads = 4\n    seq_len = 128\n    d_model = 64\n    block_size_M\
    \ = 64\n    block_size_N = 64\n    sm_scale = 0.1\n\n    # Create random input\
    \ tensors\n    q = torch.randn((batch_size, num_heads, seq_len, d_model), dtype=torch.float16,\
    \ device='cuda')\n    k = torch.randn((batch_size, num_heads, seq_len, d_model),\
    \ dtype=torch.float16, device='cuda')\n    v = torch.randn((batch_size, num_heads,\
    \ seq_len, d_model), dtype=torch.float16, device='cuda')\n    seqlens = torch.randint(low=1,\
    \ high=seq_len, size=(batch_size,), dtype=torch.int32, device='cuda')\n\n    #\
    \ Sparse pattern tensors\n    block_count = torch.randint(low=1, high=seq_len\
    \ // block_size_M, size=(batch_size, num_heads, seq_len // block_size_M), dtype=torch.int32,\
    \ device='cuda')\n    block_offset = torch.randint(low=0, high=seq_len, size=(batch_size,\
    \ num_heads, seq_len // block_size_M, 4), dtype=torch.int32, device='cuda')  #\
    \ NNZ_S = 4\n    column_count = torch.randint(low=1, high=seq_len // block_size_N,\
    \ size=(batch_size, num_heads, seq_len // block_size_M), dtype=torch.int32, device='cuda')\n\
    \    column_index = torch.randint(low=0, high=seq_len, size=(batch_size, num_heads,\
    \ seq_len // block_size_M, 8), dtype=torch.int32, device='cuda')  # NNZ_V = 8\n\
    \n    # Test case 1\n    output1 = _triton_mixed_sparse_attention(\n        q,\
    \ k, v, seqlens, block_count, block_offset, column_count, column_index, sm_scale,\
    \ block_size_M, block_size_N\n    )\n    \n    # Test case 2 (different block\
    \ size)\n    block_size_M_alt = 32\n    block_size_N_alt = 32\n    output2 = _triton_mixed_sparse_attention(\n\
    \        q, k, v, seqlens, block_count, block_offset, column_count, column_index,\
    \ sm_scale, block_size_M_alt, block_size_N_alt\n    )\n\n    # Test case 3 (different\
    \ scale)\n    sm_scale_alt = 0.2\n    output3 = _triton_mixed_sparse_attention(\n\
    \        q, k, v, seqlens, block_count, block_offset, column_count, column_index,\
    \ sm_scale_alt, block_size_M, block_size_N\n    )\n\n    # Test case 4 (different\
    \ sequence lengths)\n    seqlens_alt = torch.randint(low=1, high=seq_len, size=(batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    output4 = _triton_mixed_sparse_attention(\n\
    \        q, k, v, seqlens_alt, block_count, block_offset, column_count, column_index,\
    \ sm_scale, block_size_M, block_size_N\n    )\n\n    return {\n        \"test_case_1\"\
    : output1,\n        \"test_case_2\": output2,\n        \"test_case_3\": output3,\n\
    \        \"test_case_4\": output4,\n    }\n\n# Run the test\nresult_gold = test_triton_mixed_sparse_attention()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py mixed_sparse_attention.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- mixed_sparse_attention
