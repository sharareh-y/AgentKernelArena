compile_command:
- python int8_dequant_matmul.py
correctness_command:
- python int8_dequant_matmul_perf.py
performance_command:
- tb_eval -f int8_dequant_matmul.py -o int8_dequant_matmul_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The `_int8_matmul_rowwise_dequantize` kernel in Triton performs matrix multiplication\
    \ with int8 inputs, applying row-wise dequantization, and supports optional bias\
    \ addition. The kernel uses a block-wise computation strategy defined by the parameters\
    \ BLOCK_M, BLOCK_N, and BLOCK_K. SPLIT_K enables further parallelization across\
    \ the K dimension by dividing the work into segments. The grid size for launching\
    \ the kernel is determined by the lambda function `grid`, which calculates the\
    \ number of blocks required based on input dimensions and block sizes. The kernel\
    \ first computes a per-block matrix product using the `tl.dot` operation. It accumulates\
    \ results in an `acc` tensor of type tl.int32, which is then scaled by factors\
    \ loaded from `state_x_ptr` and `state_w_ptr`. If bias is present, it's added\
    \ post-dequantization. The final result is stored or atomically added to matrix\
    \ C based on the SPLIT_K value. The outer function `int8_matmul_rowwise_dequantize`\
    \ sets up the parameters and input tensors, handling strides and data contiguity,\
    \ and invokes the Triton kernel with optimized configurations.\n    \nThe test\
    \ code is:\n\n\ndef test_int8_matmul_rowwise_dequantize():\n    # Define test\
    \ inputs\n    test_cases = {}\n    \n    # Test case 1\n    M1, K1, N1 = 256,\
    \ 128, 256  # Example dimensions\n    a1 = torch.randint(-128, 127, (M1, K1),\
    \ dtype=torch.int8, device='cuda')\n    b1 = torch.randint(-128, 127, (K1, N1),\
    \ dtype=torch.int8, device='cuda')\n    state_x1 = torch.rand(M1, dtype=torch.float32,\
    \ device='cuda')\n    state_w1 = torch.rand(N1, dtype=torch.float32, device='cuda')\n\
    \    bias1 = torch.rand(N1, dtype=torch.float16, device='cuda')  # Optional, can\
    \ be None\n\n    # Call the wrapper function\n    c1 = int8_matmul_rowwise_dequantize(a1,\
    \ b1, state_x1, state_w1, bias1)\n    test_cases['test_case_1'] = c1\n\n    #\
    \ Test case 2: No bias\n    M2, K2, N2 = 128, 64, 128\n    a2 = torch.randint(-128,\
    \ 127, (M2, K2), dtype=torch.int8, device='cuda')\n    b2 = torch.randint(-128,\
    \ 127, (K2, N2), dtype=torch.int8, device='cuda')\n    state_x2 = torch.rand(M2,\
    \ dtype=torch.float32, device='cuda')\n    state_w2 = torch.rand(N2, dtype=torch.float32,\
    \ device='cuda')\n    bias2 = None\n\n    # Call the wrapper function\n    c2\
    \ = int8_matmul_rowwise_dequantize(a2, b2, state_x2, state_w2, bias2)\n    test_cases['test_case_2']\
    \ = c2\n\n    return test_cases\n\nresult_gold = test_int8_matmul_rowwise_dequantize()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py int8_dequant_matmul.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- int8_dequant_matmul
