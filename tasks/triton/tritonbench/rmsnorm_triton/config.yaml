compile_command:
- python rmsnorm_triton.py
correctness_command:
- python rmsnorm_triton_perf.py
performance_command:
- tb_eval -f rmsnorm_triton.py -o rmsnorm_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel function `rmsnorm_triton` is designed to perform\
    \ RMS normalization on a given 3D tensor `x`, which is stored in `x_ptr`. The\
    \ normalization is applied over the last dimension of `x` with a specified size\
    \ `N_SIZE`. The function takes pointers to the input tensor `x_ptr`, the RMS normalization\
    \ weights `rms_w_ptr`, and the output tensor `output_ptr`. It also requires the\
    \ strides for input tensor, weight, and output tensor, as well as the constants\
    \ `N_SIZE`, `eps`, and `BLOCK_N_SIZE`.\n\n            Within the kernel, `pid_batch`\
    \ and `pid_m` identify the current batch and row of the matrix being processed.\
    \ The kernel calculates the variance by iterating over chunks of size `BLOCK_N_SIZE`\
    \ and accumulating the sum of squares of the elements. This is divided by `N_SIZE`\
    \ to get the variance, from which the reciprocal of the standard deviation `rstd`\
    \ is derived using an epsilon `eps` for stability.\n\n            The function\
    \ then proceeds to normalize the input, scale by the weights, and write the output\
    \ back to `output_ptr`. The iteration over chunks ensures efficient memory access\
    \ patterns. The wrapper function `rmsnorm_triton_wrapper` sets up the input data,\
    \ initializes the output tensor, and launches the kernel with the appropriate\
    \ grid dimensions based on batch size and the second dimension M of the input\
    \ tensor.\n            \nThe test code is:\n\n\ndef test_rmsnorm_triton():\n \
    \   results = {}\n    \n    # Case 1\n    batch, M, K = 2, 4, 1024\n    x = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    rms_w = torch.randn((K,), dtype=torch.float16,\
    \ device='cuda')\n    eps = 1e-6\n    out = rmsnorm_triton_wrapper(x, rms_w, eps)\n\
    \    results['test_case_1'] = out\n\n    # Case 2: Different eps value\n    eps\
    \ = 1e-5\n    out = rmsnorm_triton_wrapper(x, rms_w, eps)\n    results['test_case_2']\
    \ = out\n\n    # Case 3: Different batch size\n    batch, M, K = 3, 4, 1024\n\
    \    x = torch.randn((batch, M, K), dtype=torch.float16, device='cuda')\n    rms_w\
    \ = torch.randn((K,), dtype=torch.float16, device='cuda')\n    eps = 1e-6\n  \
    \  out = rmsnorm_triton_wrapper(x, rms_w, eps)\n    results['test_case_3'] = out\n\
    \n    # Case 4: Different M size\n    batch, M, K = 2, 5, 1024\n    x = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    rms_w = torch.randn((K,), dtype=torch.float16,\
    \ device='cuda')\n    eps = 1e-6\n    out = rmsnorm_triton_wrapper(x, rms_w, eps)\n\
    \    results['test_case_4'] = out\n\n    return results\n\nresult_gold = test_rmsnorm_triton()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py rmsnorm_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rmsnorm_triton
