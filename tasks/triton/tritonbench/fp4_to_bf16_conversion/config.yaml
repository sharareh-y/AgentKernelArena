compile_command:
- python fp4_to_bf16_conversion.py
correctness_command:
- python fp4_to_bf16_conversion_perf.py
performance_command:
- tb_eval -f fp4_to_bf16_conversion.py -o fp4_to_bf16_conversion_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The Triton kernel and function provided are designed to efficiently convert packed\
    \ fp4 values to bf16 format while applying a scaling factor represented in e8m0\
    \ format. \n\nKernel `triton_f4_to_scaled_bf16_kernel`:\n- Function: Convert 4-bit\
    \ floating-point numbers to bfloat16 with scaling.\n- Inputs: \n  - `x_ptr`: Pointer\
    \ to packed 4-bit floats.\n  - `s_ptr`: Pointer to scaling factors in e8m0 format.\n\
    \  - `output_ptr`: Pointer where results are stored.\n  - `n_elements_in`: Number\
    \ of packed elements to process.\n  - Constant expressions for masks and biases\
    \ used in conversion logic (e.g., `SIGN_MASK_F4`).\n- Outputs: Scaled bf16 tensor.\n\
    - Process: \n  - Decomposes each packed byte into two 4-bit numbers.\n  - Adjusts\
    \ the sign and mantissa bits to convert to bf16, applying bitwise operations.\n\
    \  - Corrects for special cases like zero and denormals with masks.\n  - Uses\
    \ Triton-specific constructs for parallel processing, calculating the correct\
    \ output block and grid sizes.\n\nWrapper Function `triton_f4_to_scaled_bf16`:\n\
    - Function: Setup and launch the Triton kernel with correct parameters.\n- Inputs:\n\
    \  - `x`: The tensor of packed 4-bit floats.\n  - `s_e8m0`: Scaling factor tensor.\n\
    \  - `mx_block_size`: Block size for processing.\n- Outputs: Tensor with bf16\
    \ values, scaled.\n- Implementation:\n  - Determines shape and size of output\
    \ tensor.\n  - Ensures input tensor is contiguous and on the CUDA device.\n  -\
    \ Computes grid configuration using the number of input elements.\n  - Launches\
    \ `triton_f4_to_scaled_bf16_kernel` with the computed grid and configuration parameters,\
    \ ensuring efficient conversion and scaling of input data.\n\nThe test code is:\n\
    \n\ndef test_triton_f4_to_scaled_bf16():\n    device = 'cuda'\n    mx_block_size\
    \ = 32\n    n_elements_in = 1024\n\n    # \u521B\u5EFA\u4E00\u4E2A uint8 \u5F20\
    \u91CF\uFF0C\u6BCF\u4E2A\u5143\u7D20\u5305\u542B\u4E24\u4E2A fp4\uFF0C\u6545\u8F93\
    \u51FA\u5927\u5C0F\u5C06\u4F1A\u662F n_elements_in * 2\n    x = torch.randint(0,\
    \ 256, (n_elements_in,), dtype=torch.uint8, device=device)\n\n    # \u6839\u636E\
    \u5185\u6838\u903B\u8F91:\n    # n_elements_out = n_elements_in * 2\n    # n_elements_s\
    \ = n_elements_out // 32\n    # \u8FD9\u91CC\u662F 2048 // 32 = 64\n    n_elements_out\
    \ = n_elements_in * 2\n    n_elements_s = n_elements_out // 32\n\n    # \u521B\
    \u5EFA s_e8m0 \u5F20\u91CF\uFF0C\u5047\u8BBE\u5176\u4E3A\u968F\u673A\u6574\u6570\
    \u8303\u56F4[0, 255] (e8m0\u683C\u5F0F)\n    # \u5B9E\u9645\u4F7F\u7528\u4E2D\u5E94\
    \u4F9D\u636E\u60A8\u7684\u573A\u666F\u63D0\u4F9B\u5408\u9002\u7684scale\u503C\n\
    \    s_e8m0 = torch.randint(0, 256, (n_elements_s,), dtype=torch.uint8, device=device)\n\
    \n    # \u5206\u652F1: BLOCK_SIZE_IN = 128\n    output1 = triton_f4_to_scaled_bf16(x,\
    \ s_e8m0, mx_block_size)\n\n    # \u5206\u652F2: BLOCK_SIZE_IN = 256\n    output2\
    \ = triton_f4_to_scaled_bf16(x, s_e8m0, mx_block_size)\n\n    # \u5206\u652F3:\
    \ BLOCK_SIZE_IN = 512\n    output3 = triton_f4_to_scaled_bf16(x, s_e8m0, mx_block_size)\n\
    \n    # \u5206\u652F4: BLOCK_SIZE_IN = 1024\n    output4 = triton_f4_to_scaled_bf16(x,\
    \ s_e8m0, mx_block_size)\n\n    # \u5206\u652F5: BLOCK_SIZE_IN = 2048\n    output5\
    \ = triton_f4_to_scaled_bf16(x, s_e8m0, mx_block_size)\n\n    # \u5C06\u6BCF\u4E2A\
    \u5206\u652F\u7684\u7ED3\u679C\u4FDD\u5B58\u5728\u5B57\u5178\u4E2D\n    results\
    \ = {\n        \"test_case_1\": output1,\n        \"test_case_2\": output2,\n\
    \        \"test_case_3\": output3,\n        \"test_case_4\": output4,\n      \
    \  \"test_case_5\": output5,\n    }\n\n    return results\n\nresult_gold = test_triton_f4_to_scaled_bf16()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fp4_to_bf16_conversion.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fp4_to_bf16_conversion
