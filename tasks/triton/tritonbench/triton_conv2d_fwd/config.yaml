compile_command:
- python triton_conv2d_fwd.py
correctness_command:
- python triton_conv2d_fwd_perf.py
performance_command:
- tb_eval -f triton_conv2d_fwd.py -o triton_conv2d_fwd_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton function `conv2d_forward_kernel` executes a 2D convolution,\
    \ computing an output feature map by sliding a filter across the input data.\n\
    \            The kernel uses parallel processing to divide the workload over multiple\
    \ threads, facilitated by Triton's block-based computation.\n            The kernel\
    \ requires pointers to input, weight, and output arrays, along with dimensions\
    \ and strides for these arrays. \n            Additional parameters include the\
    \ kernel size, stride, padding, groups, and flags for computation precision like\
    \ FP16 and TF32.\n            The block size constants for batch, input features,\
    \ and output features control how the computation is partitioned among threads.\n\
    \            The `conv2d_forward` function wraps this kernel, preparing the input\
    \ and output tensors and calculating dimensions and strides.\n            The\
    \ function computes the output height and width using the convolution formula\
    \ and initializes an empty output tensor.\n            It then computes the necessary\
    \ block and grid sizes for launching the Triton kernel, which performs the main\
    \ computation.\n            After execution, the function returns the computed\
    \ output tensor.\n            \nThe test code is:\n\n\n# Test cases\ndef test_conv2d_forward():\n\
    \    results = {}\n\n    # Test case 1: Basic test\n    input_tensor = torch.randn(1,\
    \ 3, 32, 32, device='cuda', dtype=torch.float32)\n    weight_tensor = torch.randn(16,\
    \ 3, 3, 3, device='cuda', dtype=torch.float32)\n    output_tensor = conv2d_forward(input_tensor,\
    \ weight_tensor, 3, 3, 1, 1, 0, 0, 1)\n    results[\"test_case_1\"] = output_tensor\n\
    \n    # Test case 2: With padding and stride\n    input_tensor = torch.randn(1,\
    \ 3, 32, 32, device='cuda', dtype=torch.float32)\n    weight_tensor = torch.randn(16,\
    \ 3, 3, 3, device='cuda', dtype=torch.float32)\n    output_tensor = conv2d_forward(input_tensor,\
    \ weight_tensor, 3, 3, 2, 2, 1, 1, 1)\n    results[\"test_case_2\"] = output_tensor\n\
    \n    # Test case 3: With groups\n    input_tensor = torch.randn(1, 6, 32, 32,\
    \ device='cuda', dtype=torch.float32)\n    weight_tensor = torch.randn(16, 3,\
    \ 3, 3, device='cuda', dtype=torch.float32)\n    output_tensor = conv2d_forward(input_tensor,\
    \ weight_tensor, 3, 3, 1, 1, 0, 0, 2)\n    results[\"test_case_3\"] = output_tensor\n\
    \n    # Test case 4: Different kernel size\n    input_tensor = torch.randn(1,\
    \ 3, 32, 32, device='cuda', dtype=torch.float32)\n    weight_tensor = torch.randn(16,\
    \ 3, 5, 5, device='cuda', dtype=torch.float32)\n    output_tensor = conv2d_forward(input_tensor,\
    \ weight_tensor, 5, 5, 1, 1, 0, 0, 1)\n    results[\"test_case_4\"] = output_tensor\n\
    \n    return results\n\n# Execute test\nresult_gold = test_conv2d_forward()\n\n\
    \nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py triton_conv2d_fwd.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- triton_conv2d_fwd
