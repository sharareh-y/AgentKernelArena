compile_command:
- python context_attn_bloom.py
correctness_command:
- python context_attn_bloom_perf.py
performance_command:
- tb_eval -f context_attn_bloom.py -o context_attn_bloom_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton kernel is designed to perform the forward pass\
    \ of a context attention mechanism, primarily used in Transformer architectures.\
    \ This kernel is tailored for execution on a GPU, utilizing Triton's capabilities\
    \ for efficient parallel processing.\n\n            The key Triton function, '_fwd_kernel',\
    \ is executed over a grid defined by batch, head, and input length dimensions.\
    \ It processes multiple queries, keys, and values, computing attention scores\
    \ and subsequently deriving an output tensor. This function supports batched and\
    \ multi-headed attention, allowing flexibility in model architectures.\n\n   \
    \         Within '_fwd_kernel', queries (Q) are loaded for each block, and their\
    \ dot product with keys (K) is calculated. This product is scaled by a factor\
    \ derived from the head dimension, followed by the application of the softmax\
    \ function to produce attention weights. These weights are then multiplied with\
    \ values (V) to accumulate the output, which represents the weighted sum based\
    \ on attention scores.\n\n            Special attention is given to handling different\
    \ sequence lengths, batching, and the multi-head structure, with each kernel instance\
    \ operating independently across these dimensions. Stride parameters ensure that\
    \ memory is accessed correctly based on input tensor shapes.\n\n            The\
    \ 'context_attention_fwd' function orchestrates this process by setting up the\
    \ necessary kernel arguments, computing grid dimensions based on the input size,\
    \ and selecting appropriate block sizes for optimal performance. It accounts for\
    \ hardware specifics, such as different configurations for Tesla GPUs, ensuring\
    \ that the kernel runs efficiently across various setups.\n            \nThe test\
    \ code is:\n\n\nimport torch\nimport numpy as np\n\ndef test_context_attention_fwd():\n\
    \    Z, H, N_CTX, D_HEAD = 10, 6, 500, 96\n    dtype = torch.float16\n    Z =\
    \ 1\n    q = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\"\
    ).normal_(mean=0.1, std=0.2)\n    k = torch.empty((Z * N_CTX + 7000, H, D_HEAD),\
    \ dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2)\n    v = torch.empty((Z\
    \ * N_CTX + 7000, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3,\
    \ std=0.2)\n    o = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.3, std=0.2)\n    req_to_token_indexs = torch.zeros((10,\
    \ Z * N_CTX + 7000), dtype=torch.int32, device=\"cuda\")\n    max_input_len =\
    \ N_CTX\n    Z = 1\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32, device=\"\
    cuda\")\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"cuda\")\n\
    \    b_req_idx = torch.ones((Z,), dtype=torch.int32, device=\"cuda\")\n    b_prompt_cache_len\
    \ = torch.zeros(1, dtype=torch.int32, device=\"cuda\")\n    b_prompt_cache_len[0]\
    \ = 0\n    prompt_cache_len = 0\n\n    b_seq_len[0] = 500\n    b_req_idx[0] =\
    \ 0\n    req_to_token_indexs[0][: prompt_cache_len + N_CTX] = torch.tensor(\n\
    \        np.arange(prompt_cache_len + N_CTX), dtype=torch.int32\n    ).cuda()\n\
    \n    result_gold = context_attention_fwd(\n        q,\n        k,\n        v,\n\
    \        o,\n        b_req_idx,\n        b_start_loc,\n        b_seq_len + prompt_cache_len,\n\
    \        b_prompt_cache_len,\n        max_input_len,\n        req_to_token_indexs,\n\
    \    )\n    return result_gold\n\nresult_gold = test_context_attention_fwd()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py context_attn_bloom.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- context_attn_bloom
