compile_command:
- python l2_norm_triton2.py
correctness_command:
- python l2_norm_triton2_perf.py
performance_command:
- tb_eval -f l2_norm_triton2.py -o l2_norm_triton2_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided code defines a Triton-accelerated L2 normalization routine,\
    \ consisting of two parts: a forward pass `_l2_norm_fwd` and a backward pass `_l2_norm_bwd`.\
    \ Both parts utilize their respective kernels `_l2_norm_fwd_1pass_kernel` and\
    \ `_l2_norm_bwd_kernel` for GPU-accelerated computations.\n\n            - `_l2_norm_fwd_1pass_kernel`:\
    \ This function is a Triton kernel that performs the forward L2 normalization\
    \ on a per-row basis for a 2D input tensor `X`. The input tensor is expected to\
    \ have its last dimension stride as 1. The kernel computes the L2 norm by:\n \
    \             * Loading a block of data from the current row specified by `row\
    \ = tl.program_id(0)`.\n              * Calculating the sum of squares to obtain\
    \ the variance.\n              * Computing the reciprocal square root of the variance\
    \ (rstd) to normalize `x`.\n              * Storing the normalized result into\
    \ the output tensor `Y`.\n\n            - `_l2_norm_bwd_kernel`: This kernel performs\
    \ the backward pass necessary for gradient computation during L2 normalization.\
    \ It takes the input tensor `X`, its gradients `DY`, and outputs the gradient\
    \ with respect to `X` in `DX`. The computations include:\n              * Loading\
    \ the inputs and gradients for the row defined by `row = tl.program_id(0)`.\n\
    \              * Using the precomputed variance and rstd from the forward pass\
    \ to determine gradients.\n              * Applying normalization logic to compute\
    \ the gradient of the loss with respect to the input `X`.\n              * Storing\
    \ the computed gradient into `DX`.\n\n            Both `_l2_norm_fwd` and `_l2_norm_bwd`\
    \ handle tensor reshaping and preparation before kernel invocation. They also\
    \ enforce a constraint on the feature dimension size, ensuring it remains below\
    \ 64KB to fit Triton's kernel memory limits. These routines are designed to work\
    \ in environments with PyTorch and Triton, aiming at efficient parallel execution\
    \ on modern GPUs.\n            \nThe test code is:\n\n\nimport torch\n\ndef test_l2_norm_triton():\n\
    \    # Test parameters\n    batch_size, dim = 8, 128  # Define dimensions for\
    \ test tensor\n    eps = 1e-6\n\n    # Initialize input tensor\n    x = torch.randn((batch_size,\
    \ dim), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n    \n    #\
    \ Dictionary to store test results\n    test_results = {}\n\n    # Forward pass\
    \ test\n    y = _l2_norm_fwd(x, eps=eps)\n    test_results[\"test_case_1\"] =\
    \ y\n\n    # Backward pass test\n    dy = torch.ones_like(y, device=\"cuda\")\n\
    \    dx_analytical = _l2_norm_bwd(x, dy, eps=eps)\n    test_results[\"test_case_2\"\
    ] = dx_analytical\n\n    return test_results\n\nresult_gold = test_l2_norm_triton()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py l2_norm_triton2.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- l2_norm_triton2
