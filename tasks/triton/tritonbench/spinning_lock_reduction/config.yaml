compile_command:
- python spinning_lock_reduction.py
correctness_command:
- python spinning_lock_reduction_perf.py
performance_command:
- tb_eval -f spinning_lock_reduction.py -o spinning_lock_reduction_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The function `spinning_lock_kernel` is a Triton kernel that performs a reduction\
    \ operation using a spinning lock \n    mechanism for synchronization. This kernel\
    \ takes input pointers `P`, `C`, and `locks`, with integer parameters \n    `num_sms`,\
    \ `k`, `M`, `N`, `stride_cm`, and `stride_cn`, and two compile-time constants\
    \ `BLOCK_SIZE_M` and \n    `BLOCK_SIZE_N`. Each block is identified by a unique\
    \ program id `pid`, which determines its position in a virtual \n    grid through\
    \ `pid_m` and `pid_n`. The kernel initializes an accumulator `acc` to zeros. It\
    \ then loops up to 9 times, \n    performing accumulation only for the first thread\
    \ (when `pid % k == 0`) by iterating over other participating threads \n    and\
    \ accumulating their data if successful in acquiring the lock via `tl.atomic_cas`.\
    \ Threads that don't participate \n    directly in reduction (i.e., where `pid\
    \ % k != 0`) store the accumulated result in `P` and release the lock using \n\
    \    `tl.atomic_xchg`. Finally, the results are written to `C` using computed\
    \ offsets, and a mask ensures that only valid \n    memory locations are accessed.\
    \ The `spinning_lock` function orchestrates the kernel launch, preparing the execution\
    \ \n    grid according to `num_sms` and invoking the kernel with the specified\
    \ block sizes.\n    \nThe test code is:\n\n\ndef test_spinning_lock():\n    #\
    \ Parameters\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    M = 1024\n \
    \   N = 1024\n    num_sms = 304\n    k = 3\n\n    # Initialize tensors\n    P\
    \ = torch.zeros((num_sms * BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=torch.float32, device='cuda')\n\
    \    C = torch.zeros((M, N), dtype=torch.float32, device='cuda')\n    locks =\
    \ torch.zeros(num_sms, dtype=torch.int32, device='cuda')\n\n    stride_cm = C.stride(0)\n\
    \    stride_cn = C.stride(1)\n\n    # Run the Triton kernel for different branches\n\
    \    result = {}\n\n    # Test case 1: pid % k == 0\n    spinning_lock(P, C, locks,\
    \ num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N)\n    result['test_case_1']\
    \ = C.clone()\n\n    # Test case 2: pid % k != 0\n    k = 2  # Change k to ensure\
    \ pid % k != 0 for some pids\n    spinning_lock(P, C, locks, num_sms, k, M, N,\
    \ stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N)\n    result['test_case_2']\
    \ = C.clone()\n\n    # Test case 3: num_sms < pid + k\n    num_sms = 2  # Reduce\
    \ num_sms to ensure num_sms < pid + k\n    spinning_lock(P, C, locks, num_sms,\
    \ k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N)\n    result['test_case_3']\
    \ = C.clone()\n\n    # Test case 4: next_pid < pid + k and next_pid < num_sms\n\
    \    num_sms = 5  # Adjust num_sms to ensure next_pid < pid + k and next_pid <\
    \ num_sms\n    spinning_lock(P, C, locks, num_sms, k, M, N, stride_cm, stride_cn,\
    \ BLOCK_SIZE_M, BLOCK_SIZE_N)\n    result['test_case_4'] = C.clone()\n\n    return\
    \ result\n\nresult_gold = test_spinning_lock()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ spinning_lock_reduction.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- spinning_lock_reduction
