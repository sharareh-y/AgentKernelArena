compile_command:
- python block_sparse_attn.py
correctness_command:
- python block_sparse_attn_perf.py
performance_command:
- tb_eval -f block_sparse_attn.py -o block_sparse_attn_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The function `block_sparse_attention_kernel` is a Triton GPU kernel designed\
    \ to perform block-sparse attention for transformer models in the context of prompt\
    \ processing. It assumes a past sequence length of zero and right-padding only.\
    \ The kernel takes as inputs the query tensor Q, key tensor K, value tensor V,\
    \ and layout data in CSR (Compressed Sparse Row) format, specifically `layout_csr_row_indices`\
    \ and `layout_csr_col_indices`. The kernel executes in a grid determined by the\
    \ number of query blocks and the product of batch size and number of heads. It\
    \ utilizes offsets to correctly index into these tensors, leveraging the block\
    \ sizes BLOCK_M, BLOCK_N, and BLOCK_D. Each block's indices in the query (Q),\
    \ key (K), and value (V) tensors are calculated, and memory is managed using these\
    \ indices to perform the necessary loads. The kernel performs key calculations\
    \ including computing the dot product between query and key blocks (qk), scaling\
    \ by `softmax_scale`, and adjusting using softmax to prevent overflow. The results\
    \ are accumulated over the required dimensions to produce the final attention-weighted\
    \ values which are stored in the output tensor `out`. Support for multi-head attention\
    \ is built-in, with special handling for head grouping based on `num_heads` and\
    \ `num_kv_heads`. If multiple data blocks are present (NUM_D_BLOCKS > 1), additional\
    \ accumulation is performed to account for the segmented data.\n    \nThe test\
    \ code is:\n\n\nimport torch\n\n# Test cases\ndef test_block_sparse_attention():\n\
    \    # Define test parameters\n    B, H, M, D = 2, 4, 16, 32\n    N = 32\n   \
    \ num_kv_heads = 2\n    BLOCK_M = 16\n    BLOCK_N = 16\n    BLOCK_D = 16\n   \
    \ NUM_D_BLOCKS = 2\n    EVEN_M = True\n    EVEN_N = True\n    num_layout = 1\n\
    \    softmax_scale = 1.0\n\n    # Create random input tensors\n    Q = torch.randn((B,\
    \ H, M, D), device='cuda', dtype=torch.float32)\n    K = torch.randn((B, num_kv_heads,\
    \ N, D), device='cuda', dtype=torch.float32)\n    V = torch.randn((B, num_kv_heads,\
    \ N, D), device='cuda', dtype=torch.float32)\n\n    # Create layout indices\n\
    \    layout_csr_row_indices = torch.tensor([0, 2, 4], device='cuda', dtype=torch.int32)\n\
    \    layout_csr_col_indices = torch.tensor([0, 1, 2, 3], device='cuda', dtype=torch.int32)\n\
    \    layout_csr_row_stride_h = 3\n    layout_csr_col_stride_h = 4\n\n    # Call\
    \ the wrapper function for the first test case\n    out1 = block_sparse_attention(\n\
    \        Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h,\
    \ layout_csr_col_stride_h,\n        num_layout, softmax_scale, H, num_kv_heads,\
    \ M, BLOCK_M, EVEN_M, BLOCK_N, EVEN_N, BLOCK_D, NUM_D_BLOCKS\n    )\n\n    # Modify\
    \ parameters for additional test cases\n    EVEN_M = False\n    EVEN_N = False\n\
    \n    # Call the wrapper function for the second test case\n    out2 = block_sparse_attention(\n\
    \        Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h,\
    \ layout_csr_col_stride_h,\n        num_layout, softmax_scale, H, num_kv_heads,\
    \ M, BLOCK_M, EVEN_M, BLOCK_N, EVEN_N, BLOCK_D, NUM_D_BLOCKS\n    )\n\n    # Return\
    \ results in a dictionary\n    return {\n        \"test_case_1\": out1,\n    \
    \    \"test_case_2\": out2\n    }\n\n# Run the test\nresult_gold = test_block_sparse_attention()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py block_sparse_attn.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- block_sparse_attn
