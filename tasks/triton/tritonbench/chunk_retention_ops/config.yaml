compile_command:
- python chunk_retention_ops.py
correctness_command:
- python chunk_retention_ops_perf.py
performance_command:
- tb_eval -f chunk_retention_ops.py -o chunk_retention_ops_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This code implements the forward and backward pass of a chunk retention\
    \ operation using the Triton framework to optimize tensor computations. \n\n \
    \           It consists of multiple Triton kernel definitions and their Python\
    \ wrappers for executing these kernels with specified parameters. \n\n       \
    \     - `chunk_retention_fwd_kernel_h` is used in the forward pass to compute\
    \ hidden states. It iteratively processes input tensors `k` and `v` across specified\
    \ chunks (size `BT`) and updates the hidden state tensor `h`, optionally using\
    \ an initial state and storing the final state. It takes into account block indices\
    \ and handles boundaries with `boundary_check`.\n\n            - `chunk_retention_fwd_kernel_o`\
    \ computes the final output tensor by combining `q`, `k`, and `v`, applying scaling,\
    \ and handling chunk-wise processing with decay factors for block-wise computation.\n\
    \n            - `chunk_retention_bwd_kernel_dh` calculates gradients for hidden\
    \ states during the backward pass, processing `q`, `do`, and updating the gradient\
    \ tensor `dh`.\n\n            - `chunk_retention_bwd_kernel_dqkv` computes gradients\
    \ for the input tensors `q`, `k`, and `v` by processing `do`, `dh`, and hidden\
    \ state `h`.\n\n            These kernels are executed by the Python functions:\n\
    \            - `chunk_fwd_h_fn`: Sets up and calls `chunk_retention_fwd_kernel_h`.\n\
    \            - `chunk_fwd_o_fn`: Sets up and calls `chunk_retention_fwd_kernel_o`.\n\
    \            - `chunk_bwd_dh_fn`: Sets up and calls `chunk_retention_bwd_kernel_dh`.\n\
    \            - `chunk_bwd_dqkv_fn`: Sets up and calls `chunk_retention_bwd_kernel_dqkv`.\n\
    \n            `ChunkRetentionFunction` is a custom PyTorch `autograd.Function`\
    \ that utilizes these kernels for differentiable operations. It manages context\
    \ saving and restores for gradient computations. The `forward` method initializes\
    \ computations while the `backward` method orchestrates the gradient computations.\n\
    \n            The `chunk_retention` function offers a user-friendly interface,\
    \ allowing the operation to be performed on input tensors with options for initial\
    \ and final states, scaling, and checkpointing, facilitating both forward and\
    \ backward passes efficiently.\n            \nThe test code is:\n\n\nimport torch\n\
    \ndef test_chunk_retention_with_backward():\n    # Define dimensions\n    B, H,\
    \ T, K, V = 2, 4, 128, 64, 64\n\n    # Create random input tensors\n    q = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, requires_grad=True, device='cuda')\n    k = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, requires_grad=True, device='cuda')\n    v = torch.randn(B,\
    \ H, T, V, dtype=torch.float32, requires_grad=True, device='cuda')\n\n    # Test\
    \ case 1: Without initial state and without final state output\n    o, _ = chunk_retention(q,\
    \ k, v, output_final_state=False, checkpoint_level=0)\n    loss = o.sum()  # Define\
    \ a simple loss function\n    loss.backward()  # Perform backward pass\n\n   \
    \ # Reset gradients for the next test\n    q.grad.zero_()\n    k.grad.zero_()\n\
    \    v.grad.zero_()\n\n    # Test case 2: With initial state and final state output\n\
    \    initial_state = torch.randn(B, H, K, V, dtype=torch.float32, requires_grad=True,\
    \ device='cuda')\n    o, final_state = chunk_retention(q, k, v, initial_state=initial_state,\
    \ output_final_state=True, checkpoint_level=1)\n    loss = o.sum() + final_state.sum()\n\
    \    loss.backward()\n\n    # Reset gradients for the next test\n    q.grad.zero_()\n\
    \    k.grad.zero_()\n    v.grad.zero_()\n\n    # Test case 3: Different checkpoint\
    \ levels\n    for checkpoint_level in [0, 1]:\n        o, _ = chunk_retention(q,\
    \ k, v, output_final_state=False, checkpoint_level=checkpoint_level)\n       \
    \ loss = o.sum()\n        loss.backward()\n        q.grad.zero_()\n        k.grad.zero_()\n\
    \        v.grad.zero_()\n\n    # Test case 4: Verify all kernels are executed\
    \ correctly\n    h, _ = chunk_fwd_h_fn(k, v, BT=64, initial_state=None, output_final_state=False)\n\
    \    o = chunk_fwd_o_fn(h, q, k, v, BT=64, scale=0.1)\n    dh = chunk_bwd_dh_fn(o,\
    \ q, k, v, BT=64, scale=0.1)\n    dq, dk, dv = chunk_bwd_dqkv_fn(o, q, k, v, h,\
    \ dh, scale=0.1)\n\n    # Collect results in a dictionary\n    results = {\n \
    \       \"test_case_1\": (o.shape, loss.item()),\n        \"test_case_2\": (o.shape,\
    \ final_state.shape, loss.item()),\n        \"test_case_3\": [(o.shape, loss.item())\
    \ for _ in range(2)],\n        \"test_case_4\": (h.shape, o.shape, dh.shape, dq.shape,\
    \ dk.shape, dv.shape)\n    }\n    return results\n\n# Execute the test function\n\
    result_gold = test_chunk_retention_with_backward()\n\n\nDon't append test code\
    \ to the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ chunk_retention_ops.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_retention_ops
