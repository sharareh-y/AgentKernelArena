compile_command:
- python matmul_kernel.py
correctness_command:
- python matmul_kernel_perf.py
performance_command:
- tb_eval -f matmul_kernel.py -o matmul_kernel_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        This Triton kernel `matmul_kernel` is designed for matrix multiplication\
    \ of two large matrices, `a` and `b`, storing the result in `c`. The kernel operates\
    \ using a blocked approach, where `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`\
    \ dictate the dimensions of sub-matrices each thread block handles. The logical\
    \ process begins by calculating each thread block's starting position in the output\
    \ matrix via `pid_m` and `pid_n`, which are program IDs along the two major axes.\
    \ Matrix strides (`stride_am`, `stride_ak`, `stride_bk`, `stride_bn`, `stride_cm`,\
    \ `stride_cn`) are used for accessing elements efficiently. Offsets `offs_am`,\
    \ `offs_bn`, and `offs_k` are calculated to locate the correct sub-blocks of `a`\
    \ and `b` for loading. The computation iterates over the K dimension in blocks,\
    \ performing a dot product of sub-blocks `a` and `b`, accumulating results in\
    \ `accumulator`. After processing all blocks along K, the result in `accumulator`\
    \ is cast to `float16` and stored in `c`. The outer `matmul` function configures\
    \ the kernel launch, calculating grid sizes as a function of the matrix dimensions\
    \ `M`, `N`, and `K` divided by their respective block sizes.\n    \nThe test code\
    \ is:\n\n\ndef test_matmul():\n    BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K =\
    \ 64, 128, 64\n    M, N, K = 4096, 4096, 4096\n\n    # Initialize matrices on\
    \ CUDA device\n    c = torch.empty((M, N), device='cuda:0', dtype=torch.float16)\n\
    \    a = torch.rand((M, K), device='cuda:0', dtype=torch.float16)\n    b = torch.rand((K,\
    \ N), device='cuda:0', dtype=torch.float16)\n\n    # Call the matmul function\
    \ multiple times\n    test_case_1 = matmul(c, a, b, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N,\
    \ BLOCK_SIZE_K)\n\n    # Additional test cases to cover more branches\n    BLOCK_SIZE_M,\
    \ BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 64, 128\n    test_case_2 = matmul(c, a, b,\
    \ M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K)\n\n    BLOCK_SIZE_M, BLOCK_SIZE_N,\
    \ BLOCK_SIZE_K = 256, 256, 64\n    test_case_3 = matmul(c, a, b, M, N, K, BLOCK_SIZE_M,\
    \ BLOCK_SIZE_N, BLOCK_SIZE_K)\n\n    BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\
    \ = 32, 32, 32\n    test_case_4 = matmul(c, a, b, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N,\
    \ BLOCK_SIZE_K)\n\n    return {\n        \"test_case_1\": test_case_1,\n     \
    \   \"test_case_2\": test_case_2,\n        \"test_case_3\": test_case_3,\n   \
    \     \"test_case_4\": test_case_4\n    }\n\nresult_gold = test_matmul()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matmul_kernel.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_kernel
