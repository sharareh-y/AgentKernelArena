compile_command:
- python chunk_linear_attn.py
correctness_command:
- python chunk_linear_attn_perf.py
performance_command:
- tb_eval -f chunk_linear_attn.py -o chunk_linear_attn_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton implementation facilitates efficient linear attention\
    \ computation by organizing calculations into specialized kernels. The `chunk_linear_attn_fwd_kernel_h`\
    \ kernel initializes an intermediate tensor `h`, iteratively updating it by loading\
    \ and computing the dot product of block pointers derived from key (`k`) and value\
    \ (`v`) tensors across multiple time steps, incorporating an initial state if\
    \ provided. The `chunk_linear_attn_fwd_kernel_o` kernel calculates the output\
    \ tensor `o` by iterating over dimensions of the query (`q`), `k`, and `h` tensors,\
    \ performing dot products to compute weighted sums and handling the softmax attention\
    \ scoring with masked elements set to zero. The backward pass is divided into\
    \ `chunk_linear_attn_bwd_kernel_dh`, which computes the gradient for `h` by propagating\
    \ gradients from output to input space, and `chunk_linear_attn_bwd_kernel_dqkv`,\
    \ which calculates the gradients for `q`, `k`, and `v` by reusing the intermediate\
    \ computations in the forward pass. These kernels employ Triton's block pointer\
    \ feature, boundary checks, and arithmetic capabilities to perform operations\
    \ efficiently on GPUs. The `ChunkLinearAttentionFunction` class integrates these\
    \ Triton kernels into PyTorch's autograd framework, allowing the forward and backward\
    \ computations to be seamlessly included in PyTorch models. The function uses\
    \ input dimensions (`T`, `K`, `V`), threading model parameters (`BT`, `BK`, `BV`),\
    \ and Triton's grid and warp management to adaptively optimize for performance\
    \ and accuracy. \nThe test code is:\n\n\nimport torch\n\ndef test_chunk_linear_attn_with_backward():\n\
    \    # Define dimensions\n    B, H, T, K, V = 2, 4, 128, 64, 64\n\n    # Create\
    \ random input tensors as leaf nodes with requires_grad=True\n    q = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, device='cuda', requires_grad=True)\n    k = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, device='cuda', requires_grad=True)\n    v = torch.randn(B,\
    \ H, T, V, dtype=torch.float32, device='cuda', requires_grad=True)\n    initial_state\
    \ = torch.zeros(B, H, K, V, dtype=torch.float32, device='cuda', requires_grad=True)\n\
    \    scale = 1.0 / (K ** 0.5)\n\n    results = {}\n\n    # Test 1: Without initial\
    \ state and without final state output\n    o, final_state = chunk_linear_attn(q,\
    \ k, v, scale, initial_state=None, output_final_state=False)\n    loss = o.sum()\n\
    \    loss.backward()\n\n    results['test_case_1'] = {\n        \"output_shape\"\
    : o.shape,\n        \"loss\": loss.item(),\n        \"q_grad_norm\": q.grad.norm().item(),\n\
    \        \"k_grad_norm\": k.grad.norm().item(),\n        \"v_grad_norm\": v.grad.norm().item(),\n\
    \    }\n\n    q.grad.zero_()\n    k.grad.zero_()\n    v.grad.zero_()\n    if initial_state.grad\
    \ is not None:\n        initial_state.grad.zero_()\n\n    # Test 2: With initial\
    \ state and final state output\n    o, final_state = chunk_linear_attn(q, k, v,\
    \ scale, initial_state=initial_state, output_final_state=True)\n    loss = o.sum()\
    \ + final_state.sum()\n    loss.backward()\n\n    results['test_case_2'] = {\n\
    \        \"output_shape\": o.shape,\n        \"final_state_shape\": final_state.shape,\n\
    \        \"loss\": loss.item(),\n        \"q_grad_norm\": q.grad.norm().item(),\n\
    \        \"k_grad_norm\": k.grad.norm().item(),\n        \"v_grad_norm\": v.grad.norm().item(),\n\
    \    }\n\n    return results\n\n# Execute the test function\nresult_gold = test_chunk_linear_attn_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py chunk_linear_attn.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_linear_attn
