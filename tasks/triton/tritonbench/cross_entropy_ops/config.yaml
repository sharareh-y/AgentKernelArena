compile_command:
- python cross_entropy_ops.py
correctness_command:
- python cross_entropy_ops_perf.py
performance_command:
- tb_eval -f cross_entropy_ops.py -o cross_entropy_ops_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            This code defines a Triton-based cross-entropy loss function with\
    \ optional label smoothing and scaled logits handling. The main components are\
    \ two kernel functions: `cross_entropy_fwd_kernel` and `cross_entropy_bwd_kernel`,\
    \ which are executed via Triton to perform computations efficiently on GPU.\n\n\
    \            The `cross_entropy_fwd_kernel` computes the forward pass of the cross-entropy\
    \ loss. It processes each row of logits and calculates the log-sum-exp (lse) and\
    \ loss for each label, considering smoothing, logit scaling, and specific class\
    \ handling for distributed tensor parallelism. The kernel handles cases where\
    \ labels are ignored and adjusts loss calculation if the label index does not\
    \ match the current block of classes. It uses triton's utilities to manage block\
    \ operations and loop unrolling efficiently.\n\n            The `cross_entropy_bwd_kernel`\
    \ calculates the backward pass, deriving the gradient of the loss with respect\
    \ to logits. It applies the chain rule considering the effect of label smoothing\
    \ and optional scaling factors. The computed probabilities are adjusted based\
    \ on whether smoothing is applied or not, and then used to compute the gradient\
    \ contribution for each logit.\n\n            The `CrossEntropyLoss` class encapsulates\
    \ the forward and backward pass logic. The `forward` method prepares data structures\
    \ for loss and lse, manages distributed computation conditions, and executes the\
    \ forward kernel. It adjusts results for cases where the workload is split across\
    \ processes or blocks. The computed losses are also adjusted to zero out contributions\
    \ from ignored labels.\n\n            The `backward` method in `CrossEntropyLoss`\
    \ class computes gradients for the logits by calling the backward kernel. It supports\
    \ an optional in-place backward for memory efficiency.\n\n            Finally,\
    \ the `cross_entropy_loss` function is a high-level entry point that applies the\
    \ `CrossEntropyLoss` operation. It takes parameters such as logits, labels, and\
    \ smoothing factors, and returns the computed losses and z-losses. The function\
    \ is designed to handle both single and multi-process scenarios seamlessly. \n\
    \            \nThe test code is:\n\n\ndef test_cross_entropy_loss():\n    # Test\
    \ case 1: Basic test without label smoothing and ignored index\n    logits = torch.randn(4,\
    \ 10, device='cuda')\n    labels = torch.randint(0, 10, (4,), device='cuda')\n\
    \    losses, z_losses = cross_entropy_loss(logits, labels)\n\n    # Test case\
    \ 2: With label smoothing\n    label_smoothing = 0.1\n    losses_smoothing, z_losses_smoothing\
    \ = cross_entropy_loss(logits, labels, label_smoothing=label_smoothing)\n\n  \
    \  # Test case 3: With ignored index\n    ignored_index = 5\n    labels[0] = ignored_index\
    \  # Set one label to be ignored\n    losses_ignored, z_losses_ignored = cross_entropy_loss(logits,\
    \ labels, ignored_index=ignored_index)\n\n    # Test case 4: With logit scale\n\
    \    logit_scale = 0.5\n    losses_logit_scale, z_losses_logit_scale = cross_entropy_loss(logits,\
    \ labels, logit_scale=logit_scale)\n\n    # Test case 5: With lse square scale\n\
    \    lse_square_scale = 0.1\n    losses_lse_square, z_losses_lse_square = cross_entropy_loss(logits,\
    \ labels, lse_square_scale=lse_square_scale)\n\n    results = {\n        \"test_case_1\"\
    : (losses, z_losses),\n        \"test_case_2\": (losses_smoothing, z_losses_smoothing),\n\
    \        \"test_case_3\": (losses_ignored, z_losses_ignored),\n        \"test_case_4\"\
    : (losses_logit_scale, z_losses_logit_scale),\n        \"test_case_5\": (losses_lse_square,\
    \ z_losses_lse_square),\n    }\n    return results\n\nresult_gold = test_cross_entropy_loss()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py cross_entropy_ops.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- cross_entropy_ops
