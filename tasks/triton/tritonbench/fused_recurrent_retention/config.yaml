compile_command:
- python fused_recurrent_retention.py
correctness_command:
- python fused_recurrent_retention_perf.py
performance_command:
- tb_eval -f fused_recurrent_retention.py -o fused_recurrent_retention_output.json
  -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The code implements a fused recurrent retention mechanism using Triton\
    \ kernels for efficient computation. The primary function is 'fused_recurrent_retention',\
    \ which takes query 'q', key 'k', value 'v', and an optional 'initial_state'.\
    \ It performs a series of matrix operations across a temporal dimension 'T' to\
    \ produce an output tensor 'o' and optionally a 'final_state' tensor.\n      \
    \  The operation is carried out by two Triton kernels: 'fused_recurrent_retention_fwd_kernel'\
    \ for the forward pass and 'fused_recurrent_retention_bwd_kernel' for the backward\
    \ pass. The forward kernel loads and processes blocks of data from 'q', 'k', 'v',\
    \ and iteratively updates an accumulator 'h' using a decay factor based on the\
    \ current head index 'i_h'. If 'USE_INITIAL_STATE' is true, the kernel begins\
    \ with a preloaded initial state. The result for each timestep is stored in 'o',\
    \ and if 'STORE_FINAL_STATE' is set, the final state is stored. The backward kernel\
    \ reverses the process, starting from 'do' to compute gradients for 'q', 'k',\
    \ 'v' while handling the updates in a reverse order.\n        Important constants\
    \ like 'BK', 'BV' define block sizes, while 'scale' scales the query tensors to\
    \ normalize outputs. The computation grid is defined by dimensions related to\
    \ input tensor shapes and Triton's efficient layout strategies.\n    \nThe test\
    \ code is:\n\n\nimport torch\n\n# Extended test function with backward propagation\n\
    def test_fused_recurrent_retention_with_backward():\n    test_results = {}\n\n\
    \    # Test parameters\n    batch_size = 2\n    n_heads = 4\n    seq_len = 8\n\
    \    d_head_qk = 16\n    d_head_v = 16\n\n    # Create random input tensors\n\
    \    q = torch.randn(batch_size, n_heads, seq_len, d_head_qk, dtype=torch.float32,\
    \ requires_grad=True, device='cuda')\n    k = torch.randn(batch_size, n_heads,\
    \ seq_len, d_head_qk, dtype=torch.float32, requires_grad=True, device='cuda')\n\
    \    v = torch.randn(batch_size, n_heads, seq_len, d_head_v, dtype=torch.float32,\
    \ requires_grad=True, device='cuda')\n\n    # Test 1: Without initial state and\
    \ without final state\n    initial_state = None\n    output_final_state = False\n\
    \    o, final_state = fused_recurrent_retention(q, k, v, initial_state=initial_state,\
    \ output_final_state=output_final_state)\n    loss = o.sum()  # Define a simple\
    \ loss function\n    loss.backward()  # Perform backward pass\n    test_results['test_case_1']\
    \ = {\n        \"output_shape\": o.shape,\n        \"final_state\": final_state,\n\
    \        \"loss\": loss.item(),\n        \"gradients_q\": q.grad.norm().item(),\n\
    \        \"gradients_k\": k.grad.norm().item(),\n        \"gradients_v\": v.grad.norm().item()\n\
    \    }\n\n    # Reset gradients for the next test\n    q.grad.zero_()\n    k.grad.zero_()\n\
    \    v.grad.zero_()\n\n    # Test 2: With initial state and without final state\n\
    \    initial_state = torch.randn(batch_size, n_heads, d_head_qk, d_head_v, dtype=torch.float32,\
    \ device='cuda', requires_grad=True)\n    o, final_state = fused_recurrent_retention(q,\
    \ k, v, initial_state=initial_state, output_final_state=False)\n    loss = o.sum()\n\
    \    loss.backward()\n    test_results['test_case_2'] = {\n        \"output_shape\"\
    : o.shape,\n        \"final_state\": final_state,\n        \"loss\": loss.item(),\n\
    \        \"gradients_q\": q.grad.norm().item(),\n        \"gradients_k\": k.grad.norm().item(),\n\
    \        \"gradients_v\": v.grad.norm().item(),\n    }\n\n    # Reset gradients\
    \ for the next test\n    q.grad.zero_()\n    k.grad.zero_()\n    v.grad.zero_()\n\
    \n    # Test 3: With initial state and with final state\n    o, final_state =\
    \ fused_recurrent_retention(q, k, v, initial_state=initial_state, output_final_state=True)\n\
    \    loss = o.sum() + final_state.sum()\n    loss.backward()\n    test_results['test_case_3']\
    \ = {\n        \"output_shape\": o.shape,\n        \"final_state_shape\": final_state.shape,\n\
    \        \"loss\": loss.item(),\n        \"gradients_q\": q.grad.norm().item(),\n\
    \        \"gradients_k\": k.grad.norm().item(),\n        \"gradients_v\": v.grad.norm().item()\n\
    \    }\n\n    # Test 4: Without initial state and with final state\n    initial_state\
    \ = None\n    output_final_state = True\n    o, final_state = fused_recurrent_retention(q,\
    \ k, v, initial_state=initial_state, output_final_state=output_final_state)\n\
    \    loss = o.sum() + final_state.sum()\n    loss.backward()\n    test_results['test_case_4']\
    \ = {\n        \"output_shape\": o.shape,\n        \"final_state_shape\": final_state.shape,\n\
    \        \"loss\": loss.item(),\n        \"gradients_q\": q.grad.norm().item(),\n\
    \        \"gradients_k\": k.grad.norm().item(),\n        \"gradients_v\": v.grad.norm().item()\n\
    \    }\n\n    return test_results\n\n# Run the test function with backward propagation\n\
    result_gold = test_fused_recurrent_retention_with_backward()\n\nDon't append test\
    \ code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py fused_recurrent_retention.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fused_recurrent_retention
