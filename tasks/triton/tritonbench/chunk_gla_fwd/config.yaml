compile_command:
- python chunk_gla_fwd.py
correctness_command:
- python chunk_gla_fwd_perf.py
performance_command:
- tb_eval -f chunk_gla_fwd.py -o chunk_gla_fwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton code is designed to handle complex matrix operations,\
    \ primarily involving computation of attention matrices and outputs using block-based\
    \ processing. Here's an intricate breakdown:\n\n            The code consists\
    \ of multiple Triton kernels, each designed for different segments of the operation:\n\
    \n            1. **chunk_gla_fwd_A_kernel_intra_sub_inter**:\n               -\
    \ Input Parameters: `q` (query), `k` (key), `g` (gradient or gating), `A` (matrix\
    \ for storing results), and several other configuration scalars.\n           \
    \    - Functionality: This kernel computes a sub-block of the matrix `A` using\
    \ input matrices `q`, `k`, and `g`. It ensures that computations are done only\
    \ for valid blocks (`i_i <= i_j` is skipped to ensure upper triangle operations).\
    \ It loads blocks, scales and modifies them using exponential transformations,\
    \ and accumulates results in `b_A`. The computed block is then stored back.\n\n\
    \            2. **chunk_gla_fwd_A_kernel_intra_sub_intra**:\n               -\
    \ Similar in setup to the first kernel but focuses on processing sub-blocks intra-thread\
    \ to cover diagonal segments of `A`.\n               - Implements logic to sum\
    \ over the product of matrix blocks while considering exponentials and scaling\
    \ for attention-like computations.\n\n            3. **chunk_gla_fwd_A_kernel_intra_sub_intra_split**:\n\
    \               - Introduces splitting of computation along the `K` dimension\
    \ when `K` is large. This kernel writes intermediate results into `A_intra`, allowing\
    \ for memory-efficient partial computation across the `K` dimension.\n\n     \
    \       4. **chunk_gla_fwd_A_kernel_intra_sub_intra_merge**:\n               -\
    \ Takes partially computed blocks from `A_intra` and merges them into the main\
    \ matrix `A`. This step is necessary to consolidate the split results into a coherent\
    \ output.\n\n            5. **chunk_gla_fwd_kernel_o**:\n               - Manages\
    \ the construction of output `o`, crucial for attention mechanisms.\n        \
    \       - It processes inputs using gated mechanisms and cumulative sums, combining\
    \ them with the previous results to form the final output `o`.\n\n           \
    \ Wrapper Functions:\n            - `chunk_fwd_intra_gated_gk_fn`: Executes the\
    \ sequence of kernels responsible for calculating `A` based on the dimensions\
    \ and splits specified.\n            - `chunk_fwd_o_gated_gk_fn`: Employs the\
    \ previously computed `A` along with other inputs to determine the final result\
    \ `o`.\n\n            The kernels utilize Triton's efficient block operations,\
    \ leveraging parallelism in GPU architectures. This includes advanced techniques\
    \ like `tl.dot` for fast matrix multiplication and `tl.load`/`tl.store` for seamless\
    \ data movement across the GPU.\n            \nThe test code is:\n\n\ndef test_chunk_gla_fwd():\n\
    \    # \u6D4B\u8BD5\u6B63\u5E38\u7684\u8F93\u5165\u89C4\u6A21\n    B = 2  # batch\
    \ size\n    H = 2  # number of heads\n    T = 128  # sequence length\n    K =\
    \ 256  # key length\n    V = 64  # value length\n    BT = 16  # block size for\
    \ T\n    BC = 16  # block size for C (head dimension)\n    BK = 64  # block size\
    \ for K (key length)\n    scale = 1.0  # scaling factor\n\n    q = torch.randn(B,\
    \ H, T, K, dtype=torch.float32, device='cuda')\n    k = torch.randn(B, H, T, K,\
    \ dtype=torch.float32, device='cuda')\n    v = torch.randn(B, H, T, V, dtype=torch.float32,\
    \ device='cuda')\n    g = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n\
    \    h = torch.randn(B, H, K, V, dtype=torch.float32, device='cuda')\n    \n \
    \   A = chunk_fwd_intra_gated_gk_fn(q, k, g, scale, BT)\n    o = chunk_fwd_o_gated_gk_fn(q,\
    \ v, g.cumsum(dim=-1), A, h, BT, scale)\n\n    result = {}\n    result['test_case_1']\
    \ = o.shape\n\n    # \u6D4B\u8BD5 K > 256 \u7684\u60C5\u51B5\n    B = 2\n    H\
    \ = 2\n    T = 128\n    K = 512  # \u8BBE\u7F6E K > 256\n    V = 64\n    BT =\
    \ 16\n    BC = 16\n    BK = 128\n    scale = 1.0\n\n    q = torch.randn(B, H,\
    \ T, K, dtype=torch.float32, device='cuda')\n    k = torch.randn(B, H, T, K, dtype=torch.float32,\
    \ device='cuda')\n    v = torch.randn(B, H, T, V, dtype=torch.float32, device='cuda')\n\
    \    g = torch.randn(B, H, T, K, dtype=torch.float32, device='cuda')\n    h =\
    \ torch.randn(B, H, K, V, dtype=torch.float32, device='cuda')\n\n    A = chunk_fwd_intra_gated_gk_fn(q,\
    \ k, g, scale, BT)\n    o = chunk_fwd_o_gated_gk_fn(q, v, g.cumsum(dim=-1), A,\
    \ h, BT, scale)\n\n    result['test_case_3'] = o.shape\n\n    return result\n\n\
    result_gold = test_chunk_gla_fwd()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ chunk_gla_fwd.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- chunk_gla_fwd
