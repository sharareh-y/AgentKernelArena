compile_command:
- python logsumexp_fwd.py
correctness_command:
- python logsumexp_fwd_perf.py
performance_command:
- tb_eval -f logsumexp_fwd.py -o logsumexp_fwd_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton operator `logsumexp_fwd_kernel` calculates the log-sum-exp\
    \ over the last dimension of an input tensor. This is often used to stabilize\
    \ the computation of the exponential function on input elements, especially with\
    \ large values. The kernel operates in parallel using Triton, leveraging the GPU\
    \ architecture for efficiency.\n\n        The `logsumexp_fwd_kernel` kernel is\
    \ decorated with `triton.autotune` to optimize its execution across different\
    \ hardware configurations, particularly varying the number of warps between 1\
    \ and 32. It uses a heuristic `HAS_SCALE` to determine whether the input tensor\
    \ should be scaled.\n\n        Within the kernel, we calculate the indices `i_n`\
    \ and `i_d` using Triton's `program_id`, which represent the current program's\
    \ block IDs. The indices `o_d` and `m_d` determine the positions to operate on\
    \ and apply masking to avoid out-of-bounds memory accesses. The input tensor `x`\
    \ is loaded into a block `b_x`, potentially scaled, and the maximum value `b_m`\
    \ is computed along the block dimension. The kernel then calculates the log of\
    \ the sum of exponentials, adds the maximum value `b_m`, and stores the result\
    \ in `z`.\n\n        The `logsumexp_fwd` function prepares the input tensor by\
    \ reshaping it and calculates `N`, `D`, and `B`. It determines `ND`, the division\
    \ of `D` by `B`, creating an empty tensor `z` to hold results. The kernel is invoked\
    \ with `(N, ND)` blocks. After computation, `z` is reduced along the last dimension\
    \ to produce the final result. If a different output data type is desired, it\
    \ casts `z` to this type before returning it.\n    \nThe test code is:\n\n\ndef\
    \ test_logsumexp_fwd():\n    batch_size = 4\n    seq_len = 64  # \u6700\u540E\u4E00\
    \u4E2A\u7EF4\u5EA6\u957F\u5EA6\n    scale = 0.5  # \u7F29\u653E\u56E0\u5B50\n\n\
    \    # Test 1: Basic Random Input\n    x = torch.randn((batch_size, seq_len),\
    \ device='cuda', dtype=torch.float32)\n    z1 = logsumexp_fwd(x)\n\n    # Test\
    \ 2: Input with Scale\n    x = torch.randn((batch_size, seq_len), device='cuda',\
    \ dtype=torch.float32)\n    z2 = logsumexp_fwd(x, scale=scale)\n\n    # Test 3:\
    \ Higher Dimensional Input\n    x = torch.randn((batch_size, 16, seq_len), device='cuda',\
    \ dtype=torch.float32)\n    z3 = logsumexp_fwd(x)\n\n    # Test 4: Input with\
    \ Different Data Type\n    x = torch.randn((batch_size, seq_len), device='cuda',\
    \ dtype=torch.float32)\n    z4 = logsumexp_fwd(x, dtype=torch.float64)\n\n   \
    \ results = {\n        \"test_case_1\": z1,\n        \"test_case_2\": z2,\n  \
    \      \"test_case_3\": z3,\n        \"test_case_4\": z4\n    }\n    return results\n\
    \nresult_gold = test_logsumexp_fwd()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ logsumexp_fwd.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- logsumexp_fwd
