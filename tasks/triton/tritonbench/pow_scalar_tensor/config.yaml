compile_command:
- python pow_scalar_tensor.py
correctness_command:
- python pow_scalar_tensor_perf.py
performance_command:
- tb_eval -f pow_scalar_tensor.py -o pow_scalar_tensor_output.json -run_on_code -ds
  tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton operator is designed to compute the power of each element\
    \ in a tensor by a scalar value, using a specialized kernel function `pow_func_scalar_tensor_kernel_rank_1`.\
    \ The wrapper function `pow_func_scalar_tensor_wrapper_rank_1` sets up the computation\
    \ by ensuring input-output shape alignment and using heuristics to determine optimal\
    \ computation parameters, such as tile size, number of warps, and grid configuration.\
    \ It handles both torch `Tensor` and `StridedBuffer` inputs. The kernel function\
    \ processes data in parallel, managing memory offsets and data loading using block\
    \ pointers, and performs boundary checks to ensure safe memory operations. The\
    \ operation is divided into tiles, and depending on the configuration, it either\
    \ uses a monolithic or a grid-stride-loop style kernel execution. The use of Triton's\
    \ `pow` function ensures compatibility and efficiency in computing the exponentiation\
    \ on the GPU.\n            \nThe test code is:\n\n\ndef test_pow_func_scalar_tensor_wrapper_rank_1():\n\
    \    # Case 1: Test with a 1D tensor, matching shapes for input and output, scalar\
    \ exponent\n    in_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32,\
    \ device=\"cuda\")\n    out_tensor = torch.zeros_like(in_tensor, device=\"cuda\"\
    )\n    scalar_value = 2.0  # Exponent for the pow operation\n    \n    # Call\
    \ the kernel wrapper function\n    pow_func_scalar_tensor_wrapper_rank_1(scalar_value,\
    \ in_tensor, out0=out_tensor)\n\n    # Case 2: Test with a different exponent\
    \ (fractional)\n    scalar_value = 0.5\n    pow_func_scalar_tensor_wrapper_rank_1(scalar_value,\
    \ in_tensor, out0=out_tensor)\n\n    # Case 3: Test with a different shape (larger\
    \ tensor)\n    in_tensor_large = torch.tensor([2.0, 4.0, 8.0, 16.0, 32.0, 64.0,\
    \ 128.0], dtype=torch.float32, device=\"cuda\")\n    out_tensor_large = torch.zeros_like(in_tensor_large,\
    \ device=\"cuda\")\n    scalar_value = 3.0\n    pow_func_scalar_tensor_wrapper_rank_1(scalar_value,\
    \ in_tensor_large, out0=out_tensor_large)\n\n    # Case 4: Test with negative\
    \ values in the input tensor\n    in_tensor_negative = torch.tensor([-1.0, -2.0,\
    \ -3.0], dtype=torch.float32, device=\"cuda\")\n    out_tensor_negative = torch.zeros_like(in_tensor_negative,\
    \ device=\"cuda\")\n    scalar_value = 2.0  # Exponent should result in positive\
    \ values for even powers\n    pow_func_scalar_tensor_wrapper_rank_1(scalar_value,\
    \ in_tensor_negative, out0=out_tensor_negative)\n\n    # Case 5: Test with large\
    \ exponent\n    in_tensor_large_exp = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32,\
    \ device=\"cuda\")\n    out_tensor_large_exp = torch.zeros_like(in_tensor_large_exp,\
    \ device=\"cuda\")\n    scalar_value = 10.0  # Large exponent\n    pow_func_scalar_tensor_wrapper_rank_1(scalar_value,\
    \ in_tensor_large_exp, out0=out_tensor_large_exp)\n\n    # Collect results\n \
    \   results = {\n        \"test_case_1\": out_tensor.clone().cpu().numpy(),\n\
    \        \"test_case_2\": out_tensor.clone().cpu().numpy(),\n        \"test_case_3\"\
    : out_tensor_large.clone().cpu().numpy(),\n        \"test_case_4\": out_tensor_negative.clone().cpu().numpy(),\n\
    \        \"test_case_5\": out_tensor_large_exp.clone().cpu().numpy(),\n    }\n\
    \    return results\n\n# Run the test function\nresult_gold = test_pow_func_scalar_tensor_wrapper_rank_1()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py pow_scalar_tensor.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- pow_scalar_tensor
