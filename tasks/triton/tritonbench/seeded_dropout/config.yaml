compile_command:
- python seeded_dropout.py
correctness_command:
- python seeded_dropout_perf.py
performance_command:
- tb_eval -f seeded_dropout.py -o seeded_dropout_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided Triton kernel _seeded_dropout and the Python function seeded_dropout\
    \ implement a seeded dropout operation on a given tensor x. The purpose of this\
    \ implementation is to simulate the dropout operation, which is typically used\
    \ to prevent overfitting in neural networks by randomly setting elements of the\
    \ input tensor to zero based on a probability p. \n\n        The function _seeded_dropout\
    \ is a Triton kernel, meaning it's executed in parallel on a GPU. It takes several\
    \ arguments: x_ptr and output_ptr, which are pointers to the input and output\
    \ data; n_elements, the number of elements to process; p, the probability of setting\
    \ a value to zero; seed, a fixed seed for random number generation to ensure reproducibility;\
    \ and BLOCK_SIZE, a constant expression defining the size of blocks of data processed\
    \ by each kernel instance. The kernel calculates offsets for each block based\
    \ on the program ID, loads data from the input pointer using these offsets, and\
    \ generates a random number per element using tl.rand. A mask is created where\
    \ random numbers greater than the probability p indicate elements that are kept.\
    \ These elements are scaled by 1/(1-p) to keep the output mean consistent with\
    \ the input. The result is stored in the output pointer, ensuring values outside\
    \ the bounds of n_elements are masked.\n\n        The function seeded_dropout\
    \ is a wrapper around the Triton kernel. It initializes an output tensor of the\
    \ same shape and type as the input tensor x and ensures the input tensor is contiguous.\
    \ It calculates the grid size needed to cover all input elements given the specified\
    \ BLOCK_SIZE, which determines how many elements each kernel instance will process.\
    \ The Triton kernel _seeded_dropout is then launched with this grid configuration.\
    \ Finally, the output tensor is returned, containing the dropout-applied version\
    \ of the input tensor.\n    \nThe test code is:\n\n\nimport torch\n\n# Test for\
    \ the seeded_dropout function\ndef test_seeded_dropout():\n    # Input tensor\n\
    \    x = torch.randn(size=(10,)).cuda()\n    results = {}\n    # Test with the\
    \ same seed\n    results['test_case_1'] = seeded_dropout(x, p=0.5, seed=123)\n\
    \    results['test_case_2'] = seeded_dropout(x, p=0.5, seed=123)\n    # Test with\
    \ a different seed\n    results['test_case_3'] = seeded_dropout(x, p=0.5, seed=512)\n\
    \    # Test with a different probability\n    results['test_case_4'] = seeded_dropout(x,\
    \ p=0.3, seed=123)\n    return results\n\n# Run tests\nresult_gold = test_seeded_dropout()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py seeded_dropout.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- seeded_dropout
