compile_command:
- python matmul_persistent_triton.py
correctness_command:
- python matmul_persistent_triton_perf.py
performance_command:
- tb_eval -f matmul_persistent_triton.py -o matmul_persistent_triton_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The `matmul_kernel_persistent` function is a Triton JIT-compiled kernel\
    \ designed for matrix multiplication with persistent thread execution across different\
    \ tiles of the matrices. The function takes pointers to input matrices `a_ptr`,\
    \ `b_ptr`, and an output matrix `c_ptr`, along with the dimensions `M`, `N`, `K`,\
    \ and strides for each matrix. It uses configurable block sizes for partitioning\
    \ the matrices and utilizes GPU resources effectively by leveraging multiple streaming\
    \ multiprocessors (NUM_SMS).\n\n        The kernel divides the matrices into blocks\
    \ of size `BLOCK_SIZE_M x BLOCK_SIZE_N` and iterates over `BLOCK_SIZE_K` tiles\
    \ of the shared dimension `K`. Each tile is processed independently, and results\
    \ are accumulated in the `accumulator` array. The kernel employs mask handling\
    \ to manage edge cases where block indices exceed matrix dimensions and uses `tl.load`\
    \ and `tl.store` operations with masks to safely access global memory. The accumulated\
    \ results are converted to the appropriate data type before storing them in the\
    \ output matrix.\n\n        The `matmul_persistent` function acts as a wrapper\
    \ to initialize and launch the Triton kernel. It validates input tensor shapes\
    \ and data types and computes the appropriate grid size based on the input dimensions\
    \ and hardware capabilities. The function configures kernel parameters such as\
    \ block sizes, number of stages, and warps based on the input data type, and executes\
    \ the `matmul_kernel_persistent` with these parameters. The output is an allocated\
    \ tensor `c` which contains the resulting product of matrices `a` and `b`.\n \
    \   \nThe test code is:\n\n\nimport torch\n\n# Test for matmul_persistent\ndef\
    \ test_matmul_persistent():\n    M, K, N = 256, 128, 256\n    results = {}\n \
    \   \n    # Test case 1\n    a = torch.randn((M, K), dtype=torch.float16, device='cuda')\n\
    \    b = torch.randn((K, N), dtype=torch.float16, device='cuda')\n    c = matmul_persistent(a,\
    \ b)\n    results['test_case_1'] = c\n\n    return results\n\n# Run all tests\n\
    result_gold = test_matmul_persistent()\n# test_matmul_persistent()\n\n\nDon't\
    \ append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py matmul_persistent_triton.py\
    \ {kernel_path}` to check the correctness and performance.The kernel_path is where\
    \ you stored the generated code.\nCall Status means whether the code can be executed,\
    \ Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_persistent_triton
