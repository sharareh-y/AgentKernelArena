compile_command:
- python rbe_triton_transform.py
correctness_command:
- python rbe_triton_transform_perf.py
performance_command:
- tb_eval -f rbe_triton_transform.py -o rbe_triton_transform_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The `rbe_triton` kernel applies a position-dependent transformation\
    \ to a 3D input tensor `x` of shape [batch, M, K], performing operations on real\
    \ and imaginary parts separately. Each thread block processes a rectangular section\
    \ of the tensor determined by the block size, `BLOCK_SIZE_M` (2) for the M dimension\
    \ and `BLOCK_SIZE_K` (1024) for the K dimension. The kernel uses `program_id`\
    \ to compute the offsets `offs_m` and `offs_n` for each block, loading data from\
    \ `x` into local variables `real` and `imag`, considering only even-numbered indices\
    \ for real components. Sine and cosine values for position-dependent transformations\
    \ are precomputed via `get_freq_multi_tokens`, with `theta=10000`. These frequency\
    \ values are used to transform `real` and `imag` into `out_real` and `out_imag`,\
    \ which are then written back to the output tensor `out` using calculated offsets\
    \ and appropriate masks. The `rbe_triton_wrapper` function prepares the parameters,\
    \ defines the execution grid based on input dimensions, and launches the kernel,\
    \ managing data flow and maintaining tensor consistency.\n            \nThe test\
    \ code is:\n\n\n# Test for rbe_triton_wrapper\ndef test_rbe_triton():\n    results\
    \ = {}\n    batch, M, K = 2, 4, 1024\n\n    # Test case 1\n    x1 = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    pos1 = 0\n    out1 = rbe_triton_wrapper(x1,\
    \ pos1)\n    results['test_case_1'] = out1\n\n    # Test case 2\n    x2 = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    pos2 = 1\n    out2 = rbe_triton_wrapper(x2,\
    \ pos2)\n    results['test_case_2'] = out2\n\n    # Test case 3\n    x3 = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    pos3 = 2\n    out3 = rbe_triton_wrapper(x3,\
    \ pos3)\n    results['test_case_3'] = out3\n\n    # Test case 4\n    x4 = torch.randn((batch,\
    \ M, K), dtype=torch.float16, device='cuda')\n    pos4 = 3\n    out4 = rbe_triton_wrapper(x4,\
    \ pos4)\n    results['test_case_4'] = out4\n\n    return results\n\nresult_gold\
    \ = test_rbe_triton()\n\n\nDon't append test code to the kernel code or edit test\
    \ function.\n\nThe generated code should be written into a python file.\nIf you\
    \ have already created a file and wrote the code into it, edit the code directly\
    \ in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ rbe_triton_transform.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- rbe_triton_transform
