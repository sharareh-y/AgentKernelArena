compile_command:
- python reversed_cumsum.py
correctness_command:
- python reversed_cumsum_perf.py
performance_command:
- tb_eval -f reversed_cumsum.py -o reversed_cumsum_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    The `chunk_global_reversed_cumsum_vector` function performs a reversed cumulative\
    \ sum on a 4D tensor `s` using a custom Triton kernel, suitable for batched operations\
    \ with attention-like structures. The function begins by extracting the dimensions\
    \ of the input tensor [B, H, T, S] and sets a spatial block size `BS` of 32. The\
    \ output tensor `z` is initialized with the same shape as `s`, but with the specified\
    \ `dtype`. The kernel `chunk_global_reversed_cumsum_vector_kernel` is configured\
    \ to optimize over various block sizes (`BT`) and numbers of warps to efficiently\
    \ handle different input sizes. Within the kernel, a loop iterates backwards over\
    \ time blocks determined by `BT`, starting from the end (`tl.cdiv(T, BT) - 1`)\
    \ to the beginning. A block pointer is created for both the input and output tensors.\
    \ The kernel loads the input block `b_s`, computes a masked dot product with a\
    \ lower triangular mask `m_s` to ensure the cumulative sum is reversed, and stores\
    \ the result in the corresponding block of the output tensor `z`. This operation\
    \ ensures only the elements at or after the current time step are included. The\
    \ cumulative sum `b_z` is updated after processing each block, adding the sum\
    \ of the current block `b_s` along the feature dimension. The kernel ensures boundary\
    \ checks during load and store operations to manage out-of-bounds memory accesses.\
    \ The result is returned in `z` after the kernel execution, representing the reversed\
    \ cumulative sum across the specified dimension.\n    \nThe test code is:\n\n\n\
    import torch\n\n# Test for chunk_global_reversed_cumsum_vector\ndef test_chunk_global_reversed_cumsum_vector():\n\
    \    results = {}\n    \n    # Test case 1\n    B, H, T, S = 2, 3, 4, 5\n    s\
    \ = torch.rand((B, H, T, S), dtype=torch.float32).cuda()\n    result = chunk_global_reversed_cumsum_vector(s)\n\
    \    results['test_case_1'] = result\n\n    # Test case 2\n    B, H, T, S = 1,\
    \ 1, 8, 8\n    s = torch.rand((B, H, T, S), dtype=torch.float32).cuda()\n    result\
    \ = chunk_global_reversed_cumsum_vector(s)\n    results['test_case_2'] = result\n\
    \n    # Test case 3\n    B, H, T, S = 4, 2, 16, 16\n    s = torch.rand((B, H,\
    \ T, S), dtype=torch.float32).cuda()\n    result = chunk_global_reversed_cumsum_vector(s)\n\
    \    results['test_case_3'] = result\n\n    # Test case 4\n    B, H, T, S = 3,\
    \ 3, 32, 32\n    s = torch.rand((B, H, T, S), dtype=torch.float32).cuda()\n  \
    \  result = chunk_global_reversed_cumsum_vector(s)\n    results['test_case_4']\
    \ = result\n\n    return results\n\n# Run all tests\nresult_gold = test_chunk_global_reversed_cumsum_vector()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py reversed_cumsum.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- reversed_cumsum
