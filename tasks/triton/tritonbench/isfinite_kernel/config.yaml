compile_command:
- python isfinite_kernel.py
correctness_command:
- python isfinite_kernel_perf.py
performance_command:
- tb_eval -f isfinite_kernel.py -o isfinite_kernel_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The code implements a Triton-based operator for checking the finiteness\
    \ of tensor elements. The `isfinite_func_wrapper_rank_1` function is a wrapper\
    \ that processes input and output tensors, ensuring they have matching shapes.\
    \ It uses heuristic functions to determine optimal tile sizes (`heuristics_for_tile_size`)\
    \ and the number of warps (`heuristics_for_num_warps`) for efficient execution.\
    \ The function calculates task parameters like `num_ctas` and `grid`, and then\
    \ calls the Triton kernel `isfinite_func_kernel_rank_1`. \n\n            The `isfinite_func_kernel_rank_1`\
    \ kernel operates on rank-1 tensors and employs either a monolithic or grid-stride-loop\
    \ approach based on `one_tile_per_cta`. The kernel leverages Triton's pointer\
    \ and block pointer types to load data elements from the input tensor into a Triton\
    \ tensor (`in0`). It uses boundary checking to prevent out-of-bounds accesses.\
    \ The kernel applies the `isfinite_func` function, which determines finiteness\
    \ by calling `_isfinited` for fp64 or `_finitef` for other types, casting appropriately.\
    \ Results are stored back into the output tensor using the Triton `store` function.\
    \ The kernel dynamically adjusts its execution strategy based on the input size\
    \ and structure, ensuring both safety and performance.\n            \nThe test\
    \ code is:\n\n\ndef test_isfinite_func_wrapper_rank_1():\n    results = {}\n\n\
    \    # Test case 1: float32, single CTA\n    in_tensor_1 = torch.randn(512, dtype=torch.float32,\
    \ device='cuda')  # shape (512,)\n    out_tensor_1 = torch.empty_like(in_tensor_1,\
    \ dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_1, out0=out_tensor_1)\n\
    \    results['test_case_1'] = out_tensor_1\n\n    # Test case 2: float32, multiple\
    \ CTAs\n    in_tensor_2 = torch.randn(4096, dtype=torch.float32, device='cuda')\
    \  # shape (4096,)\n    out_tensor_2 = torch.empty_like(in_tensor_2, dtype=torch.bool)\n\
    \    isfinite_func_wrapper_rank_1(in_tensor_2, out0=out_tensor_2)\n    results['test_case_2']\
    \ = out_tensor_2\n\n    # Test case 3: float64, single CTA\n    in_tensor_3 =\
    \ torch.randn(512, dtype=torch.float64, device='cuda')  # shape (512,)\n    out_tensor_3\
    \ = torch.empty_like(in_tensor_3, dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_3,\
    \ out0=out_tensor_3)\n    results['test_case_3'] = out_tensor_3\n\n    # Test\
    \ case 4: float64, multiple CTAs\n    in_tensor_4 = torch.randn(4096, dtype=torch.float64,\
    \ device='cuda')  # shape (4096,)\n    out_tensor_4 = torch.empty_like(in_tensor_4,\
    \ dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_4, out0=out_tensor_4)\n\
    \    results['test_case_4'] = out_tensor_4\n\n    # Test case 5: float32 tensor\
    \ with infinities\n    in_tensor_5 = torch.tensor([float('inf'), -float('inf'),\
    \ 1.0, 2.0, float('nan')], dtype=torch.float32, device='cuda')\n    out_tensor_5\
    \ = torch.empty_like(in_tensor_5, dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_5,\
    \ out0=out_tensor_5)\n    results['test_case_5'] = out_tensor_5\n\n    # Test\
    \ case 6: float64 tensor with infinities\n    in_tensor_6 = torch.tensor([float('inf'),\
    \ -float('inf'), 1.0, 2.0, float('nan')], dtype=torch.float64, device='cuda')\n\
    \    out_tensor_6 = torch.empty_like(in_tensor_6, dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_6,\
    \ out0=out_tensor_6)\n    results['test_case_6'] = out_tensor_6\n\n    # Test\
    \ case 7: float32 tensor with mixed extreme and normal values\n    in_tensor_7\
    \ = torch.cat([torch.tensor([float('inf'), -float('inf')], dtype=torch.float32,\
    \ device='cuda'),\n                            torch.randn(510, dtype=torch.float32,\
    \ device='cuda')])\n    out_tensor_7 = torch.empty_like(in_tensor_7, dtype=torch.bool)\n\
    \    isfinite_func_wrapper_rank_1(in_tensor_7, out0=out_tensor_7)\n    results['test_case_7']\
    \ = out_tensor_7\n\n    # Test case 8: float64 tensor with mixed extreme and normal\
    \ values\n    in_tensor_8 = torch.cat([torch.tensor([float('inf'), -float('inf')],\
    \ dtype=torch.float64, device='cuda'),\n                            torch.randn(510,\
    \ dtype=torch.float64, device='cuda')])\n    out_tensor_8 = torch.empty_like(in_tensor_8,\
    \ dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_8, out0=out_tensor_8)\n\
    \    results['test_case_8'] = out_tensor_8\n\n    # Test case 9: float32 tensor\
    \ with NaN and finite values\n    in_tensor_9 = torch.tensor([float('nan'), 0.0,\
    \ -1.0, float('nan'), 3.14], dtype=torch.float32, device='cuda')\n    out_tensor_9\
    \ = torch.empty_like(in_tensor_9, dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_9,\
    \ out0=out_tensor_9)\n    results['test_case_9'] = out_tensor_9\n\n    # Test\
    \ case 10: float64 tensor with NaN and finite values\n    in_tensor_10 = torch.tensor([float('nan'),\
    \ 0.0, -1.0, float('nan'), 3.14], dtype=torch.float64, device='cuda')\n    out_tensor_10\
    \ = torch.empty_like(in_tensor_10, dtype=torch.bool)\n    isfinite_func_wrapper_rank_1(in_tensor_10,\
    \ out0=out_tensor_10)\n    results['test_case_10'] = out_tensor_10\n\n    return\
    \ results\n\nresult_gold = test_isfinite_func_wrapper_rank_1()\n\n\nDon't append\
    \ test code to the kernel code or edit test function.\n\nThe generated code should\
    \ be written into a python file.\nIf you have already created a file and wrote\
    \ the code into it, edit the code directly in the file.\nTest the code by running\
    \ `python python_bindings/tritonbench.py isfinite_kernel.py {kernel_path}` to\
    \ check the correctness and performance.The kernel_path is where you stored the\
    \ generated code.\nCall Status means whether the code can be executed, Exec Status\
    \ means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- isfinite_kernel
