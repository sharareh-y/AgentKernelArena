compile_command:
- python context_attn_llama.py
correctness_command:
- python context_attn_llama_perf.py
performance_command:
- tb_eval -f context_attn_llama.py -o context_attn_llama_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The Triton kernel `_fwd_kernel` is designed to execute a context attention\
    \ mechanism, essential in transformer architectures. \n        It accepts tensors\
    \ Q, K, and V representing queries, keys, and values, respectively, alongside\
    \ several metadata and stride parameters to account for complex batching and sequence\
    \ operations. \n        The kernel handles operations for attention computation:\
    \ it initially calculates dot products between Q and K, scales these products\
    \ by `sm_scale`, and applies an exponential function to obtain softmax values.\
    \ \n        The calculated attention scores are then utilized to weight the values\
    \ in V, and the results are accumulated and stored in the `Out` tensor. \n   \
    \     This process involves masking techniques to handle variable sequence lengths\
    \ and prompt caching, ensuring causal ordering. \n        The kernel leverages\
    \ Triton's parallel execution capabilities through grid and block configurations\
    \ determined by inputs such as `BLOCK_M`, `BLOCK_N`, and `kv_group_num`.\n\n \
    \       The `context_attention_fwd` function orchestrates the execution of the\
    \ Triton kernel by setting up the computational environment. \n        It calculates\
    \ the appropriate block size `BLOCK_M` based on whether the Tesla architecture\
    \ is in use, establishes shape constraints, and determines the scaling factor\
    \ `sm_scale` using the inverse square root of the query dimension `Lq`, adjusted\
    \ by a constant for compatibility with Triton's exponential function. \n     \
    \   The function configures execution parameters, including batch and head dimensions,\
    \ through a grid lambda function, and invokes the Triton kernel with all required\
    \ inputs and meta parameters, such as tensor strides, kv head grouping, and block\
    \ dimensionality. \n        This setup ensures efficient parallel attention computation\
    \ across potentially large input sequences.\n    \nThe test code is:\n\n\nimport\
    \ torch\n\ndef test_context_attention_fwd():\n    import torch\n    import numpy\
    \ as np\n\n    Z, H, N_CTX, D_HEAD = 16, 16, 2048, 128\n    dtype = torch.float16\n\
    \    prompt_cache_len = 128\n    q = torch.empty((Z * (N_CTX - prompt_cache_len),\
    \ H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2)\n    k\
    \ = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4,\
    \ std=0.2)\n    v = torch.empty((Z * N_CTX, H, D_HEAD), dtype=dtype, device=\"\
    cuda\").normal_(mean=0.3, std=0.2)\n    o = torch.empty((Z * (N_CTX - prompt_cache_len),\
    \ H, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2)\n\n   \
    \ req_to_token_indexs = torch.empty((1000, N_CTX + 7000), dtype=torch.int32, device=\"\
    cuda\")\n    max_input_len = N_CTX\n    b_start_loc = torch.zeros((Z,), dtype=torch.int32,\
    \ device=\"cuda\")\n    b_seq_len = torch.ones((Z,), dtype=torch.int32, device=\"\
    cuda\")\n    b_req_idx = torch.ones((Z,), dtype=torch.int32, device=\"cuda\")\n\
    \    b_prompt_cache_len = torch.zeros(Z, dtype=torch.int32, device=\"cuda\")\n\
    \n    results = {}\n\n    # Test case 1\n    context_attention_fwd(\n        q,\
    \ k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len,\
    \ req_to_token_indexs\n    )\n    results['test_case_1'] = o.clone()\n\n    #\
    \ Test case 2: Different prompt cache length\n    b_prompt_cache_len = torch.full((Z,),\
    \ 64, dtype=torch.int32, device=\"cuda\")\n    context_attention_fwd(\n      \
    \  q, k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len,\
    \ req_to_token_indexs\n    )\n    results['test_case_2'] = o.clone()\n\n    #\
    \ Test case 3: Different sequence length\n    b_seq_len = torch.full((Z,), 1024,\
    \ dtype=torch.int32, device=\"cuda\")\n    context_attention_fwd(\n        q,\
    \ k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len,\
    \ req_to_token_indexs\n    )\n    results['test_case_3'] = o.clone()\n\n    #\
    \ Test case 4: Different request index\n    b_req_idx = torch.arange(Z, dtype=torch.int32,\
    \ device=\"cuda\")\n    context_attention_fwd(\n        q, k, v, o, b_req_idx,\
    \ b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len, req_to_token_indexs\n\
    \    )\n    results['test_case_4'] = o.clone()\n\n    return results\n\nresult_gold\
    \ = test_context_attention_fwd()\n\n\nDon't append test code to the kernel code\
    \ or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ context_attn_llama.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- context_attn_llama
