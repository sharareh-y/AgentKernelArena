compile_command:
- python diag_ssm_triton.py
correctness_command:
- python diag_ssm_triton_perf.py
performance_command:
- tb_eval -f diag_ssm_triton.py -o diag_ssm_triton_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \    The Triton SSM kernel functions define efficient GPU computations for state-space\
    \ models by iterating through sequence data. \n\n    `diag_ssm_forward_kernel`\
    \ handles the forward pass for real-valued inputs, applying the transformation\
    \ `s = s * Lambda + x` across a sequence of data stored in `x_ptr`. It reads initial\
    \ states from `s_ptr` and writes outputs to `y_ptr`. The transformation is iteratively\
    \ applied for each timestep `t` in the sequence `length`, over dimensions defined\
    \ by `batch_size` and `dim`, using a grid that spans `(batch_size * dim)` divided\
    \ into blocks of `BLOCK_SIZE`.\n\n    `diag_ssm_forward_kernel_complex` extends\
    \ this to complex numbers, handling real and imaginary parts separately, and performing\
    \ complex multiplication and addition.\n\n    For the backward pass, `diag_ssm_backward_kernel`\
    \ and its complex counterpart compute gradients needed for learning, propagating\
    \ them backwards from `grad_y_ptr` through the sequence, updating `grad_s_ptr`,\
    \ `grad_x_ptr`, and `grad_lambda_ptr`. Gradients respect the chain rule, ensuring\
    \ consistency with the complex autograd requirements in PyTorch.\n\n    The enclosing\
    \ class `_ssm_forward` uses these kernels within PyTorch's autograd framework,\
    \ handling input validation and complex data type checks. It initiates GPU kernel\
    \ execution by defining a grid configuration function to optimize computation.\n\
    \    \nThe test code is:\n\ndef test_diag_ssm_triton():\n    # \u6D4B\u8BD5\u53C2\
    \u6570\n    batch_size, dim, length = 2, 3, 5  # \u5B9A\u4E49\u6D4B\u8BD5\u5F20\
    \u91CF\u7684\u7EF4\u5EA6\n    BLOCK_SIZE = 128  # Triton\u6838\u7684\u5757\u5927\
    \u5C0F\n\n    # \u521D\u59CB\u5316\u8F93\u5165\u5F20\u91CF\uFF0C\u786E\u4FDD requires_grad=True\n\
    \    # \u5B9E\u6570\u5F20\u91CF\n    s_real = torch.randn((batch_size, dim), dtype=torch.float32,\
    \ device=\"cuda\", requires_grad=True)\n    x_real = torch.randn((length, batch_size,\
    \ dim), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n    Lambda_real\
    \ = torch.rand((dim,), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n\
    \    \n    # \u590D\u6570\u5F20\u91CF\n    s_complex = torch.randn((batch_size,\
    \ dim), dtype=torch.complex64, device=\"cuda\", requires_grad=True)\n    x_complex\
    \ = torch.randn((length, batch_size, dim), dtype=torch.complex64, device=\"cuda\"\
    , requires_grad=True)\n    Lambda_complex = torch.rand((dim,), dtype=torch.complex64,\
    \ device=\"cuda\", requires_grad=True)\n\n    # Triton\u524D\u5411\u4F20\u64AD\
    \uFF0C\u5BF9\u4E8E\u5B9E\u6570Lambda\n    y_triton_real = diag_ssm_forward_triton(s_real,\
    \ x_real, Lambda_real)\n    # Triton\u524D\u5411\u4F20\u64AD\uFF0C\u5BF9\u4E8E\
    \u590D\u6570Lambda\n    y_triton_complex = diag_ssm_forward_triton(s_complex,\
    \ x_complex, Lambda_complex)\n\n    # Triton\u53CD\u5411\u4F20\u64AD\uFF0C\u5BF9\
    \u4E8E\u5B9E\u6570Lambda\n    grad_output_real = torch.ones_like(y_triton_real,\
    \ device=\"cuda\")\n    y_triton_real.backward(grad_output_real)\n    # Triton\u53CD\
    \u5411\u4F20\u64AD\uFF0C\u5BF9\u4E8E\u590D\u6570Lambda\n    grad_output_complex\
    \ = torch.ones_like(y_triton_complex, device=\"cuda\")\n    y_triton_complex.backward(grad_output_complex)\n\
    \n    results = {\n        \"test_case_1\": {\n            \"y_triton_real\":\
    \ y_triton_real,\n            \"grad_s_real\": s_real.grad.clone(),\n        \
    \    \"grad_x_real\": x_real.grad.clone(),\n            \"grad_Lambda_real\":\
    \ Lambda_real.grad.clone(),\n        },\n        \"test_case_2\": {\n        \
    \    \"y_triton_complex\": y_triton_complex,\n            \"grad_s_complex\":\
    \ s_complex.grad.clone(),\n            \"grad_x_complex\": x_complex.grad.clone(),\n\
    \            \"grad_Lambda_complex\": Lambda_complex.grad.clone(),\n        }\n\
    \    }\n    \n    return results\n\nresult_gold = test_diag_ssm_triton()\n\n\n\
    Don't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py diag_ssm_triton.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- diag_ssm_triton
