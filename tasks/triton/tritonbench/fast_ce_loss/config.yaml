compile_command:
- python fast_ce_loss.py
correctness_command:
- python fast_ce_loss_perf.py
performance_command:
- tb_eval -f fast_ce_loss.py -o fast_ce_loss_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton-based implementation provides optimized GPU kernels for\
    \ cross-entropy loss calculations. It comprises several key functions:\n     \
    \       \n            - `calculate_settings(n)`: Determines the optimal block\
    \ size and number of warps for GPU execution based on the vocabulary size `n`.\
    \ It ensures the block size does not exceed `MAX_FUSED_SIZE`.\n            \n\
    \            - `_cross_entropy_forward(...)`: This kernel calculates the cross-entropy\
    \ loss for a single input row using a block of threads. It optionally applies\
    \ softcapping and logit scaling to the logits. It performs the log-sum-exp calculation\
    \ for numerical stability and stores the result. If the label is valid (not `-100`),\
    \ it computes the loss by subtracting the logit corresponding to the true label\
    \ from the log-sum-exp value.\n            \n            - `_chunked_cross_entropy_forward(...)`:\
    \ This variant handles cases where the vocabulary size exceeds `MAX_FUSED_SIZE`.\
    \ It breaks the computation into chunks. It uses similar logic as `_cross_entropy_forward`\
    \ but accumulates partial log-sum-exp results across chunks.\n            \n \
    \           - `_cross_entropy_backward(...)`: Computes gradients with respect\
    \ to logits. For each block of logits, it calculates the softmax derivative, applies\
    \ optional transformations (softcapping, logit scaling), and stores the result\
    \ multiplied by the incoming gradient `dlosses`.\n            \n            The\
    \ `Fast_CrossEntropyLoss` class encapsulates these kernels. Its `forward` method\
    \ checks if chunking is needed, sets up the data, and launches the appropriate\
    \ kernel(s). The `backward` method launches `_cross_entropy_backward` to compute\
    \ gradients.\n            \n            The `fast_cross_entropy_loss` function\
    \ processes batched inputs and computes the mean loss, accounting for potentially\
    \ masked labels (value `-100` indicates masking).\n            \nThe test code\
    \ is:\n\n\nimport torch\n\ndef test_fast_cross_entropy_loss_with_backward():\n\
    \    # Test case 1: Basic test without softcapping or logit scaling\n    logits\
    \ = torch.randn(2, 3, 5, device='cuda:0', requires_grad=True)  # Batch size 2,\
    \ sequence length 3, vocab size 5\n    labels = torch.tensor([[1, 2, 3], [0, 1,\
    \ 4]], device='cuda:0')  # Corresponding labels\n    loss = fast_cross_entropy_loss(logits,\
    \ labels)\n\n    # Perform backward pass\n    loss.backward()\n\n    # Reset gradients\n\
    \    logits.grad.zero_()\n\n    # Test case 2: With logit softcapping\n    logit_softcapping\
    \ = 0.5\n    loss = fast_cross_entropy_loss(logits, labels, logit_softcapping=logit_softcapping)\n\
    \n    # Perform backward pass\n    loss.backward()\n\n    # Reset gradients\n\
    \    logits.grad.zero_()\n\n    # Test case 3: With logit scaling\n    logit_scaling\
    \ = 1.5\n    loss = fast_cross_entropy_loss(logits, labels, logit_scaling=logit_scaling)\n\
    \n    # Perform backward pass\n    loss.backward()\n\n    # Reset gradients\n\
    \    logits.grad.zero_()\n\n    # Test case 4: With both softcapping and logit\
    \ scaling\n    loss = fast_cross_entropy_loss(logits, labels, logit_softcapping=logit_softcapping,\
    \ logit_scaling=logit_scaling)\n\n    # Perform backward pass\n    loss.backward()\n\
    \n    # Reset gradients\n    logits.grad.zero_()\n\n    # Test case 5: Handling\
    \ ignore index (-100)\n    labels_with_ignore = torch.tensor([[1, -100, 3], [0,\
    \ 1, -100]], device='cuda:0')\n    loss = fast_cross_entropy_loss(logits, labels_with_ignore)\n\
    \n    # Perform backward pass\n    loss.backward()\n\n    return {\n        \"\
    test_case_1\": loss.item(),\n        \"test_case_2\": loss.item(),\n        \"\
    test_case_3\": loss.item(),\n        \"test_case_4\": loss.item(),\n        \"\
    test_case_5\": loss.item()\n    }\n\nresult_gold = test_fast_cross_entropy_loss_with_backward()\n\
    \n\nDon't append test code to the kernel code or edit test function.\n\nThe generated\
    \ code should be written into a python file.\nIf you have already created a file\
    \ and wrote the code into it, edit the code directly in the file.\nTest the code\
    \ by running `python python_bindings/tritonbench.py fast_ce_loss.py {kernel_path}`\
    \ to check the correctness and performance.The kernel_path is where you stored\
    \ the generated code.\nCall Status means whether the code can be executed, Exec\
    \ Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- fast_ce_loss
