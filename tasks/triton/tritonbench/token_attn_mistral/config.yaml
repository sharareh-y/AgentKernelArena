compile_command:
- python token_attn_mistral.py
correctness_command:
- python token_attn_mistral_perf.py
performance_command:
- tb_eval -f token_attn_mistral.py -o token_attn_mistral_output.json -run_on_code
  -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `_fwd_kernel_token_att2` is designed to compute\
    \ attention over tokens in a parallelized fashion. It takes several inputs: probability\
    \ matrix `Prob`, value tensor `V`, and an output tensor `Out` among other auxiliary\
    \ inputs such as `Req_to_tokens` that maps requests to tokens, and several batch-related\
    \ indices and strides. \n\n            - The kernel computes attention by iterating\
    \ over token sequences (`BLOCK_N` size blocks) within a batch and head context\
    \ (`cur_batch` and `cur_head`). The indices are adjusted based on the current\
    \ position, head, and sequence length to ensure that only valid indices are processed.\n\
    \            \n            - For each block of tokens, it loads relevant slices\
    \ of the `Prob` and `V` tensors into `p_value` and `v_value`. These are multiplied\
    \ and accumulated into `acc`, the accumulator for the final result.\n\n      \
    \      - Once all blocks are processed, `acc` is cast to the output's data type\
    \ and stored in the output tensor `Out` at the corresponding position calculated\
    \ via strides.\n\n            The function `token_att_fwd2` serves as a Python\
    \ interface for this Triton kernel. It initializes parameters like block size\
    \ (`BLOCK`), grid dimensions, and computes `kv_group_num` to determine the head\
    \ groups for key-value pairs. It then calls the Triton kernel with these parameters.\
    \ No gradients are computed as the function is decorated with `@torch.no_grad()`.\n\
    \            \nThe test code is:\n\n\n# Define the test function\ndef test_token_att_fwd2():\n\
    \    # Define the dimensions\n    batch_size = 2\n    num_heads = 4\n    seq_len\
    \ = 128\n    d_model = 64\n    sliding_window = 64\n\n    # Create random tensors\
    \ for inputs\n    prob = torch.rand((num_heads, seq_len), dtype=torch.float32,\
    \ device='cuda')\n    v = torch.rand((num_heads, seq_len, d_model), dtype=torch.float32,\
    \ device='cuda')\n    Req_to_tokens = torch.randint(0, seq_len, (batch_size, seq_len),\
    \ dtype=torch.int32, device='cuda')\n    B_req_idx = torch.randint(0, batch_size,\
    \ (batch_size,), dtype=torch.int32, device='cuda')\n    B_Start_Loc = torch.zeros((batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    B_Seqlen = torch.full((batch_size,),\
    \ seq_len, dtype=torch.int32, device='cuda')\n    B_Att_Start_Loc = torch.zeros((batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    B_Att_Seqlen = torch.full((batch_size,),\
    \ seq_len, dtype=torch.int32, device='cuda')\n\n    results = {}\n\n    # Test\
    \ case 1\n    out1 = torch.zeros((batch_size, num_heads, d_model), dtype=torch.float32,\
    \ device='cuda')\n    token_att_fwd2(\n        prob, v, out1, Req_to_tokens, B_req_idx,\
    \ B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, sliding_window\n    )\n\
    \    results['test_case_1'] = out1.clone()\n\n    # Test case 2 (different sliding_window\
    \ size)\n    sliding_window = 32\n    out2 = torch.zeros((batch_size, num_heads,\
    \ d_model), dtype=torch.float32, device='cuda')\n    token_att_fwd2(\n       \
    \ prob, v, out2, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen, B_Att_Start_Loc,\
    \ B_Att_Seqlen, sliding_window\n    )\n    results['test_case_2'] = out2.clone()\n\
    \n    # Test case 3 (different sequence length for Req_to_tokens)\n    Req_to_tokens\
    \ = torch.randint(0, seq_len, (batch_size, seq_len // 2), dtype=torch.int32, device='cuda')\n\
    \    out3 = torch.zeros((batch_size, num_heads, d_model), dtype=torch.float32,\
    \ device='cuda')\n    token_att_fwd2(\n        prob, v, out3, Req_to_tokens, B_req_idx,\
    \ B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, sliding_window\n    )\n\
    \    results['test_case_3'] = out3.clone()\n\n    # Test case 4 (different batch\
    \ size)\n    batch_size = 4\n    prob = torch.rand((num_heads, seq_len), dtype=torch.float32,\
    \ device='cuda')\n    v = torch.rand((num_heads, seq_len, d_model), dtype=torch.float32,\
    \ device='cuda')\n    Req_to_tokens = torch.randint(0, seq_len, (batch_size, seq_len),\
    \ dtype=torch.int32, device='cuda')\n    B_req_idx = torch.randint(0, batch_size,\
    \ (batch_size,), dtype=torch.int32, device='cuda')\n    B_Start_Loc = torch.zeros((batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    B_Seqlen = torch.full((batch_size,),\
    \ seq_len, dtype=torch.int32, device='cuda')\n    B_Att_Start_Loc = torch.zeros((batch_size,),\
    \ dtype=torch.int32, device='cuda')\n    B_Att_Seqlen = torch.full((batch_size,),\
    \ seq_len, dtype=torch.int32, device='cuda')\n\n    out4 = torch.zeros((batch_size,\
    \ num_heads, d_model), dtype=torch.float32, device='cuda')\n    token_att_fwd2(\n\
    \        prob, v, out4, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen, B_Att_Start_Loc,\
    \ B_Att_Seqlen, sliding_window\n    )\n    results['test_case_4'] = out4.clone()\n\
    \n    return results\n\n\n# Execute the test function\nif __name__ == '__main__':\n\
    \    result_gold = test_token_att_fwd2()\n\n\nDon't append test code to the kernel\
    \ code or edit test function.\n\nThe generated code should be written into a python\
    \ file.\nIf you have already created a file and wrote the code into it, edit the\
    \ code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ token_attn_mistral.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- token_attn_mistral
