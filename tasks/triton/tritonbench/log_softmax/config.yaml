compile_command:
- python log_softmax.py
correctness_command:
- python log_softmax_perf.py
performance_command:
- tb_eval -f log_softmax.py -o log_softmax_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \        The provided code implements a custom log softmax operation using Triton,\
    \ an efficient parallel programming library for GPUs, and integrates it as a PyTorch\
    \ autograd function. The code comprises two main Triton kernels and a Python class\
    \ to wrap them.\n\n        The 'log_softmax_kernel' Triton kernel computes the\
    \ log softmax along a specified dimension of a tensor. It does so by first calculating\
    \ the row-wise maximum to stabilize the softmax computation, subtracting this\
    \ maximum from each element, and then computing the exponentials. The sum of these\
    \ exponentials for each row is used to normalize the outputs. The kernel uses\
    \ triton.autotune and triton.heuristics for performance optimization based on\
    \ input size, leveraging BLOCK_M and BLOCK_N constants to define block sizes and\
    \ num_warps to optimize parallel execution.\n\n        The 'log_softmax_backward_kernel'\
    \ is the backward pass for the log softmax operation. It calculates the gradient\
    \ of the input tensor by using the chain rule, considering the output and the\
    \ gradient of the output from the forward pass. This involves subtracting the\
    \ weighted exponentiated output from the gradient, scaled by the sum of the gradients\
    \ along the specified dimension.\n\n        The 'LogSoftmax' class serves as a\
    \ PyTorch custom autograd function, which integrates the Triton kernels into PyTorch's\
    \ autograd system. In the 'forward' method, it calculates necessary dimensions\
    \ (M and N) and the grid size for kernel invocation, invoking 'log_softmax_kernel'\
    \ to compute the result. The result is saved for backward computation. The 'backward'\
    \ method retrieves the saved output and uses 'log_softmax_backward_kernel' to\
    \ calculate the input gradient, correctly considering dimensionality for both\
    \ forward and backward passes.\n\n        Finally, the 'log_softmax' function\
    \ acts as a convenient wrapper for users, accepting a tensor and a dimension along\
    \ which to apply the log softmax, while handling contiguous memory and optional\
    \ dtype specification. It applies the 'LogSoftmax' function, returning the result\
    \ tensor with the same shape as the input.\n    \nThe test code is:\n\n\ndef test_log_softmax():\n\
    \    # \u8F93\u5165\u5F20\u91CF\u7684\u5F62\u72B6\n    b, h, n, d = 2, 8, 128,\
    \ 64  # batch_size, num_heads, seq_len, embed_dim\n    \n    # \u521B\u5EFA\u968F\
    \u673A\u7684\u8F93\u5165\u5F20\u91CF\n    x = torch.randn((b, h, n, d), dtype=torch.float32,\
    \ device='cuda', requires_grad=True)\n    \n    # \u524D\u5411\u4F20\u64AD\n \
    \   out = log_softmax(x, dim=-1)\n    \n    # \u53CD\u5411\u4F20\u64AD\n    out.sum().backward()\
    \  # \u8BA1\u7B97\u603B\u548C\u7684\u68AF\u5EA6\n\n    # \u8FD4\u56DEresults\n\
    \    results = {\n        'test_case_1': (\n            out.cpu().detach().numpy(),\
    \  # \u524D\u5411\u4F20\u64AD\u8F93\u51FA\n            x.grad.cpu().detach().numpy()\
    \  # x\u7684\u68AF\u5EA6\n        )\n    }\n    \n    return results\n\n\nresult_gold\
    \ = test_log_softmax()\n# print(result_gold)\n\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ log_softmax.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- log_softmax
