compile_command:
- python matmul_tma.py
correctness_command:
- python matmul_tma_perf.py
performance_command:
- tb_eval -f matmul_tma.py -o matmul_tma_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The Triton kernel `matmul_tma_load_store` is designed to perform\
    \ block-based matrix multiplication. It uses the `tl.make_block_ptr` function\
    \ to create block pointers for matrices A, B, and C, each with specific shapes\
    \ and strides, which facilitate efficient loading of sub-matrices. The parameters\
    \ `BLOCK_M`, `BLOCK_N`, and `BLOCK_K` define the shape of these sub-matrices.\
    \ The kernel loads blocks of A and B using `tl.load` and computes the matrix product\
    \ using `tl.dot`. If `OUTPUT_F16` is set, it converts the result to float16 before\
    \ storing it with `tl.store`. The Python wrapper `warpper_tma_load_store` generates\
    \ random matrices A and B, optionally transposes them, allocates matrix C, and\
    \ calls the kernel. The wrapper accepts multiple parameters like matrix dimensions\
    \ (M, N, K), number of warps and CTAs, transpose flags for A and B, and the output\
    \ format for C.\n            \nThe test code is:\n\n\nimport torch\n\ndef test_all_branches():\n\
    \    M, N, K = 128, 128, 128\n    NUM_CTAS = 1\n    NUM_WARPS = 4\n    \n    results\
    \ = {}\n\n    # Test case 1: No transposition, output in float32\n    out = warpper_tma_load_store(M,\
    \ N, K, NUM_CTAS, NUM_WARPS, TRANS_A=False, TRANS_B=False, OUTPUT_F16=False)\n\
    \    results[\"test_case_1\"] = out\n\n    # Test case 2: Transpose A, no transpose\
    \ B, output in float32\n    out = warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS,\
    \ TRANS_A=True, TRANS_B=False, OUTPUT_F16=False)\n    results[\"test_case_2\"\
    ] = out\n\n    # Test case 3: No transpose A, transpose B, output in float32\n\
    \    out = warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A=False,\
    \ TRANS_B=True, OUTPUT_F16=False)\n    results[\"test_case_3\"] = out\n\n    #\
    \ Test case 4: Transpose A, transpose B, output in float32\n    out = warpper_tma_load_store(M,\
    \ N, K, NUM_CTAS, NUM_WARPS, TRANS_A=True, TRANS_B=True, OUTPUT_F16=False)\n \
    \   results[\"test_case_4\"] = out\n\n    # Test case 5: No transposition, output\
    \ in float16\n    out = warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A=False,\
    \ TRANS_B=False, OUTPUT_F16=True)\n    results[\"test_case_5\"] = out\n\n    #\
    \ Test case 6: Transpose A, no transpose B, output in float16\n    out = warpper_tma_load_store(M,\
    \ N, K, NUM_CTAS, NUM_WARPS, TRANS_A=True, TRANS_B=False, OUTPUT_F16=True)\n \
    \   results[\"test_case_6\"] = out\n\n    # Test case 7: No transpose A, transpose\
    \ B, output in float16\n    out = warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS,\
    \ TRANS_A=False, TRANS_B=True, OUTPUT_F16=True)\n    results[\"test_case_7\"]\
    \ = out\n\n    # Test case 8: Transpose A, transpose B, output in float16\n  \
    \  out = warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A=True, TRANS_B=True,\
    \ OUTPUT_F16=True)\n    results[\"test_case_8\"] = out\n\n    return results\n\
    \n# Execute the test function and store the result in result_gold\nresult_gold\
    \ = test_all_branches()\n\n# print(result_gold)\n\nDon't append test code to the\
    \ kernel code or edit test function.\n\nThe generated code should be written into\
    \ a python file.\nIf you have already created a file and wrote the code into it,\
    \ edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ matmul_tma.py {kernel_path}` to check the correctness and performance.The kernel_path\
    \ is where you stored the generated code.\nCall Status means whether the code\
    \ can be executed, Exec Status means whether the code is functionally correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_tma
