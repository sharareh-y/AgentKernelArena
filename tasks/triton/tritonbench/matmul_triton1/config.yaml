compile_command:
- python matmul_triton1.py
correctness_command:
- python matmul_triton1_perf.py
performance_command:
- tb_eval -f matmul_triton1.py -o matmul_triton1_output.json -run_on_code -ds tbg
task_type: instruction2triton
task_result_template: null
prompt:
  cheatsheet: null
  instructions: "You are a expert in writing Triton operators for efficient GPU programming.\
    \ Use triton language write a kernel and wrapper according following instruction.\n\
    \            The provided Triton kernel named 'matmul_kernel' is designed for\
    \ performing block-wise matrix multiplication. \n            It operates on matrices\
    \ x and y of dimensions (m_size, k_size) and (k_size, n_size), respectively, and\
    \ outputs a result matrix z of dimensions (m_size, n_size).\n            The kernel\
    \ begins by determining which block of the output matrix to calculate based on\
    \ the program's id (`pid`). \n            It computes the indices for these blocks\
    \ by dividing the output matrix into smaller blocks, with sizes determined by\
    \ m_block_size, k_block_size, and n_block_size.\n            Using these indices,\
    \ the kernel computes memory offsets to fetch blocks of data from the input matrices\
    \ x and y.\n            These blocks are loaded into shared memory using `tl.load`,\
    \ which leverages the high bandwidth of shared memory to accelerate computation.\n\
    \            The core computation is performed using `tl.dot`, which computes\
    \ the dot product of the fetched sub-matrices, accumulating results in a local\
    \ variable `z`.\n            This operation is repeated for all k blocks to fully\
    \ compute the matrix product.\n            Once the computation for the current\
    \ block is complete, the resulting sub-matrix `z` is stored back to the output\
    \ matrix z in global memory.\n            The `matmul` function acts as a wrapper\
    \ around this kernel, setting up the grid and blocks, and initializing the output\
    \ matrix. \n            It calculates the required grid size to cover all blocks\
    \ of the output matrix and launches the kernel with appropriate arguments.\n \
    \           \nThe test code is:\n\n\nimport torch\n\ndef test_matmul():\n    #\
    \ Test the matmul function with different block sizes\n    x = torch.randn(16,\
    \ 16, device='cuda:0')\n    y = torch.randn(16, 16, device='cuda:0')\n\n    #\
    \ Test case 1\n    output1 = matmul(x, y)\n\n    return {\n        \"test_case_1\"\
    : output1,\n    }\n\nresult_gold = test_matmul()\n\n\nDon't append test code to\
    \ the kernel code or edit test function.\n\nThe generated code should be written\
    \ into a python file.\nIf you have already created a file and wrote the code into\
    \ it, edit the code directly in the file.\nTest the code by running `python python_bindings/tritonbench.py\
    \ matmul_triton1.py {kernel_path}` to check the correctness and performance.The\
    \ kernel_path is where you stored the generated code.\nCall Status means whether\
    \ the code can be executed, Exec Status means whether the code is functionally\
    \ correct.\n"
  source_code: null
source_file_path:
- null
target_kernel_functions:
- matmul_triton1
